<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | Yunlong (Yolo) Tang </title> <meta name="author" content="Yunlong (Yolo) Tang"> <meta name="description" content="* Equal Contribution | â€  Corresponding Author"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo/icon.jpg?313e0410176f41a6e55d7f7cd141e7a4"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yunlong10.github.io/publications/"> <script src="/assets/js/theme.js?6eeff2fb375dfabd5e33c4eefb1fadbc"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Yunlong</span> (Yolo) Tang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description">* Equal Contribution | â€  Corresponding Author</p> </header> <article> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">2026</h2> <ol class="bibliography"><li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">AAAI Demo</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/cat-v-480.webp 480w,/assets/img/publication_preview/cat-v-800.webp 800w,/assets/img/publication_preview/cat-v-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/cat-v.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cat-v.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="tang2025catv" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2504.05541" rel="external nofollow noopener" target="_blank">Caption Anything in Video: Fine-grained Object-centric Captioning via Spatiotemporal Multimodal Prompting</a> </div> <div class="author"> <em>Yunlong Tang</em>,Â Jing Bi,Â Chao Huang,Â Susan Liang,Â Daiki Shimada,Â Hang Hua,Â Yunzhong Xiao,Â Yizhi Song,Â Pinxin Liu,Â Mingqian Feng, and <span class="more-authors" title="click to view 9 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '9 more authors' ? 'Junjia Guo, Zhuo Liu, Luchuan Song, Ali Vosoughi, Jinxi He, Liu He, Zeliang Zhang, Jiebo Luo, Chenliang Xu&lt;sup&gt;â€ &lt;/sup&gt;' : '9 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">9 more authors</span> </div> <div class="periodical"> <em>AAAI Demonstartion Program; arXiv preprint arXiv:2504.05541</em>, 2026 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite </a> <a href="https://www.youtube.com/watch?v=2eiPVKXEoxw" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/yunlong10/CAT-V" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/yunlong10/CAT-V?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="abstract hidden"> <p> We present CAT-V (Caption AnyThing in Video), a training-free framework for fine-grained object-centric video captioning that enables detailed descriptions of user-selected objects through time. CAT-V integrates three key components: a Segmenter based on SAMURAI for precise object segmentation across frames, a Temporal Analyzer powered by TRACE-Uni for accurate event boundary detection and temporal analysis, and a Captioner using InternVL-2.5 for generating detailed object-centric descriptions. Through spatiotemporal visual prompts and chain-of-thought reasoning, our framework generates detailed, temporally-aware descriptions of objectsâ€™ attributes, actions, statuses, interactions, and environmental contexts without requiring additional training data. CAT-V supports flexible user interactions through various visual prompts (points, bounding boxes, and irregular regions) and maintains temporal sensitivity by tracking object states and interactions across different time segments. Our approach addresses limitations of existing video captioning methods, which either produce overly abstract descriptions or lack object-level precision, enabling fine-grained, object-specific descriptions while maintaining temporal coherence and spatial accuracy. The GitHub repository for this project is available at: https://github.com/yunlong10/CAT-V </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tang2025catv</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Caption Anything in Video: Fine-grained Object-centric Captioning via Spatiotemporal Multimodal Prompting}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tang, Yunlong and Bi, Jing and Huang, Chao and Liang, Susan and Shimada, Daiki and Hua, Hang and Xiao, Yunzhong and Song, Yizhi and Liu, Pinxin and Feng, Mingqian and Guo, Junjia and Liu, Zhuo and Song, Luchuan and Vosoughi, Ali and He, Jinxi and He, Liu and Zhang, Zeliang and Luo, Jiebo and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{AAAI Demonstartion Program; arXiv preprint arXiv:2504.05541}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2026}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>window.__GS_CITES_FILLED__||(window.__GS_CITES_FILLED__=!0,fetch("https://raw.githubusercontent.com/yunlong10/citation-badge/main/per_article_citations.json",{cache:"no-cache"}).then(function(t){return t.json()}).then(function(t){document.querySelectorAll(".gs-cites").forEach(function(n){var e=n.getAttribute("data-article-id"),c=t&&Object.prototype.hasOwnProperty.call(t,e)?t[e]:0;n.textContent=String(c)})})["catch"](function(){document.querySelectorAll(".gs-cites").forEach(function(t){t.textContent="N/A"})}));</script> </li></ol> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/video-r4-480.webp 480w,/assets/img/publication_preview/video-r4-800.webp 800w,/assets/img/publication_preview/video-r4-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/video-r4.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="video-r4.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="tang2025videor4" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2511.17490" rel="external nofollow noopener" target="_blank">Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination</a> </div> <div class="author"> <em>Yolo Yunlong Tang</em>,Â Daiki Shimada,Â Hang Hua,Â Chao Huang,Â Jing Bi,Â Rogerio Feris,Â andÂ Chenliang Xu </div> <div class="periodical"> <em>arXiv preprint arXiv:2511.17490</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://yunlong10.github.io/Video-R4/" class="btn btn-sm z-depth-0" role="button">Website</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite </a> <a href="https://github.com/yunlong10/Video-R4" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/yunlong10/Video-R4?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="abstract hidden"> <p> Understanding text-rich videos requires reading small, transient textual cues that often demand repeated inspection. Yet most video QA models rely on single-pass perception over fixed frames, leading to hallucinations and failures on fine-grained evidence. Inspired by how humans pause, zoom, and re-read critical regions, we introduce Video-R4 (Reinforcing Text-Rich Video Reasoning with Visual Rumination), a video reasoning LMM that performs visual rumination: iteratively selecting frames, zooming into informative regions, re-encoding retrieved pixels, and updating its reasoning state. We construct two datasets with executable rumination trajectories: Video-R4-CoT-17k for supervised practice and Video-R4-RL-30k for reinforcement learning. We propose a multi-stage rumination learning framework that progressively finetunes a 7B LMM to learn atomic and mixing visual operations via SFT and GRPO-based RL. Video-R4-7B achieves state-of-the-art results on M4-ViteVQA and further generalizes to multi-page document QA, slides QA, and generic video QA, demonstrating that iterative rumination is an effective paradigm for pixel-grounded multimodal reasoning. For more information, readers are recommended to visit the repository at https://github.com/yunlong10/Video-R4. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tang2025videor4</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tang, Yolo Yunlong and Shimada, Daiki and Hua, Hang and Huang, Chao and Bi, Jing and Feris, Rogerio and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2511.17490}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>window.__GS_CITES_FILLED__||(window.__GS_CITES_FILLED__=!0,fetch("https://raw.githubusercontent.com/yunlong10/citation-badge/main/per_article_citations.json",{cache:"no-cache"}).then(function(t){return t.json()}).then(function(t){document.querySelectorAll(".gs-cites").forEach(function(n){var e=n.getAttribute("data-article-id"),c=t&&Object.prototype.hasOwnProperty.call(t,e)?t[e]:0;n.textContent=String(c)})})["catch"](function(){document.querySelectorAll(".gs-cites").forEach(function(t){t.textContent="N/A"})}));</script> </li> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/toolmem-480.webp 480w,/assets/img/publication_preview/toolmem-800.webp 800w,/assets/img/publication_preview/toolmem-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/toolmem.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="toolmem.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="xiao2025toolmem" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2510.06664" rel="external nofollow noopener" target="_blank">ToolMem: Enhancing Multimodal Agents with Learnable Tool Capability Memory</a> </div> <div class="author"> Yunzhong Xiao,Â Yangmin Li,Â Hewei Wang,Â <em>Yolo Yunlong Tang</em>,Â andÂ Zora Zhiruo Wang </div> <div class="periodical"> <em>arXiv preprint arXiv:2510.06664</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2510.06664.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite </a> </div> <div class="abstract hidden"> <p> Agents utilizing tools powered by large language models (LLMs) or vision-language models (VLMs) have demonstrated remarkable progress in diverse tasks across text and visual modalities. Unlike traditional tools such as calculators, which give deterministic outputs, neural tools perform uncertainly across task scenarios. While different tools for a task may excel in varied scenarios, existing agents typically rely on fixed tools, thus limiting the flexibility in selecting the most suitable tool for specific tasks. In contrast, humans snowball their understanding of the capabilities of different tools by interacting with them, and apply this knowledge to select the optimal tool when solving a future task. To build agents that similarly benefit from this process, we propose ToolMem that enables agents to develop memories of tool capabilities from previous interactions, by summarizing their strengths and weaknesses and storing them in memory; at inference, the agent can retrieve relevant entries from ToolMem, and select the best tool to solve individual tasks more accurately. We evaluate ToolMem on learning varied text generation and text-to-image generation neural tools. Compared to no-memory, generic agents, we find ToolMem-augmented agents predict tool performance 14.8% and 28.7% more accurately across text and multimodal generation scenarios. Moreover, ToolMem facilitates optimal tool selection among multiple choices by 21% and 24% absolute increases in respective scenarios. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">xiao2025toolmem</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ToolMem: Enhancing Multimodal Agents with Learnable Tool Capability Memory}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xiao, Yunzhong and Li, Yangmin and Wang, Hewei and Tang, Yolo Yunlong and Wang, Zora Zhiruo}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2510.06664}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>window.__GS_CITES_FILLED__||(window.__GS_CITES_FILLED__=!0,fetch("https://raw.githubusercontent.com/yunlong10/citation-badge/main/per_article_citations.json",{cache:"no-cache"}).then(function(t){return t.json()}).then(function(t){document.querySelectorAll(".gs-cites").forEach(function(n){var e=n.getAttribute("data-article-id"),c=t&&Object.prototype.hasOwnProperty.call(t,e)?t[e]:0;n.textContent=String(c)})})["catch"](function(){document.querySelectorAll(".gs-cites").forEach(function(t){t.textContent="N/A"})}));</script> </li> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/videolmm_posttraining-480.webp 480w,/assets/img/publication_preview/videolmm_posttraining-800.webp 800w,/assets/img/publication_preview/videolmm_posttraining-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/videolmm_posttraining.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="videolmm_posttraining.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="tang2025videolmm_posttraining" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2510.05034" rel="external nofollow noopener" target="_blank">Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models ðŸ”¥ðŸ”¥ðŸ”¥</a> </div> <div class="author"> <em>Yolo Yunlong Tang</em>,Â Jing Bi,Â Pinxin Liu,Â Zhenyu Pan,Â Zhangyun Tan,Â Qianxiang Shen,Â Jiani Liu,Â Hang Hua,Â Junjia Guo,Â Yunzhong Xiao, and <span class="more-authors" title="click to view 17 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '17 more authors' ? 'Chao Huang, Zhiyuan Wang, Susan Liang, Xinyi Liu, Yizhi Song, Junhua Huang, Jia-Xing Zhong, Bozheng Li, Daiqing Qi, Ziyun Zeng, Ali Vosoughi, Luchuan Song, Zeliang Zhang, Daiki Shimada, Han Liu, Jiebo Luo, Chenliang Xu' : '17 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">17 more authors</span> </div> <div class="periodical"> <em>arXiv preprint arXiv:2510.05034</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2510.05034.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite </a> <a href="https://github.com/yunlong10/Awesome-Video-LMM-Post-Training" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/yunlong10/Awesome-Video-LMM-Post-Training?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="abstract hidden"> <p> Video understanding represents the most challenging frontier in computer vision. It requires models to reason about complex spatiotemporal relationships, long-term dependencies, and multimodal evidence. The recent emergence of Video-Large Multimodal Models (Video-LMMs), which integrate visual encoders with powerful decoder-based language models, has demonstrated remarkable capabilities in video understanding tasks. However, the critical phase that transforms these models from basic perception systems into sophisticated reasoning engines, post-training, remains fragmented across the literature. This survey provides the first comprehensive examination of post-training methodologies for Video-LMMs, encompassing three fundamental pillars: supervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL) from verifiable objectives, and test-time scaling (TTS) through enhanced inference computation. We present a structured taxonomy that clarifies the roles, interconnections, and video-specific adaptations of these techniques, addressing unique challenges such as temporal localization, spatiotemporal grounding, long video efficiency, and multimodal evidence integration. Through systematic analysis of representative methods, we synthesize key design principles, insights, and evaluation protocols while identifying critical open challenges in reward design, scalability, and cost-performance optimization. We further curate essential benchmarks, datasets, and metrics to facilitate rigorous assessment of post-training effectiveness. This survey aims to provide researchers and practitioners with a unified framework for advancing Video-LMM capabilities. Additional resources and updates are maintained at: https://arxiv.org/abs/2510.05034 </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tang2025videolmm_posttraining</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models ðŸ”¥ðŸ”¥ðŸ”¥}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tang, Yolo Yunlong and Bi, Jing and Liu, Pinxin and Pan, Zhenyu and Tan, Zhangyun and Shen, Qianxiang and Liu, Jiani and Hua, Hang and Guo, Junjia and Xiao, Yunzhong and Huang, Chao and Wang, Zhiyuan and Liang, Susan and Liu, Xinyi and Song, Yizhi and Huang, Junhua and Zhong, Jia-Xing and Li, Bozheng and Qi, Daiqing and Zeng, Ziyun and Vosoughi, Ali and Song, Luchuan and Zhang, Zeliang and Shimada, Daiki and Liu, Han and Luo, Jiebo and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2510.05034}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>window.__GS_CITES_FILLED__||(window.__GS_CITES_FILLED__=!0,fetch("https://raw.githubusercontent.com/yunlong10/citation-badge/main/per_article_citations.json",{cache:"no-cache"}).then(function(t){return t.json()}).then(function(t){document.querySelectorAll(".gs-cites").forEach(function(n){var e=n.getAttribute("data-article-id"),c=t&&Object.prototype.hasOwnProperty.call(t,e)?t[e]:0;n.textContent=String(c)})})["catch"](function(){document.querySelectorAll(".gs-cites").forEach(function(t){t.textContent="N/A"})}));</script> </li> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/advevo-marl-480.webp 480w,/assets/img/publication_preview/advevo-marl-800.webp 800w,/assets/img/publication_preview/advevo-marl-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/advevo-marl.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="advevo-marl.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="pan2025advevo_marl" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2510.01586" rel="external nofollow noopener" target="_blank">AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning</a> </div> <div class="author"> Zhenyu Pan,Â Yiting Zhang,Â Zhuo Liu,Â <em>Yolo Yunlong Tang</em>,Â Zeliang Zhang,Â Haozheng Luo,Â Yuwei Han,Â Jianshu Zhang,Â Dennis Wu,Â Hong-Yu Chen, and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Haoran Lu, Haoyang Fang, Manling Li, Chenliang Xu, Philip S. Yu, Han Liu' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em>arXiv preprint arXiv:2510.01586</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2510.01586.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite </a> </div> <div class="abstract hidden"> <p> LLM-based multi-agent systems excel at planning, tool use, and role coordination, but their openness and interaction complexity also expose them to jailbreak, prompt-injection, and adversarial collaboration. Existing defenses fall into two lines: (i) self-verification that asks each agent to pre-filter unsafe instructions before execution, and (ii) external guard modules that police behaviors. The former often underperforms because a standalone agent lacks sufficient capacity to detect cross-agent unsafe chains and delegation-induced risks; the latter increases system overhead and creates a single-point-of-failureâ€”once compromised, system-wide safety collapses, and adding more guards worsens cost and complexity. To solve these challenges, we propose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning framework that internalizes safety into task agents. Rather than relying on external guards, AdvEvo-MARL jointly optimizes attackers (which synthesize evolving jailbreak prompts) and defenders (task agents trained to both accomplish their duties and resist attacks) in adversarial learning environments. To stabilize learning and foster cooperation, we introduce a public baseline for advantage estimation: agents within the same functional group share a group-level mean-return baseline, enabling lower-variance updates and stronger intra-group coordination. Across representative attack scenarios, AdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas baselines reach up to 38.33%, while preservingâ€”and sometimes improvingâ€”task accuracy (up to +3.67% on reasoning tasks). These results show that safety and utility can be jointly improved without relying on extra guard agents or added system overhead. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">pan2025advevo_marl</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pan, Zhenyu and Zhang, Yiting and Liu, Zhuo and Tang, Yolo Yunlong and Zhang, Zeliang and Luo, Haozheng and Han, Yuwei and Zhang, Jianshu and Wu, Dennis and Chen, Hong-Yu and Lu, Haoran and Fang, Haoyang and Li, Manling and Xu, Chenliang and Yu, Philip S. and Liu, Han}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2510.01586}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>window.__GS_CITES_FILLED__||(window.__GS_CITES_FILLED__=!0,fetch("https://raw.githubusercontent.com/yunlong10/citation-badge/main/per_article_citations.json",{cache:"no-cache"}).then(function(t){return t.json()}).then(function(t){document.querySelectorAll(".gs-cites").forEach(function(n){var e=n.getAttribute("data-article-id"),c=t&&Object.prototype.hasOwnProperty.call(t,e)?t[e]:0;n.textContent=String(c)})})["catch"](function(){document.querySelectorAll(".gs-cites").forEach(function(t){t.textContent="N/A"})}));</script> </li> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mmperspective-480.webp 480w,/assets/img/publication_preview/mmperspective-800.webp 800w,/assets/img/publication_preview/mmperspective-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/mmperspective.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mmperspective.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="tang2025mmperspective" class="col-sm-8"> <div class="title"> <a href="https://openreview.net/forum?id=VkuNZHv1fe" rel="external nofollow noopener" target="_blank">MMPerspective: Do MLLMs Understand Perspective? A Comprehensive Benchmark for Perspective Perception, Reasoning, and Robustness</a> </div> <div class="author"> <em>Yunlong Tang</em>,Â Pinxin Liu,Â Zhangyun Tan,Â Mingqian Feng,Â Rui Mao,Â Chao Huang,Â Jing Bi,Â Yunzhong Xiao,Â Susan Liang,Â Hang Hua, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Ali Vosoughi, Luchuan Song, Zeliang Zhang, Chenliang Xu' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>The Thirty-ninth Annual Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>) Datasets and Benchmarks Track</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://yunlong10.github.io/MMPerspective/" class="btn btn-sm z-depth-0" role="button">Website</a> <a href="/assets/pdf/mmperspective.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite </a> <a href="https://github.com/yunlong10/MMPerspective" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/yunlong10/MMPerspective?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tang2025mmperspective</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MMPerspective: Do MLLMs Understand Perspective? A Comprehensive Benchmark for Perspective Perception, Reasoning, and Robustness}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tang, Yunlong and Liu, Pinxin and Tan, Zhangyun and Feng, Mingqian and Mao, Rui and Huang, Chao and Bi, Jing and Xiao, Yunzhong and Liang, Susan and Hua, Hang and Vosoughi, Ali and Song, Luchuan and Zhang, Zeliang and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The Thirty-ninth Annual Conference on Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>window.__GS_CITES_FILLED__||(window.__GS_CITES_FILLED__=!0,fetch("https://raw.githubusercontent.com/yunlong10/citation-badge/main/per_article_citations.json",{cache:"no-cache"}).then(function(t){return t.json()}).then(function(t){document.querySelectorAll(".gs-cites").forEach(function(n){var e=n.getAttribute("data-article-id"),c=t&&Object.prototype.hasOwnProperty.call(t,e)?t[e]:0;n.textContent=String(c)})})["catch"](function(){document.querySelectorAll(".gs-cites").forEach(function(t){t.textContent="N/A"})}));</script> </li> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mmigbench-480.webp 480w,/assets/img/publication_preview/mmigbench-800.webp 800w,/assets/img/publication_preview/mmigbench-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/mmigbench.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mmigbench.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hua2025mmigbench" class="col-sm-8"> <div class="title"> <a href="https://openreview.net/forum?id=PXkdqh4SXT" rel="external nofollow noopener" target="_blank">MMIG-Bench: Towards Comprehensive and Explainable Evaluation of Multi-Modal Image Generation Models</a> </div> <div class="author"> Hang Hua,Â Ziyun Zeng,Â Yizhi Song,Â <em>Yunlong Tang</em>,Â Liu He,Â Daniel Aliaga,Â Wei Xiong,Â andÂ Jiebo Luo </div> <div class="periodical"> <em>The Thirty-ninth Annual Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>) Datasets and Benchmarks Track</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://hanghuacs.github.io/MMIG-Bench/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="/assets/pdf/mmigbench.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite </a> <a href="https://github.com/hanghuacs/MMIG-Bench" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/hanghuacs/MMIG-Bench?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hua2025mmigbench</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MMIG-Bench: Towards Comprehensive and Explainable Evaluation of Multi-Modal Image Generation Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hua, Hang and Zeng, Ziyun and Song, Yizhi and Tang, Yunlong and He, Liu and Aliaga, Daniel and Xiong, Wei and Luo, Jiebo}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The Thirty-ninth Annual Conference on Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>window.__GS_CITES_FILLED__||(window.__GS_CITES_FILLED__=!0,fetch("https://raw.githubusercontent.com/yunlong10/citation-badge/main/per_article_citations.json",{cache:"no-cache"}).then(function(t){return t.json()}).then(function(t){document.querySelectorAll(".gs-cites").forEach(function(n){var e=n.getAttribute("data-article-id"),c=t&&Object.prototype.hasOwnProperty.call(t,e)?t[e]:0;n.textContent=String(c)})})["catch"](function(){document.querySelectorAll(".gs-cites").forEach(function(t){t.textContent="N/A"})}));</script> </li> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/zerosep-480.webp 480w,/assets/img/publication_preview/zerosep-800.webp 800w,/assets/img/publication_preview/zerosep-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/zerosep.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="zerosep.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="huang2025zerosep" class="col-sm-8"> <div class="title"> <a href="https://openreview.net/forum?id=IIjiNTR1cV" rel="external nofollow noopener" target="_blank">ZeroSep: Separate Anything in Audio with Zero Training</a> </div> <div class="author"> Chao Huang,Â Yuesheng Ma,Â Junxuan Huang,Â Susan Liang,Â <em>Yunlong Tang</em>,Â Jing Bi,Â Wenqiang Liu,Â Nima Mesgarani,Â andÂ Chenliang Xu </div> <div class="periodical"> <em>The Thirty-ninth Annual Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://wikichao.github.io/ZeroSep/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite </a> <a href="https://github.com/WikiChao/ZeroSep" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/WikiChao/ZeroSep?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="abstract hidden"> <p> Audio source separation is fundamental for machines to understand complex acoustic environments and underpins numerous audio applications. Current supervised deep learning approaches, while powerful, are limited by the need for extensive, task-specific labeled data and struggle to generalize to the immense variability and open-set nature of real-world acoustic scenes. Inspired by the success of generative foundation models, we investigate whether pre-trained text-guided audio diffusion models can overcome these limitations. We make a surprising discovery: zero-shot source separation can be achieved purely through a pre-trained text-guided audio diffusion model under the right configuration. Our method, named ZeroSep, works by inverting the mixed audio into the diffusion modelâ€™s latent space and then using text conditioning to guide the denoising process to recover individual sources. Without any task-specific training or fine-tuning, ZeroSep repurposes the generative diffusion model for a discriminative separation task and inherently supports open-set scenarios through its rich textual priors. ZeroSep is compatible with a variety of pre-trained text-guided audio diffusion backbones and delivers strong separation performance on multiple separation benchmarks, surpassing even supervised methods. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">huang2025zerosep</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ZeroSep: Separate Anything in Audio with Zero Training}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huang, Chao and Ma, Yuesheng and Huang, Junxuan and Liang, Susan and Tang, Yunlong and Bi, Jing and Liu, Wenqiang and Mesgarani, Nima and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The Thirty-ninth Annual Conference on Neural Information Processing Systems (NeurIPS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>window.__GS_CITES_FILLED__||(window.__GS_CITES_FILLED__=!0,fetch("https://raw.githubusercontent.com/yunlong10/citation-badge/main/per_article_citations.json",{cache:"no-cache"}).then(function(t){return t.json()}).then(function(t){document.querySelectorAll(".gs-cites").forEach(function(n){var e=n.getAttribute("data-article-id"),c=t&&Object.prototype.hasOwnProperty.call(t,e)?t[e]:0;n.textContent=String(c)})})["catch"](function(){document.querySelectorAll(".gs-cites").forEach(function(t){t.textContent="N/A"})}));</script> </li> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/harnessing-480.webp 480w,/assets/img/publication_preview/harnessing-800.webp 800w,/assets/img/publication_preview/harnessing-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/harnessing.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="harnessing.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="liu2025harnessing" class="col-sm-8"> <div class="title"> <a href="https://openreview.net/forum?id=ZW2BADJKGU" rel="external nofollow noopener" target="_blank">Harnessing the Computation Redundancy in ViTs to Boost Adversarial Transferability</a> </div> <div class="author"> Jiani Liu,Â Zhiyuan Wang,Â Zeliang Zhang,Â Chao Huang,Â Susan Liang,Â <em>Yunlong Tang</em>,Â andÂ Chenliang Xu </div> <div class="periodical"> <em>The Thirty-ninth Annual Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2504.10804" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/pdf/harnessing.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite </a> </div> <div class="abstract hidden"> <p> Vision Transformers (ViTs) have demonstrated remarkable success in computer vision tasks, yet their vulnerability to adversarial attacks remains a critical concern. While existing methods focus on improving adversarial robustness through training strategies, we explore a novel perspective: leveraging the inherent computation redundancy in ViTs to enhance adversarial transferability. We observe that ViTs contain significant computational redundancy across different layers and attention heads, which can be strategically exploited to generate more transferable adversarial examples. Our approach, named Harnessing Redundancy for Transferability (HRT), identifies and utilizes the most influential computation paths in ViTs to craft adversarial examples that are more likely to transfer across different models and architectures. Through extensive experiments on ImageNet and CIFAR-10, we demonstrate that HRT significantly improves adversarial transferability compared to existing methods, achieving up to 15% higher success rates in black-box attacks. Our findings provide new insights into the relationship between model architecture and adversarial robustness, opening new directions for both attack and defense strategies. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2025harnessing</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Harnessing the Computation Redundancy in ViTs to Boost Adversarial Transferability}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Jiani and Wang, Zhiyuan and Zhang, Zeliang and Huang, Chao and Liang, Susan and Tang, Yunlong and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The Thirty-ninth Annual Conference on Neural Information Processing Systems (NeurIPS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>window.__GS_CITES_FILLED__||(window.__GS_CITES_FILLED__=!0,fetch("https://raw.githubusercontent.com/yunlong10/citation-badge/main/per_article_citations.json",{cache:"no-cache"}).then(function(t){return t.json()}).then(function(t){document.querySelectorAll(".gs-cites").forEach(function(n){var e=n.getAttribute("data-article-id"),c=t&&Object.prototype.hasOwnProperty.call(t,e)?t[e]:0;n.textContent=String(c)})})["catch"](function(){document.querySelectorAll(".gs-cites").forEach(function(t){t.textContent="N/A"})}));</script> </li> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICCV</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/pi_avas-480.webp 480w,/assets/img/publication_preview/pi_avas-800.webp 800w,/assets/img/publication_preview/pi_avas-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/pi_avas.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="pi_avas.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="liang2025pavas" class="col-sm-8"> <div class="title"> <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Liang_p-AVAS_Can_Physics-Integrated_Audio-Visual_Modeling_Boost_Neural_Acoustic_Synthesis_ICCV_2025_paper.html" rel="external nofollow noopener" target="_blank">p-AVAS: Can Physics-Integrated Audio-Visual Modeling Boost Neural Acoustic Synthesis?</a> </div> <div class="author"> Susan Liang,Â Chao Huang,Â <em>Yunlong Tang</em>,Â Zeliang Zhang,Â andÂ Chenliang Xu </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Liang_p-AVAS_Can_Physics-Integrated_Audio-Visual_Modeling_Boost_Neural_Acoustic_Synthesis_ICCV_2025_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://liangsusan-git.github.io/project/pi_avas/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite </a> </div> <div class="abstract hidden"> <p> The Audio-Visual Acoustic Synthesis (AVAS) task aims to model realistic audio propagation behavior within a specific visual scene. Prior works often rely on sparse image representations to guide acoustic synthesis. However, we argue that this approach is insufficient to capture the intricate physical properties of the environment and may struggle with generalization across diverse scenes. In this work, we review the limitations of existing pipelines and address the research question: Can we leverage physical audio-visual associations to enhance neural acoustic synthesis? We introduce Physics-Integrated Audio-Visual Acoustic Synthesis (PI-AVAS or \pi-AVAS), a novel framework designed with two key objectives. i) Generalization: We develop a vision-guided audio simulation framework that leverages physics-based sound propagation. By explicitly modeling vision-grounded geometry and sound rays, our approach achieves robust performance across diverse visual environments. ii) Realism: While simulation-based approaches offer generalizability, they often compromise on realism. To mitigate this, we incorporate a second stage for data-centric refinement, where we propose a flow matching-based audio refinement model to narrow the gap between simulation and real-world audio-visual scenes. Extensive experiments demonstrate the effectiveness and robustness of our method. We achieve state-of-the-art performance on the RWAVS-Gen, RWAVS, and RAF datasets. Additionally, we show that our approach can be seamlessly integrated with existing methods to significantly improve their performance. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">liang2025pavas</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liang, Susan and Huang, Chao and Tang, Yunlong and Zhang, Zeliang and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{p-AVAS: Can Physics-Integrated Audio-Visual Modeling Boost Neural Acoustic Synthesis?}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{13942-13951}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>window.__GS_CITES_FILLED__||(window.__GS_CITES_FILLED__=!0,fetch("https://raw.githubusercontent.com/yunlong10/citation-badge/main/per_article_citations.json",{cache:"no-cache"}).then(function(t){return t.json()}).then(function(t){document.querySelectorAll(".gs-cites").forEach(function(n){var e=n.getAttribute("data-article-id"),c=t&&Object.prototype.hasOwnProperty.call(t,e)?t[e]:0;n.textContent=String(c)})})["catch"](function(){document.querySelectorAll(".gs-cites").forEach(function(t){t.textContent="N/A"})}));</script> </li> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICCV Workshop</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/teaser-ai4anime-480.webp 480w,/assets/img/publication_preview/teaser-ai4anime-800.webp 800w,/assets/img/publication_preview/teaser-ai4anime-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/teaser-ai4anime.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="teaser-ai4anime.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="tang2025ai4anime" class="col-sm-8"> <div class="title"> <a href="https://openaccess.thecvf.com/content/ICCV2025W/AISTORY/html/Tang_Generative_AI_for_Cel-Animation_A_Survey_ICCVW_2025_paper.html" rel="external nofollow noopener" target="_blank">Generative AI for Cel-Animation: A Survey ðŸ”¥ðŸ”¥ðŸ”¥</a> </div> <div class="author"> <em>Yunlong Tang</em>,Â Junjia Guo,Â Pinxin Liu,Â Zhiyuan Wang,Â Hang Hua,Â Jia-Xing Zhong,Â Yunzhong Xiao,Â Chao Huang,Â Luchuan Song,Â Susan Liang, and <span class="more-authors" title="click to view 7 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '7 more authors' ? 'Yizhi Song, Liu He, Jing Bi, Mingqian Feng, Xinyang Li, Zeliang Zhang, Chenliang Xu' : '7 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">7 more authors</span> </div> <div class="periodical"> <em>Proceedings of the IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>) Workshops</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openaccess.thecvf.com/content/ICCV2025W/AISTORY/papers/Tang_Generative_AI_for_Cel-Animation_A_Survey_ICCVW_2025_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/pdf/ai4anime.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite (Â <i class="fas fa-quote-right" style="font-size: 1.1em; margin-right: 0px; -webkit-text-stroke: 1px currentColor; -webkit-text-fill-color: transparent;"></i> <span class="gs-cites" data-article-id="Se3iqnhoufwC" style="color: inherit;">â€¦</span>Â ) </a> <a href="https://github.com/yunlong10/Awesome-AI4Animation" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/yunlong10/Awesome-AI4Animation?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tang2025ai4anime</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Generative AI for Cel-Animation: A Survey ðŸ”¥ðŸ”¥ðŸ”¥}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tang, Yunlong and Guo, Junjia and Liu, Pinxin and Wang, Zhiyuan and Hua, Hang and Zhong, Jia-Xing and Xiao, Yunzhong and Huang, Chao and Song, Luchuan and Liang, Susan and Song, Yizhi and He, Liu and Bi, Jing and Feng, Mingqian and Li, Xinyang and Zhang, Zeliang and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3778-3791}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>window.__GS_CITES_FILLED__||(window.__GS_CITES_FILLED__=!0,fetch("https://raw.githubusercontent.com/yunlong10/citation-badge/main/per_article_citations.json",{cache:"no-cache"}).then(function(t){return t.json()}).then(function(t){document.querySelectorAll(".gs-cites").forEach(function(n){var e=n.getAttribute("data-article-id"),c=t&&Object.prototype.hasOwnProperty.call(t,e)?t[e]:0;n.textContent=String(c)})})["catch"](function(){document.querySelectorAll(".gs-cites").forEach(function(t){t.textContent="N/A"})}));</script> </li> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">TCSVT</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/vidllm_survey-480.webp 480w,/assets/img/publication_preview/vidllm_survey-800.webp 800w,/assets/img/publication_preview/vidllm_survey-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/vidllm_survey.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="vidllm_survey.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="vidllmsurvey" class="col-sm-8"> <div class="title"> <a href="https://ieeexplore.ieee.org/document/10982110" rel="external nofollow noopener" target="_blank">Video Understanding with Large Language Models: A Survey ðŸ”¥ðŸ”¥ðŸ”¥</a> </div> <div class="author"> <em>Yunlong Tang</em>,Â Jing Bi,Â Siting Xu,Â Luchuan Song,Â Susan Liang,Â Teng Wang,Â Daoan Zhang,Â Jie An,Â Jingyang Lin,Â Rongyi Zhu, and <span class="more-authors" title="click to view 10 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '10 more authors' ? 'Ali Vosoughi, Chao Huang, Zeliang Zhang, Pinxin Liu, Mingqian Feng, Feng Zheng, Jianguo Zhang, Ping Luo, Jiebo Luo, Chenliang Xu&lt;sup&gt;â€ &lt;/sup&gt;' : '10 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">10 more authors</span> </div> <div class="periodical"> <em>IEEE Transactions on Circuits and Systems for Video Technology (<strong>TCSVT</strong>)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/document/10982110" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite (Â <i class="fas fa-quote-right" style="font-size: 1.1em; margin-right: 0px; -webkit-text-stroke: 1px currentColor; -webkit-text-fill-color: transparent;"></i> <span class="gs-cites" data-article-id="qjMakFHDy7sC" style="color: inherit;">â€¦</span>Â ) </a> <a href="https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/yunlong10/Awesome-LLMs-for-Video-Understanding?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="abstract hidden"> <p> With the burgeoning growth of online video platforms and the escalating volume of video content, the demand for proficient video understanding tools has intensified markedly. Given the remarkable capabilities of large language models (LLMs) in language and multimodal tasks, this survey provides a detailed overview of recent advancements in video understanding that harness the power of LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly advanced, particularly their ability for open-ended multi-granularity (general, temporal, and spatiotemporal) reasoning combined with commonsense knowledge, suggesting a promising path for future video understanding. We examine the unique characteristics and capabilities of Vid-LLMs, categorizing the approaches into three main types: Video Analyzer x LLM, Video Embedder x LLM, and (Analyzer + Embedder) x LLM. Furthermore, we identify five sub-types based on the functions of LLMs in Vid-LLMs: LLM as Summarizer, LLM as Manager, LLM as Text Decoder, LLM as Regressor, and LLM as Hidden Layer. Furthermore, this survey presents a comprehensive study of the tasks, datasets, benchmarks, and evaluation methodologies for Vid-LLMs. Additionally, it explores the expansive applications of Vid-LLMs across various domains, highlighting their remarkable scalability and versatility in real-world video understanding challenges. Finally, it summarizes the limitations of existing Vid-LLMs and outlines directions for future research. For more information, readers are recommended to visit the repository at https://github.com/yunlong10/awesome-llms-for-video-understanding. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">vidllmsurvey</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Video Understanding with Large Language Models: A Survey ðŸ”¥ðŸ”¥ðŸ”¥}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tang, Yunlong and Bi, Jing and Xu, Siting and Song, Luchuan and Liang, Susan and Wang, Teng and Zhang, Daoan and An, Jie and Lin, Jingyang and Zhu, Rongyi and Vosoughi, Ali and Huang, Chao and Zhang, Zeliang and Liu, Pinxin and Feng, Mingqian and Zheng, Feng and Zhang, Jianguo and Luo, Ping and Luo, Jiebo and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>window.__GS_CITES_FILLED__||(window.__GS_CITES_FILLED__=!0,fetch("https://raw.githubusercontent.com/yunlong10/citation-badge/main/per_article_citations.json",{cache:"no-cache"}).then(function(t){return t.json()}).then(function(t){document.querySelectorAll(".gs-cites").forEach(function(n){var e=n.getAttribute("data-article-id"),c=t&&Object.prototype.hasOwnProperty.call(t,e)?t[e]:0;n.textContent=String(c)})})["catch"](function(){document.querySelectorAll(".gs-cites").forEach(function(t){t.textContent="N/A"})}));</script> </li> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CVPR</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/vidcomposition-480.webp 480w,/assets/img/publication_preview/vidcomposition-800.webp 800w,/assets/img/publication_preview/vidcomposition-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/vidcomposition.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="vidcomposition.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="tang2024vidcompostion" class="col-sm-8"> <div class="title"> <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Tang_VidComposition_Can_MLLMs_Analyze_Compositions_in_Compiled_Videos_CVPR_2025_paper.html" rel="external nofollow noopener" target="_blank">VidComposition: Can MLLMs Analyze Compositions in Compiled Videos?</a> </div> <div class="author"> <em>Yunlong Tang<sup>*</sup></em>,Â Junjia Guo<sup>*</sup>,Â Hang Hua,Â Susan Liang,Â Mingqian Feng,Â Xinyang Li,Â Rui Mao,Â Chao Huang,Â Jing Bi,Â Zeliang Zhang, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Pooyan Fazli, Chenliang Xu&lt;sup&gt;â€ &lt;/sup&gt;' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the Computer Vision and Pattern Recognition Conference (<strong>CVPR</strong>)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://yunlong10.github.io/VidComposition/" class="btn btn-sm z-depth-0" role="button">Website</a> <a href="/assets/pdf/vidcomp.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite (Â <i class="fas fa-quote-right" style="font-size: 1.1em; margin-right: 0px; -webkit-text-stroke: 1px currentColor; -webkit-text-fill-color: transparent;"></i> <span class="gs-cites" data-article-id="_FxGoFyzp5QC" style="color: inherit;">â€¦</span>Â ) </a> <a href="https://github.com/yunlong10/VidComposition" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/yunlong10/VidComposition?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tang2024vidcompostion</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{VidComposition: Can MLLMs Analyze Compositions in Compiled Videos?}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tang, Yunlong and Guo, Junjia and Hua, Hang and Liang, Susan and Feng, Mingqian and Li, Xinyang and Mao, Rui and Huang, Chao and Bi, Jing and Zhang, Zeliang and Fazli, Pooyan and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{8490-8500}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>window.__GS_CITES_FILLED__||(window.__GS_CITES_FILLED__=!0,fetch("https://raw.githubusercontent.com/yunlong10/citation-badge/main/per_article_citations.json",{cache:"no-cache"}).then(function(t){return t.json()}).then(function(t){document.querySelectorAll(".gs-cites").forEach(function(n){var e=n.getAttribute("data-article-id"),c=t&&Object.prototype.hasOwnProperty.call(t,e)?t[e]:0;n.textContent=String(c)})})["catch"](function(){document.querySelectorAll(".gs-cites").forEach(function(t){t.textContent="N/A"})}));</script> </li> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CVPR</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/unveiling-480.webp 480w,/assets/img/publication_preview/unveiling-800.webp 800w,/assets/img/publication_preview/unveiling-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/unveiling.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="unveiling.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="bi2024unveiling" class="col-sm-8"> <div class="title"> <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Bi_Unveiling_Visual_Perception_in_Language_Models_An_Attention_Head_Analysis_CVPR_2025_paper.html" rel="external nofollow noopener" target="_blank">Unveiling Visual Perception in Language Models: An Attention Head Analysis Approach</a> </div> <div class="author"> Jing Bi,Â Junjia Guo,Â <em>Yunlong Tang</em>,Â Lianggong Bruce Wen,Â Zhang Liu,Â andÂ Chenliang Xu </div> <div class="periodical"> <em>In Proceedings of the Computer Vision and Pattern Recognition Conference (<strong>CVPR</strong>)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://page-visual-head.jing.vision/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="/assets/pdf/unveiling.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite </a> <a href="https://github.com/jing-bi/visual-head" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/jing-bi/visual-head?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">bi2024unveiling</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Unveiling Visual Perception in Language Models: An Attention Head Analysis Approach}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bi, Jing and Guo, Junjia and Tang, Yunlong and Wen, Lianggong Bruce and Liu, Zhang and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>window.__GS_CITES_FILLED__||(window.__GS_CITES_FILLED__=!0,fetch("https://raw.githubusercontent.com/yunlong10/citation-badge/main/per_article_citations.json",{cache:"no-cache"}).then(function(t){return t.json()}).then(function(t){document.querySelectorAll(".gs-cites").forEach(function(n){var e=n.getAttribute("data-article-id"),c=t&&Object.prototype.hasOwnProperty.call(t,e)?t[e]:0;n.textContent=String(c)})})["catch"](function(){document.querySelectorAll(".gs-cites").forEach(function(t){t.textContent="N/A"})}));</script> </li> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">AAAI</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/teaser-avicuna-480.webp 480w,/assets/img/publication_preview/teaser-avicuna-800.webp 800w,/assets/img/publication_preview/teaser-avicuna-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/teaser-avicuna.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="teaser-avicuna.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="tang2024avicuna" class="col-sm-8"> <div class="title"> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/32784" rel="external nofollow noopener" target="_blank">Empowering LLMs with Pseudo-Untrimmed Videos for Audio-Visual Temporal Understanding</a> </div> <div class="author"> <em>Yunlong Tang</em>,Â Daiki Shimada,Â Jing Bi,Â Mingqian Feng,Â Hang Hua,Â andÂ Chenliang Xu<sup>â€ </sup> </div> <div class="periodical"> <em>In Proceedings of the AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/avicuna.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite (Â <i class="fas fa-quote-right" style="font-size: 1.1em; margin-right: 0px; -webkit-text-stroke: 1px currentColor; -webkit-text-fill-color: transparent;"></i> <span class="gs-cites" data-article-id="IjCSPb-OGe4C" style="color: inherit;">â€¦</span>Â ) </a> <a href="https://github.com/yunlong10/AVicuna" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/yunlong10/AVicuna?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="abstract hidden"> <p> Large language models (LLMs) have demonstrated remarkable capabilities in natural language and multimodal domains. By fine-tuning multimodal LLMs with temporal annotations from well-annotated datasets, e.g., dense video captioning datasets, their temporal understanding capacity in video-language tasks can be obtained. However, there is a notable lack of untrimmed audio-visual video datasets with precise temporal annotations for events. This deficiency hinders LLMs from learning the alignment between time, audio-visual events, and text tokens, thus impairing their ability to localize audio-visual events in videos temporally. To address this gap, we introduce PU-VALOR, a comprehensive audio-visual dataset comprising over 114,000 pseudo-untrimmed videos with detailed temporal annotations. PU-VALOR is derived from the large-scale but coarse-annotated audio-visual dataset VALOR, through a subtle method involving event-based video clustering, random temporal scaling, and permutation. By fine-tuning a multimodal LLM on PU-VALOR, we developed AVicuna, a model capable of aligning audio-visual events with temporal intervals and corresponding text tokens. AVicuna excels in temporal localization and time-aware dialogue capabilities. Our experiments demonstrate that AVicuna effectively handles temporal understanding in audio-visual videos and achieves state-of-the-art performance on open-ended video QA, audio-visual QA, and audio-visual event dense localization tasks. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tang2024avicuna</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Empowering LLMs with Pseudo-Untrimmed Videos for Audio-Visual Temporal Understanding}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tang, Yunlong and Shimada, Daiki and Bi, Jing and Feng, Mingqian and Hua, Hang and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{39}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{7293-7301}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1609/aaai.v39i7.32784}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>window.__GS_CITES_FILLED__||(window.__GS_CITES_FILLED__=!0,fetch("https://raw.githubusercontent.com/yunlong10/citation-badge/main/per_article_citations.json",{cache:"no-cache"}).then(function(t){return t.json()}).then(function(t){document.querySelectorAll(".gs-cites").forEach(function(n){var e=n.getAttribute("data-article-id"),c=t&&Object.prototype.hasOwnProperty.call(t,e)?t[e]:0;n.textContent=String(c)})})["catch"](function(){document.querySelectorAll(".gs-cites").forEach(function(t){t.textContent="N/A"})}));</script> </li> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">AAAI</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/v2xum-llama-480.webp 480w,/assets/img/publication_preview/v2xum-llama-800.webp 800w,/assets/img/publication_preview/v2xum-llama-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/v2xum-llama.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="v2xum-llama.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hua2024v2xum" class="col-sm-8"> <div class="title"> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/32374" rel="external nofollow noopener" target="_blank">V2Xum-LLM: Cross-modal Video Summarization with Temporal Prompt Instruction Tuning</a> </div> <div class="author"> Hang Hua<sup>*</sup>,Â <em>Yunlong Tang<sup>*</sup></em>,Â Chenliang Xu,Â andÂ Jiebo Luo<sup>â€ </sup> </div> <div class="periodical"> <em>In Proceedings of the AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://hanghuacs.github.io/v2xum/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="/assets/pdf/v2xum.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite (Â <i class="fas fa-quote-right" style="font-size: 1.1em; margin-right: 0px; -webkit-text-stroke: 1px currentColor; -webkit-text-fill-color: transparent;"></i> <span class="gs-cites" data-article-id="zYLM7Y9cAGgC" style="color: inherit;">â€¦</span>Â ) </a> <a href="https://github.com/hanghuacs/V2Xum-LLM" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/hanghuacs/V2Xum-LLM?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">hua2024v2xum</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{V2Xum-LLM: Cross-modal Video Summarization with Temporal Prompt Instruction Tuning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hua, Hang and Tang, Yunlong and Xu, Chenliang and Luo, Jiebo}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{39}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3599-3607}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1609/aaai.v39i4.32374}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>window.__GS_CITES_FILLED__||(window.__GS_CITES_FILLED__=!0,fetch("https://raw.githubusercontent.com/yunlong10/citation-badge/main/per_article_citations.json",{cache:"no-cache"}).then(function(t){return t.json()}).then(function(t){document.querySelectorAll(".gs-cites").forEach(function(n){var e=n.getAttribute("data-article-id"),c=t&&Object.prototype.hasOwnProperty.call(t,e)?t[e]:0;n.textContent=String(c)})})["catch"](function(){document.querySelectorAll(".gs-cites").forEach(function(t){t.textContent="N/A"})}));</script> </li> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">AAAI</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/cardiff-480.webp 480w,/assets/img/publication_preview/cardiff-800.webp 800w,/assets/img/publication_preview/cardiff-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/cardiff.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cardiff.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="tang2024cardiff" class="col-sm-8"> <div class="title"> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/32785" rel="external nofollow noopener" target="_blank">CaRDiff: Video Salient Object Ranking Chain of Thought Reasoning for Saliency Prediction with Diffusion</a> </div> <div class="author"> <em>Yunlong Tang</em>,Â Gen Zhan,Â Li Yang,Â Yiting Liao,Â andÂ Chenliang Xu<sup>â€ </sup> </div> <div class="periodical"> <em>In Proceedings of the AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/32785" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/pdf/cardiff.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite (Â <i class="fas fa-quote-right" style="font-size: 1.1em; margin-right: 0px; -webkit-text-stroke: 1px currentColor; -webkit-text-fill-color: transparent;"></i> <span class="gs-cites" data-article-id="W7OEmFMy1HYC" style="color: inherit;">â€¦</span>Â ) </a> </div> <div class="abstract hidden"> <p> Video saliency prediction aims to identify the regions in a video that attract human attention and gaze, driven by bottom-up features from the video and top-down processes like memory and cognition. Among these top-down influences, language plays a crucial role in guiding attention by shaping how visual information is interpreted. Existing methods primarily focus on modeling perceptual information while neglecting the reasoning process facilitated by language, where ranking cues are crucial outcomes of this process and practical guidance for saliency prediction. In this paper, we propose CaRDiff (Caption, Rank, and generate with Diffusion), a framework that imitates the process by integrating multimodal large language model (MLLM), a grounding module, and a diffusion model, to enhance video saliency prediction. Specifically, we introduce a novel prompting method VSOR-CoT (Video Salient Object Ranking Chain of Thought), which utilizes an MLLM with a grounding module to caption video content and infer salient objects along with their rankings and positions. This process derives ranking maps that can be sufficiently leveraged by the diffusion model to decode the saliency maps for the given video accurately. Extensive experiments show the effectiveness of VSOR-CoT in improving the performance of video saliency prediction. The proposed CaRDiff performs better than state-of-the-art models on the MVS dataset and demonstrates cross-dataset capabilities on the DHF1k dataset through zero-shot evaluation. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tang2024cardiff</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{CaRDiff: Video Salient Object Ranking Chain of Thought Reasoning for Saliency Prediction with Diffusion}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tang, Yunlong and Zhan, Gen and Yang, Li and Liao, Yiting and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1609/aaai.v39i7.32785}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{39}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{7302-7310}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>window.__GS_CITES_FILLED__||(window.__GS_CITES_FILLED__=!0,fetch("https://raw.githubusercontent.com/yunlong10/citation-badge/main/per_article_citations.json",{cache:"no-cache"}).then(function(t){return t.json()}).then(function(t){document.querySelectorAll(".gs-cites").forEach(function(n){var e=n.getAttribute("data-article-id"),c=t&&Object.prototype.hasOwnProperty.call(t,e)?t[e]:0;n.textContent=String(c)})})["catch"](function(){document.querySelectorAll(".gs-cites").forEach(function(t){t.textContent="N/A"})}));</script> </li> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CVPR Workshop</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/fresca-480.webp 480w,/assets/img/publication_preview/fresca-800.webp 800w,/assets/img/publication_preview/fresca-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/fresca.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="fresca.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="huang2025fresca" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2504.02154" rel="external nofollow noopener" target="_blank">FreSca: Unveiling the Scaling Space in Diffusion Models</a> </div> <div class="author"> Chao Huang,Â Susan Liang,Â <em>Yunlong Tang</em>,Â Li Ma,Â Yapeng Tian,Â andÂ Chenliang Xu<sup>â€ </sup> </div> <div class="periodical"> <em>CVPR GMCV Workshop; arXiv preprint arXiv:2504.02154</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2504.02154" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite </a> <a href="https://github.com/WikiChao/FreSca" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/WikiChao/FreSca?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="abstract hidden"> <p> Diffusion models offer impressive controllability for image tasks, primarily through noise predictions that encode task-specific information and classifier-free guidance enabling adjustable scaling. This scaling mechanism implicitly defines a â€œscaling spaceâ€ whose potential for fine-grained semantic manipulation remains underexplored. We investigate this space, starting with inversion-based editing where the difference between conditional/unconditional noise predictions carries key semantic information. Our core contribution stems from a Fourier analysis of noise predictions, revealing that its low- and high-frequency components evolve differently throughout diffusion. Based on this insight, we introduce FreSca, a straightforward method that applies guidance scaling independently to different frequency bands in the Fourier domain. FreSca demonstrably enhances existing image editing methods without retraining. Excitingly, its effectiveness extends to image understanding tasks such as depth estimation, yielding quantitative gains across multiple datasets. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">huang2025fresca</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{FreSca: Unveiling the Scaling Space in Diffusion Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huang, Chao and Liang, Susan and Tang, Yunlong and Ma, Li and Tian, Yapeng and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{CVPR GMCV Workshop; arXiv preprint arXiv:2504.02154}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>window.__GS_CITES_FILLED__||(window.__GS_CITES_FILLED__=!0,fetch("https://raw.githubusercontent.com/yunlong10/citation-badge/main/per_article_citations.json",{cache:"no-cache"}).then(function(t){return t.json()}).then(function(t){document.querySelectorAll(".gs-cites").forEach(function(n){var e=n.getAttribute("data-article-id"),c=t&&Object.prototype.hasOwnProperty.call(t,e)?t[e]:0;n.textContent=String(c)})})["catch"](function(){document.querySelectorAll(".gs-cites").forEach(function(t){t.textContent="N/A"})}));</script> </li> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CVPR Workshop</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/kubrick-480.webp 480w,/assets/img/publication_preview/kubrick-800.webp 800w,/assets/img/publication_preview/kubrick-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/kubrick.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="kubrick.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="kubrick2025" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2408.10453" rel="external nofollow noopener" target="_blank">Kubrick: Multimodal Agent Collaborations for Synthetic Video Generation</a> </div> <div class="author"> Liu He,Â Yizhi Song,Â Hejun Huang,Â Pinxin Liu,Â <em>Yunlong Tang</em>,Â Daniel Aliaga,Â andÂ Xin Zhou </div> <div class="periodical"> <em>CVPR AI4CC Workshop; arXiv preprint arXiv:2408.10453</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2408.10453" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite </a> </div> <div class="abstract hidden"> <p> Text-to-video generation has been dominated by diffusion-based or autoregressive models. These novel models provide plausible versatility, but are criticized for improper physical motion, shading and illumination, camera motion, and temporal consistency. The film industry relies on manually-edited Computer-Generated Imagery (CGI) using 3D modeling software. Human-directed 3D synthetic videos address these shortcomings, but require tight collaboration between movie makers and 3D rendering experts. We introduce an automatic synthetic video generation pipeline based on Vision Large Language Model (VLM) agent collaborations. Given a language description of a video, multiple VLM agents direct various processes of the generation pipeline. They cooperate to create Blender scripts which render a video following the given description. Augmented with Blender-based movie making knowledge, the Director agent decomposes the text-based video description into sub-processes. For each sub-process, the Programmer agent produces Python-based Blender scripts based on function composing and API calling. The Reviewer agent, with knowledge of video reviewing, character motion coordinates, and intermediate screenshots, provides feedback to the Programmer agent. The Programmer agent iteratively improves scripts to yield the best video outcome. Our generated videos show better quality than commercial video generation models in five metrics on video quality and instruction-following performance. Our framework outperforms other approaches in a user study on quality, consistency, and rationality. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">kubrick2025</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Kubrick: Multimodal Agent Collaborations for Synthetic Video Generation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{He, Liu and Song, Yizhi and Huang, Hejun and Liu, Pinxin and Tang, Yunlong and Aliaga, Daniel and Zhou, Xin}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{CVPR AI4CC Workshop; arXiv preprint arXiv:2408.10453}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>window.__GS_CITES_FILLED__||(window.__GS_CITES_FILLED__=!0,fetch("https://raw.githubusercontent.com/yunlong10/citation-badge/main/per_article_citations.json",{cache:"no-cache"}).then(function(t){return t.json()}).then(function(t){document.querySelectorAll(".gs-cites").forEach(function(n){var e=n.getAttribute("data-article-id"),c=t&&Object.prototype.hasOwnProperty.call(t,e)?t[e]:0;n.textContent=String(c)})})["catch"](function(){document.querySelectorAll(".gs-cites").forEach(function(t){t.textContent="N/A"})}));</script> </li> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">3DV</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/gaussian_style-480.webp 480w,/assets/img/publication_preview/gaussian_style-800.webp 800w,/assets/img/publication_preview/gaussian_style-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/gaussian_style.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="gaussian_style.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="liu2024emo" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2402.00827" rel="external nofollow noopener" target="_blank">GaussianStyle: Gaussian Head Avatar via StyleGAN</a> </div> <div class="author"> Pinxin Liu,Â Luchuan Song,Â Daoan Zhang,Â Hang Hua,Â <em>Yunlong Tang</em>,Â Huaijin Tu,Â Jiebo Luo,Â andÂ Chenliang Xu<sup>â€ </sup> </div> <div class="periodical"> <em>International Conference on 3D Vision (<strong>3DV</strong>)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2402.00827" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite </a> <a href="https://github.com/andypinxinliu/HumanFaceProject" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/andypinxinliu/HumanFaceProject?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="abstract hidden"> <p> Existing methods like Neural Radiation Fields (NeRF) and 3D Gaussian Splatting (3DGS) have made significant strides in facial attribute control such as facial animation and components editing, yet they struggle with fine-grained representation and scalability in dynamic head modeling. To address these limitations, we propose GaussianStyle, a novel framework that integrates the volumetric strengths of 3DGS with the powerful implicit representation of StyleGAN. The GaussianStyle preserves structural information, such as expressions and poses, using Gaussian points, while projecting the implicit volumetric representation into StyleGAN to capture high-frequency details and mitigate the over-smoothing commonly observed in neural texture rendering. Experimental outcomes indicate that our method achieves state-of-the-art performance in reenactment, novel view synthesis, and animation. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2024emo</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{GaussianStyle: Gaussian Head Avatar via StyleGAN}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Pinxin and Song, Luchuan and Zhang, Daoan and Hua, Hang and Tang, Yunlong and Tu, Huaijin and Luo, Jiebo and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on 3D Vision (3DV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>window.__GS_CITES_FILLED__||(window.__GS_CITES_FILLED__=!0,fetch("https://raw.githubusercontent.com/yunlong10/citation-badge/main/per_article_citations.json",{cache:"no-cache"}).then(function(t){return t.json()}).then(function(t){document.querySelectorAll(".gs-cites").forEach(function(n){var e=n.getAttribute("data-article-id"),c=t&&Object.prototype.hasOwnProperty.call(t,e)?t[e]:0;n.textContent=String(c)})})["catch"](function(){document.querySelectorAll(".gs-cites").forEach(function(t){t.textContent="N/A"})}));</script> </li> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/reasoning_survey-480.webp 480w,/assets/img/publication_preview/reasoning_survey-800.webp 800w,/assets/img/publication_preview/reasoning_survey-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/reasoning_survey.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="reasoning_survey.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="bi2025mllmreasoning" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2504.03151" rel="external nofollow noopener" target="_blank">Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning</a> </div> <div class="author"> Jing Bi,Â Susan Liang,Â Xiaofei Zhou,Â Pinxin Liu,Â Junjia Guo,Â <em>Yunlong Tang</em>,Â Luchuan Song,Â Chao Huang,Â Guangyu Sun,Â Jinxi He, and <span class="more-authors" title="click to view 8 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '8 more authors' ? 'Jiarui Wu, Shu Yang, Daoan Zhang, Chen Chen, Lianggong Bruce Wen, Zhang Liu, Jiebo Luo, Chenliang Xu' : '8 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">8 more authors</span> </div> <div class="periodical"> <em>arXiv preprint arXiv:2504.03151</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2504.03151" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://reason.jing.vision/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="https://github.com/jing-bi/awesome-M.LLM-reasoning" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/jing-bi/awesome-M.LLM-reasoning?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="abstract hidden"> <p> Reasoning is central to human intelligence, enabling structured problem-solving across diverse tasks. Recent advances in large language models (LLMs) have greatly enhanced their reasoning abilities in arithmetic, commonsense, and symbolic domains. However, effectively extending these capabilities into multimodal contexts-where models must integrate both visual and textual inputs-continues to be a significant challenge. Multimodal reasoning introduces complexities, such as handling conflicting information across modalities, which require models to adopt advanced interpretative strategies. Addressing these challenges involves not only sophisticated algorithms but also robust methodologies for evaluating reasoning accuracy and coherence. This paper offers a concise yet insightful overview of reasoning techniques in both textual and multimodal LLMs. Through a thorough and up-to-date comparison, we clearly formulate core reasoning challenges and opportunities, highlighting practical methods for post-training optimization and test-time inference. Our work provides valuable insights and guidance, bridging theoretical frameworks and practical implementations, and sets clear directions for future research. </p> </div> </div> </div> <script>window.__GS_CITES_FILLED__||(window.__GS_CITES_FILLED__=!0,fetch("https://raw.githubusercontent.com/yunlong10/citation-badge/main/per_article_citations.json",{cache:"no-cache"}).then(function(t){return t.json()}).then(function(t){document.querySelectorAll(".gs-cites").forEach(function(n){var e=n.getAttribute("data-article-id"),c=t&&Object.prototype.hasOwnProperty.call(t,e)?t[e]:0;n.textContent=String(c)})})["catch"](function(){document.querySelectorAll(".gs-cites").forEach(function(t){t.textContent="N/A"})}));</script> </li> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/verify-480.webp 480w,/assets/img/publication_preview/verify-800.webp 800w,/assets/img/publication_preview/verify-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/verify.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="verify.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="bi2025verify" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2503.11557" rel="external nofollow noopener" target="_blank">VERIFY: A Benchmark of Visual Explanation and Reasoning for Investigating Multimodal Reasoning Fidelity</a> </div> <div class="author"> Jing Bi,Â Junjia Guo,Â Susan Liang,Â Guangyu Sun,Â Luchuan Song,Â <em>Yunlong Tang</em>,Â Jinxi He,Â Jiarui Wu,Â Ali Vosoughi,Â Chen Chen, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Chenliang Xu' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>arXiv preprint arXiv:2503.11557</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2503.11557" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://verify-eqh.pages.dev/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite </a> </div> <div class="abstract hidden"> <p> Visual reasoning is central to human cognition, enabling individuals to interpret and abstractly understand their environment. Although recent Multimodal Large Language Models (MLLMs) have demonstrated impressive performance across language and vision-language tasks, existing benchmarks primarily measure recognition-based skills and inadequately assess true visual reasoning capabilities. To bridge this critical gap, we introduce VERIFY, a benchmark explicitly designed to isolate and rigorously evaluate the visual reasoning capabilities of state-of-the-art MLLMs. VERIFY compels models to reason primarily from visual information, providing minimal textual context to reduce reliance on domain-specific knowledge and linguistic biases. Each problem is accompanied by a human-annotated reasoning path, making it the first to provide in-depth evaluation of model decision-making processes. Additionally, we propose novel metrics that assess visual reasoning fidelity beyond mere accuracy, highlighting critical imbalances in current model reasoning patterns. Our comprehensive benchmarking of leading MLLMs uncovers significant limitations, underscoring the need for a balanced and holistic approach to both perception and reasoning. For more teaser and testing, visit our project page at https://verify-eqh.pages.dev/ </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">bi2025verify</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{VERIFY: A Benchmark of Visual Explanation and Reasoning for Investigating Multimodal Reasoning Fidelity}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bi, Jing and Guo, Junjia and Liang, Susan and Sun, Guangyu and Song, Luchuan and Tang, Yunlong and He, Jinxi and Wu, Jiarui and Vosoughi, Ali and Chen, Chen and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2503.11557}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>window.__GS_CITES_FILLED__||(window.__GS_CITES_FILLED__=!0,fetch("https://raw.githubusercontent.com/yunlong10/citation-badge/main/per_article_citations.json",{cache:"no-cache"}).then(function(t){return t.json()}).then(function(t){document.querySelectorAll(".gs-cites").forEach(function(n){var e=n.getAttribute("data-article-id"),c=t&&Object.prototype.hasOwnProperty.call(t,e)?t[e]:0;n.textContent=String(c)})})["catch"](function(){document.querySelectorAll(".gs-cites").forEach(function(t){t.textContent="N/A"})}));</script> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ACM MM</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/teaser-eagle-480.webp 480w,/assets/img/publication_preview/teaser-eagle-800.webp 800w,/assets/img/publication_preview/teaser-eagle-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/teaser-eagle.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="teaser-eagle.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="bi2024eagle" class="col-sm-8"> <div class="title"> <a href="https://dl.acm.org/doi/10.1145/3664647.3681618" rel="external nofollow noopener" target="_blank">EAGLE: Egocentric AGgregated Language-video Engine</a> </div> <div class="author"> Jing Bi,Â <em>Yunlong Tang</em>,Â Luchuan Song,Â Ali Vosoughi,Â Nguyen Nguyen,Â andÂ Chenliang Xu<sup>â€ </sup> </div> <div class="periodical"> <em>In Proceedings of the 32nd ACM International Conference on Multimedia (<strong>ACM MM</strong>)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://page-eagle.jing.vision/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite </a> <a href="https://dl.acm.org/doi/suppl/10.1145/3664647.3681618/suppl_file/3664647.3681618-video.mp4" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p> The rapid evolution of egocentric video analysis brings new insights into understanding human activities and intentions from a first-person perspective. Despite this progress, the fragmentation in tasks like action recognition, procedure learning, and moment retrieval, etc., coupled with inconsistent annotations and isolated model development, hinders a holistic interpretation of video content. In response, we introduce the EAGLE (Egocentric AGgregated Language-video Engine) model and the EAGLE-400K dataset to provide a unified framework that integrates various egocentric video understanding tasks. EAGLE-400K, the first large-scale instruction-tuning dataset tailored for egocentric video, features 400K diverse samples to enhance a broad spectrum task from activity recognition to procedure knowledge learning. Moreover, EAGLE, a strong video-based multimodal large language model (MLLM), is designed to effectively capture both spatial and temporal information. In addition, we propose a set of evaluation metrics designed to facilitate a thorough assessment of MLLM for egocentric video understanding. Our extensive experiments demonstrate EAGLEâ€™s superior performance over existing models, highlighting its ability to balance task-specific understanding with comprehensive video interpretation. With EAGLE, we aim to pave the way for novel research opportunities and practical applications in real-world scenarios. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">bi2024eagle</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{EAGLE: Egocentric AGgregated Language-video Engine}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bi, Jing and Tang, Yunlong and Song, Luchuan and Vosoughi, Ali and Nguyen, Nguyen and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 32nd ACM International Conference on Multimedia (ACM MM)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1682--1691}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3664647.3681618}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>window.__GS_CITES_FILLED__||(window.__GS_CITES_FILLED__=!0,fetch("https://raw.githubusercontent.com/yunlong10/citation-badge/main/per_article_citations.json",{cache:"no-cache"}).then(function(t){return t.json()}).then(function(t){document.querySelectorAll(".gs-cites").forEach(function(n){var e=n.getAttribute("data-article-id"),c=t&&Object.prototype.hasOwnProperty.call(t,e)?t[e]:0;n.textContent=String(c)})})["catch"](function(){document.querySelectorAll(".gs-cites").forEach(function(t){t.textContent="N/A"})}));</script> </li> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ECCV Workshop</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/eccv-aim-480.webp 480w,/assets/img/publication_preview/eccv-aim-800.webp 800w,/assets/img/publication_preview/eccv-aim-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/eccv-aim.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="eccv-aim.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="moskalenko2024aim" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2409.14827" rel="external nofollow noopener" target="_blank">AIM 2024 Challenge on Video Saliency Prediction: Methods and Results</a> </div> <div class="author"> Andrey Moskalenko,Â Alexey Bryncev,Â Dmitry Vatolin,Â Radu Timofte,Â Gen Zhan,Â Li Yang,Â <em>Yunlong Tang</em>,Â Yiting Liao,Â Jiongzhi Lin,Â Baitao Huang, and <span class="more-authors" title="click to view 23 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '23 more authors' ? 'Morteza Moradi, Mohammad Moradi, Francesco Rundo, Concetto Spampinato, Ali Borji, Simone Palazzo, Yuxin Zhu, Yinan Sun, Huiyu Duan, Yuqin Cao, Ziheng Jia, Qiang Hu, Xiongkuo Min, Guangtao Zhai, Hao Fang, Runmin Cong, Xiankai Lu, Xiaofei Zhou, Wei Zhang, Chunyu Zhao, Wentao Mu, Tao Deng, Hamed R Tavakoli' : '23 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">23 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the European Conference on Computer Vision (<strong>ECCV</strong>) Workshops</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://challenges.videoprocessing.ai/challenges/video-saliency-prediction-leaderboard.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite </a> <a href="https://github.com/msu-video-group/ECCVW24_Saliency_Prediction" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/msu-video-group/ECCVW24_Saliency_Prediction?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> <span class="award-badge">Rank 1st</span> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">moskalenko2024aim</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{AIM 2024 Challenge on Video Saliency Prediction: Methods and Results}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Moskalenko, Andrey and Bryncev, Alexey and Vatolin, Dmitry and Timofte, Radu and Zhan, Gen and Yang, Li and Tang, Yunlong and Liao, Yiting and Lin, Jiongzhi and Huang, Baitao and Moradi, Morteza and Moradi, Mohammad and Rundo, Francesco and Spampinato, Concetto and Borji, Ali and Palazzo, Simone and Zhu, Yuxin and Sun, Yinan and Duan, Huiyu and Cao, Yuqin and Jia, Ziheng and Hu, Qiang and Min, Xiongkuo and Zhai, Guangtao and Fang, Hao and Cong, Runmin and Lu, Xiankai and Zhou, Xiaofei and Zhang, Wei and Zhao, Chunyu and Mu, Wentao and Deng, Tao and Tavakoli, Hamed R}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the European Conference on Computer Vision (ECCV) Workshops}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>window.__GS_CITES_FILLED__||(window.__GS_CITES_FILLED__=!0,fetch("https://raw.githubusercontent.com/yunlong10/citation-badge/main/per_article_citations.json",{cache:"no-cache"}).then(function(t){return t.json()}).then(function(t){document.querySelectorAll(".gs-cites").forEach(function(n){var e=n.getAttribute("data-article-id"),c=t&&Object.prototype.hasOwnProperty.call(t,e)?t[e]:0;n.textContent=String(c)})})["catch"](function(){document.querySelectorAll(".gs-cites").forEach(function(t){t.textContent="N/A"})}));</script> </li> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mmcomp-480.webp 480w,/assets/img/publication_preview/mmcomp-800.webp 800w,/assets/img/publication_preview/mmcomp-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/mmcomp.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mmcomp.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hua2024mmcomposition" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2410.09733" rel="external nofollow noopener" target="_blank">MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models</a> </div> <div class="author"> Hang Hua<sup>*</sup>,Â <em>Yunlong Tang<sup>*</sup></em>,Â Ziyun Zeng<sup>*</sup>,Â Liangliang Cao,Â Zhengyuan Yang,Â Hangfeng He,Â Chenliang Xu,Â andÂ Jiebo Luo </div> <div class="periodical"> <em>arXiv preprint arXiv:2410.09733</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://hanghuacs.github.io/MMComposition/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite </a> <a href="https://github.com/hanghuacs/MMComposition" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/hanghuacs/MMComposition?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="abstract hidden"> <p> The advent of large Vision-Language Models (VLMs) has significantly advanced multimodal understanding, enabling more sophisticated and accurate integration of visual and textual information across various tasks, including image and video captioning, visual question answering, and cross-modal retrieval. Despite VLMsâ€™ superior capabilities, researchers lack a comprehensive understanding of their compositionality â€“ the ability to understand and produce novel combinations of known visual and textual components. Prior benchmarks provide only a relatively rough compositionality evaluation from the perspectives of objects, relations, and attributes while neglecting deeper reasoning about object interactions, counting, and complex compositions. However, compositionality is a critical ability that facilitates coherent reasoning and understanding across modalities for VLMs. To address this limitation, we propose MMCOMPOSITION, a novel human-annotated benchmark for comprehensively and accurately evaluating VLMsâ€™ compositionality. Our proposed benchmark serves as a complement to these earlier works. With MMCOMPOSITION, we can quantify and explore the compositionality of the mainstream VLMs. Surprisingly, we find GPT-4oâ€™s compositionality inferior to the best open-source model, and we analyze the underlying reasons. Our experimental analysis reveals the limitations of VLMs in fine-grained compositional perception and reasoning, and points to areas for improvement in VLM design and training. Resources available at: https://hanghuacs.github.io/MMComposition/ </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hua2024mmcomposition</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hua, Hang and Tang, Yunlong and Zeng, Ziyun and Cao, Liangliang and Yang, Zhengyuan and He, Hangfeng and Xu, Chenliang and Luo, Jiebo}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2410.09733}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>window.__GS_CITES_FILLED__||(window.__GS_CITES_FILLED__=!0,fetch("https://raw.githubusercontent.com/yunlong10/citation-badge/main/per_article_citations.json",{cache:"no-cache"}).then(function(t){return t.json()}).then(function(t){document.querySelectorAll(".gs-cites").forEach(function(n){var e=n.getAttribute("data-article-id"),c=t&&Object.prototype.hasOwnProperty.call(t,e)?t[e]:0;n.textContent=String(c)})})["catch"](function(){document.querySelectorAll(".gs-cites").forEach(function(t){t.textContent="N/A"})}));</script> </li> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/DBD-teaser-480.webp 480w,/assets/img/publication_preview/DBD-teaser-800.webp 800w,/assets/img/publication_preview/DBD-teaser-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/DBD-teaser.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="DBD-teaser.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="feng2024more" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2406.12663" rel="external nofollow noopener" target="_blank">Do More Details Always Introduce More Hallucinations in LVLM-based Image Captioning?</a> </div> <div class="author"> Mingqian Feng,Â <em>Yunlong Tang</em>,Â Zeliang Zhang,Â andÂ Chenliang Xu<sup>â€ </sup> </div> <div class="periodical"> <em>arXiv preprint arXiv:2406.12663</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2406.12663" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite </a> </div> <div class="abstract hidden"> <p> Large Vision-Language Models (LVLMs) excel in integrating visual and linguistic contexts to produce detailed content, facilitating applications such as image captioning. However, using LVLMs to generate descriptions often faces the challenge of object hallucination (OH), where the output text misrepresents actual objects in the input image. While previous studies attribute the occurrence of OH to the inclusion of more details, our study finds technical flaws in existing metrics, leading to unreliable evaluations of models and conclusions about OH. This has sparked a debate on the question: Do more details always introduce more hallucinations in LVLM-based image captioning? In this paper, we address this debate by proposing a novel decoding strategy, Differentiated Beam Decoding (DBD), along with a reliable new set of evaluation metrics: CLIP-Precision, CLIP-Recall, and CLIP-F1. DBD decodes the wealth of information hidden in visual input into distinct language representations called unit facts in parallel. This decoding is achieved via a well-designed differential score that guides the parallel search and candidate screening. The selected unit facts are then aggregated to generate the final caption. Our proposed metrics evaluate the comprehensiveness and accuracy of image captions by comparing the embedding groups of ground-truth image regions and generated text partitions. Extensive experiments on the Visual Genome dataset validate the effectiveness of our approach, demonstrating that it produces detailed descriptions while maintaining low hallucination levels. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">feng2024more</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Do More Details Always Introduce More Hallucinations in LVLM-based Image Captioning?}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Feng, Mingqian and Tang, Yunlong and Zhang, Zeliang and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2406.12663}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>window.__GS_CITES_FILLED__||(window.__GS_CITES_FILLED__=!0,fetch("https://raw.githubusercontent.com/yunlong10/citation-badge/main/per_article_citations.json",{cache:"no-cache"}).then(function(t){return t.json()}).then(function(t){document.querySelectorAll(".gs-cites").forEach(function(n){var e=n.getAttribute("data-article-id"),c=t&&Object.prototype.hasOwnProperty.call(t,e)?t[e]:0;n.textContent=String(c)})})["catch"](function(){document.querySelectorAll(".gs-cites").forEach(function(t){t.textContent="N/A"})}));</script> </li> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/scaling-concept-480.webp 480w,/assets/img/publication_preview/scaling-concept-800.webp 800w,/assets/img/publication_preview/scaling-concept-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/scaling-concept.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="scaling-concept.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="huang2024scalingconcept" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2410.24151" rel="external nofollow noopener" target="_blank">Scaling Concept with Text-Guided Diffusion Models</a> </div> <div class="author"> Chao Huang,Â Susan Liang,Â <em>Yunlong Tang</em>,Â Yapeng Tian,Â Anurag Kumar,Â andÂ Chenliang Xu<sup>â€ </sup> </div> <div class="periodical"> <em>arXiv preprint arXiv:2410.24151</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://wikichao.github.io/ScalingConcept/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite </a> <a href="https://github.com/WikiChao/ScalingConcept" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/WikiChao/ScalingConcept?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="abstract hidden"> <p> Text-guided diffusion models have revolutionized generative tasks by producing high-fidelity content from text descriptions. They have also enabled an editing paradigm where concepts can be replaced through text conditioning (e.g., a dog to a tiger). In this work, we explore a novel approach: instead of replacing a concept, can we enhance or suppress the concept itself? Through an empirical study, we identify a trend where concepts can be decomposed in text-guided diffusion models. Leveraging this insight, we introduce ScalingConcept, a simple yet effective method to scale decomposed concepts up or down in real input without introducing new elements. To systematically evaluate our approach, we present the WeakConcept-10 dataset, where concepts are imperfect and need to be enhanced. More importantly, ScalingConcept enables a variety of novel zero-shot applications across image and audio domains, including tasks such as canonical pose generation and generative sound highlighting or removal. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">huang2024scalingconcept</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Scaling Concept with Text-Guided Diffusion Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huang, Chao and Liang, Susan and Tang, Yunlong and Tian, Yapeng and Kumar, Anurag and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2410.24151}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>window.__GS_CITES_FILLED__||(window.__GS_CITES_FILLED__=!0,fetch("https://raw.githubusercontent.com/yunlong10/citation-badge/main/per_article_citations.json",{cache:"no-cache"}).then(function(t){return t.json()}).then(function(t){document.querySelectorAll(".gs-cites").forEach(function(n){var e=n.getAttribute("data-article-id"),c=t&&Object.prototype.hasOwnProperty.call(t,e)?t[e]:0;n.textContent=String(c)})})["catch"](function(){document.querySelectorAll(".gs-cites").forEach(function(t){t.textContent="N/A"})}));</script> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CVPR Workshop</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/llmva-gebc-480.webp 480w,/assets/img/publication_preview/llmva-gebc-800.webp 800w,/assets/img/publication_preview/llmva-gebc-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/llmva-gebc.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="llmva-gebc.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="tang2023llmva" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2306.10354" rel="external nofollow noopener" target="_blank">LLMVA-GEBC: Large Language Model with Video Adapter for Generic Event Boundary Captioning</a> </div> <div class="author"> <em>Yunlong Tang</em>,Â Jinrui Zhang,Â Xiangchen Wang,Â Teng Wang,Â andÂ Feng Zheng<sup>â€ </sup> </div> <div class="periodical"> <em>CVPR LOVEU Workshop; arXiv preprint arXiv:2306.10354</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite </a> <a href="https://github.com/zjr2000/LLMVA-GEBC" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/zjr2000/LLMVA-GEBC?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> <span class="award-badge">Rank 1st</span> </div> <div class="abstract hidden"> <p> Our winning entry for the CVPR 2023 Generic Event Boundary Captioning (GEBC) competition is detailed in this paper. Unlike conventional video captioning tasks, GEBC demands that the captioning model possess an understanding of immediate changes in status around the designated video boundary, making it a difficult task. This paper proposes an effective model LLMVA-GEBC (Large Language Model with Video Adapter for Generic Event Boundary Captioning): (1) We utilize a pretrained LLM for generating human-like captions with high quality. (2) To adapt the model to the GEBC task, we take the video Q-former as an adapter and train it with the frozen visual feature extractors and LLM. Our proposed method achieved a 76.14 score on the test set and won the first place in the challenge. Our code is available at https://github.com/zjr2000/LLMVA-GEBC. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tang2023llmva</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{LLMVA-GEBC: Large Language Model with Video Adapter for Generic Event Boundary Captioning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tang, Yunlong and Zhang, Jinrui and Wang, Xiangchen and Wang, Teng and Zheng, Feng}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{CVPR LOVEU Workshop; arXiv preprint arXiv:2306.10354}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>window.__GS_CITES_FILLED__||(window.__GS_CITES_FILLED__=!0,fetch("https://raw.githubusercontent.com/yunlong10/citation-badge/main/per_article_citations.json",{cache:"no-cache"}).then(function(t){return t.json()}).then(function(t){document.querySelectorAll(".gs-cites").forEach(function(n){var e=n.getAttribute("data-article-id"),c=t&&Object.prototype.hasOwnProperty.call(t,e)?t[e]:0;n.textContent=String(c)})})["catch"](function(){document.querySelectorAll(".gs-cites").forEach(function(t){t.textContent="N/A"})}));</script> </li> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICMC</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/launchpadgpt-480.webp 480w,/assets/img/publication_preview/launchpadgpt-800.webp 800w,/assets/img/publication_preview/launchpadgpt-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/launchpadgpt.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="launchpadgpt.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="xu2023launchpadgpt" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2307.04827" rel="external nofollow noopener" target="_blank">LaunchpadGPT: Language Model as Music Visualization Designer on Launchpad</a> </div> <div class="author"> Siting Xu<sup>*</sup>,Â <em>Yunlong Tang<sup>*</sup></em>,Â andÂ Feng Zheng<sup>â€ </sup> </div> <div class="periodical"> <em>In Proceedings of the International Computer Music Conference (<strong>ICMC</strong>)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite </a> <a href="https://github.com/yunlong10/LaunchpadGPT" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/yunlong10/LaunchpadGPT?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> <span class="award-badge">Oral</span> </div> <div class="abstract hidden"> <p> Launchpad is a popular music instrument for live performance and music production. In this paper, we propose LaunchpadGPT, a language model that can generate music visualization designs for Launchpad. We train the model on a large-scale dataset of music visualization designs and demonstrate its effectiveness in generating diverse and creative designs. We also develop a web-based platform that allows users to interact with the model and generate music visualization designs in real-time. Our code is available at https://github.com/yunlong10/LaunchpadGPT. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xu2023launchpadgpt</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{LaunchpadGPT: Language Model as Music Visualization Designer on Launchpad}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu, Siting and Tang, Yunlong and Zheng, Feng}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the International Computer Music Conference (ICMC)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{213-217}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>window.__GS_CITES_FILLED__||(window.__GS_CITES_FILLED__=!0,fetch("https://raw.githubusercontent.com/yunlong10/citation-badge/main/per_article_citations.json",{cache:"no-cache"}).then(function(t){return t.json()}).then(function(t){document.querySelectorAll(".gs-cites").forEach(function(n){var e=n.getAttribute("data-article-id"),c=t&&Object.prototype.hasOwnProperty.call(t,e)?t[e]:0;n.textContent=String(c)})})["catch"](function(){document.querySelectorAll(".gs-cites").forEach(function(t){t.textContent="N/A"})}));</script> </li> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/cat-480.webp 480w,/assets/img/publication_preview/cat-800.webp 800w,/assets/img/publication_preview/cat-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/cat.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cat.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2023caption" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2305.02677" rel="external nofollow noopener" target="_blank">Caption Anything: Interactive Image Description with Diverse Multimodal Controls</a> </div> <div class="author"> Teng Wang<sup>*</sup>,Â Jinrui Zhang<sup>*</sup>,Â Junjie Fei<sup>*</sup>,Â Hao Zheng,Â <em>Yunlong Tang</em>,Â Zhe Li,Â Mingqi Gao,Â andÂ Shanshan Zhao </div> <div class="periodical"> <em>arXiv preprint arXiv:2305.02677</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2305.02677" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite (Â <i class="fas fa-quote-right" style="font-size: 1.1em; margin-right: 0px; -webkit-text-stroke: 1px currentColor; -webkit-text-fill-color: transparent;"></i> <span class="gs-cites" data-article-id="u-x6o8ySG0sC" style="color: inherit;">â€¦</span>Â ) </a> <a href="https://github.com/ttengwang/Caption-Anything" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/ttengwang/Caption-Anything?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="abstract hidden"> <p> Controllable image captioning is an emerging multimodal topic that aims to describe the image with natural language following human purpose, e.g., looking at the specified regions or telling in a particular text style. State-of-the-art methods are trained on annotated pairs of input controls and output captions. However, the scarcity of such well-annotated multimodal data largely limits their usability and scalability for interactive AI systems. Leveraging unimodal instruction-following foundation models is a promising alternative that benefits from broader sources of data. In this paper, we present Caption AnyThing (CAT), a foundation model augmented image captioning framework supporting a wide range of multimodel controls: 1) visual controls, including points, boxes, and trajectories; 2) language controls, such as sentiment, length, language, and factuality. Powered by Segment Anything Model (SAM) and ChatGPT, we unify the visual and language prompts into a modularized framework, enabling the flexible combination between different controls. Extensive case studies demonstrate the user intention alignment capabilities of our framework, shedding light on effective user interaction modeling in vision-language applications. Our code is publicly available at https://github.com/ttengwang/caption-anything </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">wang2023caption</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Caption Anything: Interactive Image Description with Diverse Multimodal Controls}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Teng and Zhang, Jinrui and Fei, Junjie and Zheng, Hao and Tang, Yunlong and Li, Zhe and Gao, Mingqi and Zhao, Shanshan}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2305.02677}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>window.__GS_CITES_FILLED__||(window.__GS_CITES_FILLED__=!0,fetch("https://raw.githubusercontent.com/yunlong10/citation-badge/main/per_article_citations.json",{cache:"no-cache"}).then(function(t){return t.json()}).then(function(t){document.querySelectorAll(".gs-cites").forEach(function(n){var e=n.getAttribute("data-article-id"),c=t&&Object.prototype.hasOwnProperty.call(t,e)?t[e]:0;n.textContent=String(c)})})["catch"](function(){document.querySelectorAll(".gs-cites").forEach(function(t){t.textContent="N/A"})}));</script> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ACCV</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/m_san-480.webp 480w,/assets/img/publication_preview/m_san-800.webp 800w,/assets/img/publication_preview/m_san-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/m_san.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="m_san.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Tang_2022_ACCV" class="col-sm-8"> <div class="title"> <a href="https://openaccess.thecvf.com/content/ACCV2022/html/Tang_Multi-modal_Segment_Assemblage_Network_for_Ad_Video_Editing_with_Importance-Coherence_ACCV_2022_paper.html" rel="external nofollow noopener" target="_blank">Multi-modal Segment Assemblage Network for Ad Video Editing with Importance-Coherence Reward</a> </div> <div class="author"> <em>Yunlong Tang</em>,Â Siting Xu,Â Teng Wang,Â Qin Lin,Â Qinglin Lu,Â andÂ Feng Zheng<sup>â€ </sup> </div> <div class="periodical"> <em>In Proceedings of the Asian Conference on Computer Vision (<strong>ACCV</strong>)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openaccess.thecvf.com/content/ACCV2022/papers/Tang_Multi-modal_Segment_Assemblage_Network_for_Ad_Video_Editing_with_Importance-Coherence_ACCV_2022_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite </a> <a href="https://github.com/yunlong10/Ads-1k" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/yunlong10/Ads-1k?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="abstract hidden"> <p> Advertisement video editing aims to automatically edit advertising videos into shorter videos while retaining coherent content and crucial information conveyed by advertisers. It mainly contains two stages: video segmentation and segment assemblage. The existing method performs well at video segmentation stages but suffers from the problems of dependencies on extra cumbersome models and poor performance at the segment assemblage stage. To address these problems, we propose M-SAN (Multi-modal Segment Assemblage Network) which can perform efficient and coherent segment assemblage task end-to-end. It utilizes multi-modal representation extracted from the segments and follows the Encoder-Decoder Ptr-Net framework with the Attention mechanism. Importance-coherence reward is designed for training M-SAN. We experiment on the Ads-1k dataset with 1000+ videos under rich ad scenarios collected from advertisers. To evaluate the methods, we propose a unified metric, Imp-Coh@Time, which comprehensively assesses the importance, coherence, and duration of the outputs at the same time. Experimental results show that our method achieves better performance than random selection and the previous method on the metric. Ablation experiments further verify that multi-modal representation and importance-coherence reward significantly improve the performance. Ads-1k dataset is available at: https://github.com/yunlong10/Ads-1k. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Tang_2022_ACCV</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tang, Yunlong and Xu, Siting and Wang, Teng and Lin, Qin and Lu, Qinglin and Zheng, Feng}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multi-modal Segment Assemblage Network for Ad Video Editing with Importance-Coherence Reward}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Asian Conference on Computer Vision (ACCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3519-3535}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>window.__GS_CITES_FILLED__||(window.__GS_CITES_FILLED__=!0,fetch("https://raw.githubusercontent.com/yunlong10/citation-badge/main/per_article_citations.json",{cache:"no-cache"}).then(function(t){return t.json()}).then(function(t){document.querySelectorAll(".gs-cites").forEach(function(n){var e=n.getAttribute("data-article-id"),c=t&&Object.prototype.hasOwnProperty.call(t,e)?t[e]:0;n.textContent=String(c)})})["catch"](function(){document.querySelectorAll(".gs-cites").forEach(function(t){t.textContent="N/A"})}));</script> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2025 Yunlong (Yolo) Tang. Theme by <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="external nofollow noopener">al-folio</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script>for(var wechatModal=document.getElementById("WeChatMod"),wechatBtn=document.querySelectorAll('[id="WeChatBtn"]'),i=0;i<wechatBtn.length;i++)wechatBtn[i].onclick=function(){wechatModal.style.display="block"};window.onclick=function(t){t.target==wechatModal&&(wechatModal.style.display="none")};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"Publications",description:"* Equal Contribution | \u2020 Corresponding Author",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/2021/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2021/distill/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/blog/2020/twitter/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march &amp; april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-started-my-part-time-internship-at-tencent-in-shenzhen-with-supervision-from-dr-wenhao-jiang-and-qin-lin-i-had-to-balance-it-with-my-university-coursework",title:"Started my part-time internship at Tencent in Shenzhen, with supervision from Dr. Wenhao...",description:"",section:"News"},{id:"news-i-left-tencent-and-joined-sustech-vip-lab-as-an-undergraduate-student-researcher",title:"I left Tencent and joined SUSTech VIP Lab as an undergraduate student researcher....",description:"",section:"News"},{id:"news-one-paper-about-multimodal-ad-video-editing-has-been-accepted-to-asian-conference-on-computer-vision-accv-2022",title:"One paper about multimodal Ad video editing has been accepted to Asian Conference...",description:"",section:"News"},{id:"news-thrilled-to-announce-i-ll-be-starting-my-ph-d-in-computer-science-at-the-university-of-rochester-from-fall-2023-working-with-prof-chenliang-xu",title:"Thrilled to announce I\u2019ll be starting my Ph.D. in Computer Science at the...",description:"",section:"News"},{id:"news-our-project-caption-anything-has-been-released-welcome-to-try-our-demo-and-star-our-github-repo",title:"Our project Caption Anything has been released! Welcome to try our demo and...",description:"",section:"News"},{id:"news-the-technical-report-for-caption-anything-has-been-released",title:"The technical report for Caption Anything has been released!",description:"",section:"News"},{id:"news-successfully-defended-my-undergraduate-thesis-titled-language-guided-video-cover-generation-which-has-been-awarded-the-excellent-undergraduate-thesis",title:"Successfully defended my undergraduate thesis titled Language-Guided Video Cover Generation, which has been...",description:"",section:"News"},{id:"news-our-team-won-the-first-place-in-loveu-long-form-video-understanding-challenge-at-cvpr-23-workshop",title:"Our team won the first place in LOVEU (Long-form Video Understanding) Challenge at...",description:"",section:"News"},{id:"news-graduated-from-sustech-obtained-my-bachelor-s-degree-and-the-honor-of-excellent-graduate-for-exceptional-performance",title:"Graduated from SUSTech, obtained my bachelor\u2019s degree and the honor of Excellent Graduate...",description:"",section:"News"},{id:"news-one-paper-accepted-to-international-computer-music-conference-icmc-2023",title:"One paper accepted to International Computer Music Conference (ICMC) 2023.",description:"",section:"News"},{id:"news-officially-joined-in-the-chenliang-xu-s-group-at-ur-cs-as-a-ph-d-student",title:"Officially joined in the Chenliang Xu\u2019s Group at UR CS as a Ph.D....",description:"",section:"News"},{id:"news-released-a-survey-for-video-understanding-with-llms-arxiv-github",title:"\ud83d\udd25\ud83d\udd25\ud83d\udd25 Released a survey for Video Understanding with LLMs (arXiv, GitHub).",description:"",section:"News"},{id:"news-i-will-join-bytedance-as-a-research-intern-this-summer",title:"I will join ByteDance as a Research Intern this summer.",description:"",section:"News"},{id:"news-released-avicuna-an-audio-visual-llm-empowered-by-pseudo-untrimmed-video-annotations-for-audio-visual-event-localization",title:"Released AVicuna, an Audio-Visual LLM empowered by pseudo-untrimmed video annotations for audio-visual event...",description:"",section:"News"},{id:"news-introducing-v2xum-llama-model-and-instruct-v2xum-dataset-for-cross-modal-video-summarization",title:"Introducing V2Xum-LLaMA model and Instruct-V2Xum dataset for cross-modal video summarization.",description:"",section:"News"},{id:"news-started-my-internship-at-bytedance-in-san-jose-ca-mentored-by-yiting-liao-amp-amp-gen-zhan",title:"\ud83c\udf1f Started my internship at ByteDance in San Jose, CA, mentored by Yiting...",description:"",section:"News"},{id:"news-introducing-differentiated-beam-decoding-dbd-a-novel-decoding-strategy-for-lvlm-hallucination-mitigation",title:"Introducing Differentiated Beam Decoding (DBD), a novel decoding strategy for LVLM hallucination mitigation....",description:"",section:"News"},{id:"news-one-paper-about-egocentric-video-understanding-with-llm-has-been-accepted-to-acm-mm-2024",title:"One paper about egocentric video understanding with LLM has been accepted to ACM...",description:"",section:"News"},{id:"news-we-39-ve-recently-updated-our-survey-quot-video-understanding-with-large-language-models-a-survey-quot",title:"\ud83d\udce2 We&#39;ve recently updated our survey: &quot;Video Understanding with Large Language Models: A...",description:"",section:"News",handler:()=>{window.location.href="/news/2024-07-23/"}},{id:"news-we-won-the-first-place-in-aim-2024-challenge-on-video-saliency-prediction-eccv-workshop-thanks-to-gen-zhan-and-li-yang",title:"\ud83c\udfc5 We won the first place in AIM 2024 Challenge on Video Saliency...",description:"",section:"News"},{id:"news-introducing-cardiff-a-framework-for-video-saliency-prediction-using-mllm-cot-reasoning-and-diffusion-model",title:"Introducing CaRDiff, a framework for video saliency prediction using MLLM CoT reasoning and...",description:"",section:"News"},{id:"news-mmcomposition-has-been-publicly-released-read-our-paper-check-out-the-latest-leaderboard-and-access-the-code-to-evaluate-your-own-models",title:"\ud83d\ude80 MMComposition has been publicly released. Read our Paper, check out the latest...",description:"",section:"News"},{id:"news-we-have-released-vidcomposition-a-benchmark-to-evaluate-mllms-understanding-of-video-compositions-project-page-paper-leaderboard",title:"We have released VidComposition, a benchmark to evaluate MLLMs\u2019 understanding of video compositions....",description:"",section:"News"},{id:"news-three-papers-on-video-llms-have-been-accepted-to-aaai-2025",title:"\ud83c\udf89 Three papers on Video-LLMs have been accepted to AAAI 2025!",description:"",section:"News"},{id:"news-introducing-our-survey-paper-on-genai-for-cel-animation-arxiv-github",title:"\ud83c\udfa8 Introducing our survey paper on GenAI for Cel-Animation \ud83d\udc49 arXiv | GitHub...",description:"",section:"News"},{id:"news-i-will-join-amazon-as-an-applied-scientist-intern-this-summer",title:"I will join Amazon as an Applied Scientist Intern this summer.",description:"",section:"News"},{id:"news-two-papers-have-been-accepted-to-cvpr-2025-including-our-vidcomposition-benchmark",title:"\ud83c\udf89 Two papers have been accepted to CVPR 2025, including our VidComposition benchmark!...",description:"",section:"News"},{id:"news-caption-anything-in-video-cat-v-has-been-released-arxiv-github",title:"\ud83d\udcf7 Caption Anything in Video (CAT-V) has been released \ud83d\udc49 arXiv | GitHub...",description:"",section:"News"},{id:"news-our-vid-llm-survey-has-been-accepted-by-the-ieee-transactions-on-circuits-and-systems-for-video-technology-tcsvt-ieee-xplore-github",title:"\ud83c\udf89 Our Vid-LLM survey has been accepted by the IEEE Transactions on Circuits...",description:"",section:"News"},{id:"news-started-my-internship-as-an-applied-scientist-intern-at-amazon-in-bellevue-wa",title:"\ud83c\udf1f Started my internship as an Applied Scientist Intern at Amazon in Bellevue,...",description:"",section:"News"},{id:"news-introducing-mmperspective-a-comprehensive-benchmark-for-mllms-on-perspective-understanding",title:"\ud83d\udcd0 Introducing MMPerspective, a comprehensive benchmark for MLLMs on perspective understanding.",description:"",section:"News"},{id:"projects-genai-for-cel-animation",title:"GenAI for Cel-Animation",description:"[ICCVW 2025] \ud83c\udfa8 A Comprehensive Survey on GenAI for Cel-Animation.",section:"Projects",handler:()=>{window.location.href="/projects/ai4anime/"}},{id:"projects-cat-\ufe0f",title:"CAT\uff3c(=^\u2025^)\u270f\ufe0f",description:"Caption-Anything is a versatile image processing tool that combines the capabilities of Segment Anything, Visual Captioning, and ChatGPT. Our solution generates descriptive captions for any object within an image, offering a range of language styles to accommodate diverse user preferences.",section:"Projects",handler:()=>{window.location.href="/projects/caption_anything/"}},{id:"projects-captionanything-in-video-cat-v",title:"CaptionAnything in Video (CAT-V)",description:"CAT-V is a training-free framework that enables fine-grained object-centric video captioning through spatiotemporal visual prompting and chain-of-thought reasoning.",section:"Projects",handler:()=>{window.location.href="/projects/cat-v/"}},{id:"projects-launchpadgpt",title:"LaunchpadGPT",description:"[ICMC 2023] \ud83c\udfb5 LaunchpadGPT: Language Model as Music Visualization Designer on Launchpad",section:"Projects",handler:()=>{window.location.href="/projects/launchpadgpt/"}},{id:"projects-mmcomposition",title:"MMComposition",description:"Benchmarking the compositionality capabilities of VLMs \ud83e\udd2f",section:"Projects",handler:()=>{window.location.href="/projects/mmcomposition/"}},{id:"projects-mmperspective",title:"MMPerspective",description:"[NeurIPS 2025] Do MLLMs Understand Perspective? A Comprehensive Benchmark for Perspective Perception, Reasoning, and Robustness is provided in this project.",section:"Projects",handler:()=>{window.location.href="/projects/mmperspective/"}},{id:"projects-scaling-concept",title:"Scaling Concept",description:"We use pretrained text-guided diffusion models to scale up/down concepts in image/audio.",section:"Projects",handler:()=>{window.location.href="/projects/scaling_concept/"}},{id:"projects-vidcomposition",title:"VidComposition",description:"[CVPR 2025] \ud83c\udfc6 See how Top MLLMs understand video compositions.",section:"Projects",handler:()=>{window.location.href="/projects/vidcomposition/"}},{id:"projects-vid-llm-survey",title:"Vid-LLM Survey",description:"[TCSVT] \ud83d\udd25 Video Understanding with Large Language Models: A Survey",section:"Projects",handler:()=>{window.location.href="/projects/vidllm_survey/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%79%75%6E%6C%6F%6E%67.%74%61%6E%67@%72%6F%63%68%65%73%74%65%72.%65%64%75","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=xf1rCgoAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/yunlong10","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/yolo-yunlong-tang","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/yoloytang","_blank")}},{id:"socials-work",title:"Work",section:"Socials",handler:()=>{window.open("https://yunlong10.github.io/assets/pdf/cv_yunlong_tang.pdf","_blank")}},{id:"socials-youtube",title:"YouTube",section:"Socials",handler:()=>{window.open("https://youtube.com/@yoloytang","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>