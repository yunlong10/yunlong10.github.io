<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | Yunlong (Yolo) Tang </title> <meta name="author" content="Yunlong (Yolo) Tang"> <meta name="description" content="* Equal Contribution | â€  Corresponding Author"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo/icon.jpg?313e0410176f41a6e55d7f7cd141e7a4"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yunlong10.github.io/publications/"> <script src="/assets/js/theme.js?6eeff2fb375dfabd5e33c4eefb1fadbc"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Yunlong</span> (Yolo) Tang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description">* Equal Contribution | â€  Corresponding Author</p> </header> <article> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/videolmm_posttraining-480.webp 480w,/assets/img/publication_preview/videolmm_posttraining-800.webp 800w,/assets/img/publication_preview/videolmm_posttraining-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/videolmm_posttraining.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="videolmm_posttraining.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="tang2025videolmm_posttraining" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2510.05034" rel="external nofollow noopener" target="_blank">Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models</a> </div> <div class="author"> <em>Yunlong Tang</em>,Â Jing Bi,Â Pinxin Liu,Â Zhenyu Pan,Â Zhangyun Tan,Â Qianxiang Shen,Â Jiani Liu,Â Hang Hua,Â Junjia Guo,Â Yunzhong Xiao, and <span class="more-authors" title="click to view 17 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '17 more authors' ? 'Chao Huang, Zhiyuan Wang, Susan Liang, Xinyi Liu, Yizhi Song, Yuhe Nie, Jia-Xing Zhong, Bozheng Li, Daiqing Qi, Ziyun Zeng, Ali Vosoughi, Luchuan Song, Zeliang Zhang, Daiki Shimada, Han Liu, Jiebo Luo, Chenliang Xu' : '17 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">17 more authors</span> </div> <div class="periodical"> <em>arXiv preprint arXiv:2510.05034</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> <a href="https://github.com/yunlong10/Awesome-Video-LMM-Post-Training" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/yunlong10/Awesome-Video-LMM-Post-Training?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tang2025videolmm_posttraining</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tang, Yunlong and Bi, Jing and Liu, Pinxin and Pan, Zhenyu and Tan, Zhangyun and Shen, Qianxiang and Liu, Jiani and Hua, Hang and Guo, Junjia and Xiao, Yunzhong and Huang, Chao and Wang, Zhiyuan and Liang, Susan and Liu, Xinyi and Song, Yizhi and Nie, Yuhe and Zhong, Jia-Xing and Li, Bozheng and Qi, Daiqing and Zeng, Ziyun and Vosoughi, Ali and Song, Luchuan and Zhang, Zeliang and Shimada, Daiki and Liu, Han and Luo, Jiebo and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2510.05034}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>function updateScholarBadgeColors(){const e=getComputedStyle(document.documentElement).getPropertyValue("--global-theme-color").trim(),t="dark"===document.documentElement.getAttribute("data-theme"),o=document.querySelectorAll(".scholar-badge");if(e&&o.length>0){const a=e.replace("#",""),r="fff";o.forEach(e=>{const o=e.getAttribute("data-citations");if(o){const l=t?"fff":"999999";e.src=`https://img.shields.io/badge/Cited%20by-${o}-${a}?logo=googlescholar&labelColor=${r}&style=flat`,e.style.border=`1px solid #${l}`,e.style.borderRadius="6px"}})}}document.addEventListener("DOMContentLoaded",updateScholarBadgeColors);const observer=new MutationObserver(function(e){e.forEach(function(e){"attributes"===e.type&&"data-theme"===e.attributeName&&updateScholarBadgeColors()})});observer.observe(document.documentElement,{attributes:!0,attributeFilter:["data-theme"]});</script> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">TCSVT</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/vidllm_survey-480.webp 480w,/assets/img/publication_preview/vidllm_survey-800.webp 800w,/assets/img/publication_preview/vidllm_survey-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/vidllm_survey.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="vidllm_survey.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="vidllmsurvey" class="col-sm-8"> <div class="title"> <a href="https://ieeexplore.ieee.org/document/10982110" rel="external nofollow noopener" target="_blank">Video Understanding with Large Language Models: A Survey ðŸ”¥ðŸ”¥ðŸ”¥</a> </div> <div class="author"> <em>Yunlong Tang<sup>*</sup></em>,Â Jing Bi<sup>*</sup>,Â Siting Xu<sup>*</sup>,Â Luchuan Song,Â Susan Liang,Â Teng Wang,Â Daoan Zhang,Â Jie An,Â Jingyang Lin,Â Rongyi Zhu, and <span class="more-authors" title="click to view 10 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '10 more authors' ? 'Ali Vosoughi, Chao Huang, Zeliang Zhang, Pinxin Liu, Mingqian Feng, Feng Zheng, Jianguo Zhang, Ping Luo, Jiebo Luo, Chenliang Xu&lt;sup&gt;â€ &lt;/sup&gt;' : '10 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">10 more authors</span> </div> <div class="periodical"> <em>IEEE Transactions on Circuits and Systems for Video Technology</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=xf1rCgoAAAAJ&amp;citation_for_view=xf1rCgoAAAAJ:qjMakFHDy7sC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img class="scholar-badge" data-citations="160" src="https://img.shields.io/badge/Cited%20by-160-4285F4?logo=googlescholar&amp;labelColor=fff" alt="160 Google Scholar citations"> </a> </div> <a href="https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/yunlong10/Awesome-LLMs-for-Video-Understanding?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">vidllmsurvey</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Video Understanding with Large Language Models: A Survey ðŸ”¥ðŸ”¥ðŸ”¥}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tang, Yunlong and Bi, Jing and Xu, Siting and Song, Luchuan and Liang, Susan and Wang, Teng and Zhang, Daoan and An, Jie and Lin, Jingyang and Zhu, Rongyi and Vosoughi, Ali and Huang, Chao and Zhang, Zeliang and Liu, Pinxin and Feng, Mingqian and Zheng, Feng and Zhang, Jianguo and Luo, Ping and Luo, Jiebo and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Circuits and Systems for Video Technology}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>function updateScholarBadgeColors(){const e=getComputedStyle(document.documentElement).getPropertyValue("--global-theme-color").trim(),t="dark"===document.documentElement.getAttribute("data-theme"),o=document.querySelectorAll(".scholar-badge");if(e&&o.length>0){const a=e.replace("#",""),r="fff";o.forEach(e=>{const o=e.getAttribute("data-citations");if(o){const l=t?"fff":"999999";e.src=`https://img.shields.io/badge/Cited%20by-${o}-${a}?logo=googlescholar&labelColor=${r}&style=flat`,e.style.border=`1px solid #${l}`,e.style.borderRadius="6px"}})}}document.addEventListener("DOMContentLoaded",updateScholarBadgeColors);const observer=new MutationObserver(function(e){e.forEach(function(e){"attributes"===e.type&&"data-theme"===e.attributeName&&updateScholarBadgeColors()})});observer.observe(document.documentElement,{attributes:!0,attributeFilter:["data-theme"]});</script> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mmperspective-480.webp 480w,/assets/img/publication_preview/mmperspective-800.webp 800w,/assets/img/publication_preview/mmperspective-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/mmperspective.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mmperspective.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="tang2025mmperspective" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2505.20426" rel="external nofollow noopener" target="_blank">MMPerspective: Do MLLMs Understand Perspective? A Comprehensive Benchmark for Perspective Perception, Reasoning, and Robustness</a> </div> <div class="author"> <em>Yunlong Tang</em>,Â Pinxin Liu,Â Mingqian Feng,Â Zhangyun Tan,Â Rui Mao,Â Chao Huang,Â Jing Bi,Â Yunzhong Xiao,Â Susan Liang,Â Hang Hua, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Ali Vosoughi, Luchuan Song, Zeliang Zhang, Chenliang Xu' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>The 39th Annual Conference on Neural Information Processing Systems</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> <a href="https://yunlong10.github.io/MMPerspective/" class="btn btn-sm z-depth-0" role="button">Website</a> <a href="https://github.com/yunlong10/MMPerspective" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/yunlong10/MMPerspective?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tang2025mmperspective</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MMPerspective: Do MLLMs Understand Perspective? A Comprehensive Benchmark for Perspective Perception, Reasoning, and Robustness}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tang, Yunlong and Liu, Pinxin and Feng, Mingqian and Tan, Zhangyun and Mao, Rui and Huang, Chao and Bi, Jing and Xiao, Yunzhong and Liang, Susan and Hua, Hang and Vosoughi, Ali and Song, Luchuan and Zhang, Zeliang and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The 39th Annual Conference on Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>function updateScholarBadgeColors(){const e=getComputedStyle(document.documentElement).getPropertyValue("--global-theme-color").trim(),t="dark"===document.documentElement.getAttribute("data-theme"),o=document.querySelectorAll(".scholar-badge");if(e&&o.length>0){const a=e.replace("#",""),r="fff";o.forEach(e=>{const o=e.getAttribute("data-citations");if(o){const l=t?"fff":"999999";e.src=`https://img.shields.io/badge/Cited%20by-${o}-${a}?logo=googlescholar&labelColor=${r}&style=flat`,e.style.border=`1px solid #${l}`,e.style.borderRadius="6px"}})}}document.addEventListener("DOMContentLoaded",updateScholarBadgeColors);const observer=new MutationObserver(function(e){e.forEach(function(e){"attributes"===e.type&&"data-theme"===e.attributeName&&updateScholarBadgeColors()})});observer.observe(document.documentElement,{attributes:!0,attributeFilter:["data-theme"]});</script> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mmigbench-480.webp 480w,/assets/img/publication_preview/mmigbench-800.webp 800w,/assets/img/publication_preview/mmigbench-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/mmigbench.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mmigbench.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hua2025mmigbench" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2505.19415" rel="external nofollow noopener" target="_blank">MMIG-Bench: Towards Comprehensive and Explainable Evaluation of Multi-Modal Image Generation Models</a> </div> <div class="author"> Hang Hua,Â Ziyun Zeng,Â Yizhi Song,Â <em>Yunlong Tang</em>,Â Liu He,Â Daniel Aliaga,Â Wei Xiong,Â andÂ Jiebo Luo </div> <div class="periodical"> <em>The 39th Annual Conference on Neural Information Processing Systems</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> <a href="https://hanghuacs.github.io/MMIG-Bench/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="https://github.com/hanghuacs/MMIG-Bench" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/hanghuacs/MMIG-Bench?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hua2025mmigbench</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MMIG-Bench: Towards Comprehensive and Explainable Evaluation of Multi-Modal Image Generation Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hua, Hang and Zeng, Ziyun and Song, Yizhi and Tang, Yunlong and He, Liu and Aliaga, Daniel and Xiong, Wei and Luo, Jiebo}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The 39th Annual Conference on Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>function updateScholarBadgeColors(){const e=getComputedStyle(document.documentElement).getPropertyValue("--global-theme-color").trim(),t="dark"===document.documentElement.getAttribute("data-theme"),o=document.querySelectorAll(".scholar-badge");if(e&&o.length>0){const a=e.replace("#",""),r="fff";o.forEach(e=>{const o=e.getAttribute("data-citations");if(o){const l=t?"fff":"999999";e.src=`https://img.shields.io/badge/Cited%20by-${o}-${a}?logo=googlescholar&labelColor=${r}&style=flat`,e.style.border=`1px solid #${l}`,e.style.borderRadius="6px"}})}}document.addEventListener("DOMContentLoaded",updateScholarBadgeColors);const observer=new MutationObserver(function(e){e.forEach(function(e){"attributes"===e.type&&"data-theme"===e.attributeName&&updateScholarBadgeColors()})});observer.observe(document.documentElement,{attributes:!0,attributeFilter:["data-theme"]});</script> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/zerosep-480.webp 480w,/assets/img/publication_preview/zerosep-800.webp 800w,/assets/img/publication_preview/zerosep-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/zerosep.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="zerosep.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="huang2025zerosep" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2505.23625" rel="external nofollow noopener" target="_blank">ZeroSep: Separate Anything in Audio with Zero Training</a> </div> <div class="author"> Chao Huang,Â Yuesheng Ma,Â Junxuan Huang,Â Susan Liang,Â <em>Yunlong Tang</em>,Â Jing Bi,Â Wenqiang Liu,Â Nima Mesgarani,Â andÂ Chenliang Xu </div> <div class="periodical"> <em>The 39th Annual Conference on Neural Information Processing Systems</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> <a href="https://wikichao.github.io/ZeroSep/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">huang2025zerosep</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ZeroSep: Separate Anything in Audio with Zero Training}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huang, Chao and Ma, Yuesheng and Huang, Junxuan and Liang, Susan and Tang, Yunlong and Bi, Jing and Liu, Wenqiang and Mesgarani, Nima and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The 39th Annual Conference on Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>function updateScholarBadgeColors(){const e=getComputedStyle(document.documentElement).getPropertyValue("--global-theme-color").trim(),t="dark"===document.documentElement.getAttribute("data-theme"),o=document.querySelectorAll(".scholar-badge");if(e&&o.length>0){const a=e.replace("#",""),r="fff";o.forEach(e=>{const o=e.getAttribute("data-citations");if(o){const l=t?"fff":"999999";e.src=`https://img.shields.io/badge/Cited%20by-${o}-${a}?logo=googlescholar&labelColor=${r}&style=flat`,e.style.border=`1px solid #${l}`,e.style.borderRadius="6px"}})}}document.addEventListener("DOMContentLoaded",updateScholarBadgeColors);const observer=new MutationObserver(function(e){e.forEach(function(e){"attributes"===e.type&&"data-theme"===e.attributeName&&updateScholarBadgeColors()})});observer.observe(document.documentElement,{attributes:!0,attributeFilter:["data-theme"]});</script> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/harnessing-480.webp 480w,/assets/img/publication_preview/harnessing-800.webp 800w,/assets/img/publication_preview/harnessing-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/harnessing.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="harnessing.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="liu2025harnessing" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2504.10804" rel="external nofollow noopener" target="_blank">Harnessing the Computation Redundancy in ViTs to Boost Adversarial Transferability</a> </div> <div class="author"> Jiani Liu,Â Zhiyuan Wang,Â Zeliang Zhang,Â Chao Huang,Â Susan Liang,Â <em>Yunlong Tang</em>,Â andÂ Chenliang Xu </div> <div class="periodical"> <em>The 39th Annual Conference on Neural Information Processing Systems</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2025harnessing</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Harnessing the Computation Redundancy in ViTs to Boost Adversarial Transferability}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Jiani and Wang, Zhiyuan and Zhang, Zeliang and Huang, Chao and Liang, Susan and Tang, Yunlong and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The 39th Annual Conference on Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>function updateScholarBadgeColors(){const e=getComputedStyle(document.documentElement).getPropertyValue("--global-theme-color").trim(),t="dark"===document.documentElement.getAttribute("data-theme"),o=document.querySelectorAll(".scholar-badge");if(e&&o.length>0){const a=e.replace("#",""),r="fff";o.forEach(e=>{const o=e.getAttribute("data-citations");if(o){const l=t?"fff":"999999";e.src=`https://img.shields.io/badge/Cited%20by-${o}-${a}?logo=googlescholar&labelColor=${r}&style=flat`,e.style.border=`1px solid #${l}`,e.style.borderRadius="6px"}})}}document.addEventListener("DOMContentLoaded",updateScholarBadgeColors);const observer=new MutationObserver(function(e){e.forEach(function(e){"attributes"===e.type&&"data-theme"===e.attributeName&&updateScholarBadgeColors()})});observer.observe(document.documentElement,{attributes:!0,attributeFilter:["data-theme"]});</script> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CVPR</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/vidcomposition-480.webp 480w,/assets/img/publication_preview/vidcomposition-800.webp 800w,/assets/img/publication_preview/vidcomposition-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/vidcomposition.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="vidcomposition.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="tang2024vidcompostion" class="col-sm-8"> <div class="title"> <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Tang_VidComposition_Can_MLLMs_Analyze_Compositions_in_Compiled_Videos_CVPR_2025_paper.html" rel="external nofollow noopener" target="_blank">VidComposition: Can MLLMs Analyze Compositions in Compiled Videos?</a> </div> <div class="author"> <em>Yunlong Tang<sup>*</sup></em>,Â Junjia Guo<sup>*</sup>,Â Hang Hua,Â Susan Liang,Â Mingqian Feng,Â Xinyang Li,Â Rui Mao,Â Chao Huang,Â Jing Bi,Â Zeliang Zhang, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Pooyan Fazli, Chenliang Xu&lt;sup&gt;â€ &lt;/sup&gt;' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the Computer Vision and Pattern Recognition Conference</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> <a href="https://yunlong10.github.io/VidComposition/" class="btn btn-sm z-depth-0" role="button">Website</a> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=xf1rCgoAAAAJ&amp;citation_for_view=xf1rCgoAAAAJ:_FxGoFyzp5QC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img class="scholar-badge" data-citations="13" src="https://img.shields.io/badge/Cited%20by-13-4285F4?logo=googlescholar&amp;labelColor=fff" alt="13 Google Scholar citations"> </a> </div> <a href="https://github.com/yunlong10/VidComposition" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/yunlong10/VidComposition?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tang2024vidcompostion</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{VidComposition: Can MLLMs Analyze Compositions in Compiled Videos?}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tang, Yunlong and Guo, Junjia and Hua, Hang and Liang, Susan and Feng, Mingqian and Li, Xinyang and Mao, Rui and Huang, Chao and Bi, Jing and Zhang, Zeliang and Fazli, Pooyan and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Computer Vision and Pattern Recognition Conference}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{8490-8500}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>function updateScholarBadgeColors(){const e=getComputedStyle(document.documentElement).getPropertyValue("--global-theme-color").trim(),t="dark"===document.documentElement.getAttribute("data-theme"),o=document.querySelectorAll(".scholar-badge");if(e&&o.length>0){const a=e.replace("#",""),r="fff";o.forEach(e=>{const o=e.getAttribute("data-citations");if(o){const l=t?"fff":"999999";e.src=`https://img.shields.io/badge/Cited%20by-${o}-${a}?logo=googlescholar&labelColor=${r}&style=flat`,e.style.border=`1px solid #${l}`,e.style.borderRadius="6px"}})}}document.addEventListener("DOMContentLoaded",updateScholarBadgeColors);const observer=new MutationObserver(function(e){e.forEach(function(e){"attributes"===e.type&&"data-theme"===e.attributeName&&updateScholarBadgeColors()})});observer.observe(document.documentElement,{attributes:!0,attributeFilter:["data-theme"]});</script> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICCVW</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/teaser-ai4anime-480.webp 480w,/assets/img/publication_preview/teaser-ai4anime-800.webp 800w,/assets/img/publication_preview/teaser-ai4anime-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/teaser-ai4anime.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="teaser-ai4anime.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="tang2025ai4anime" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2501.06250" rel="external nofollow noopener" target="_blank">Generative AI for Cel-Animation: A Survey ðŸ”¥ðŸ”¥ðŸ”¥</a> </div> <div class="author"> <em>Yunlong Tang</em>,Â Junjia Guo,Â Pinxin Liu,Â Zhiyuan Wang,Â Hang Hua,Â Jia-Xing Zhong,Â Yunzhong Xiao,Â Chao Huang,Â Luchuan Song,Â Susan Liang, and <span class="more-authors" title="click to view 7 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '7 more authors' ? 'Yizhi Song, Liu He, Jing Bi, Mingqian Feng, Xinyang Li, Zeliang Zhang, Chenliang Xu' : '7 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">7 more authors</span> </div> <div class="periodical"> <em>arXiv preprint arXiv:2501.06250</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=xf1rCgoAAAAJ&amp;citation_for_view=xf1rCgoAAAAJ:Se3iqnhoufwC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img class="scholar-badge" data-citations="20" src="https://img.shields.io/badge/Cited%20by-20-4285F4?logo=googlescholar&amp;labelColor=fff" alt="20 Google Scholar citations"> </a> </div> <a href="https://github.com/yunlong10/Awesome-AI4Animation" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/yunlong10/Awesome-AI4Animation?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tang2025ai4anime</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Generative AI for Cel-Animation: A Survey ðŸ”¥ðŸ”¥ðŸ”¥}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tang, Yunlong and Guo, Junjia and Liu, Pinxin and Wang, Zhiyuan and Hua, Hang and Zhong, Jia-Xing and Xiao, Yunzhong and Huang, Chao and Song, Luchuan and Liang, Susan and Song, Yizhi and He, Liu and Bi, Jing and Feng, Mingqian and Li, Xinyang and Zhang, Zeliang and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2501.06250}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>function updateScholarBadgeColors(){const e=getComputedStyle(document.documentElement).getPropertyValue("--global-theme-color").trim(),t="dark"===document.documentElement.getAttribute("data-theme"),o=document.querySelectorAll(".scholar-badge");if(e&&o.length>0){const a=e.replace("#",""),r="fff";o.forEach(e=>{const o=e.getAttribute("data-citations");if(o){const l=t?"fff":"999999";e.src=`https://img.shields.io/badge/Cited%20by-${o}-${a}?logo=googlescholar&labelColor=${r}&style=flat`,e.style.border=`1px solid #${l}`,e.style.borderRadius="6px"}})}}document.addEventListener("DOMContentLoaded",updateScholarBadgeColors);const observer=new MutationObserver(function(e){e.forEach(function(e){"attributes"===e.type&&"data-theme"===e.attributeName&&updateScholarBadgeColors()})});observer.observe(document.documentElement,{attributes:!0,attributeFilter:["data-theme"]});</script> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CVPR</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/unveiling-480.webp 480w,/assets/img/publication_preview/unveiling-800.webp 800w,/assets/img/publication_preview/unveiling-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/unveiling.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="unveiling.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="bi2024unveiling" class="col-sm-8"> <div class="title"> <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Bi_Unveiling_Visual_Perception_in_Language_Models_An_Attention_Head_Analysis_CVPR_2025_paper.html" rel="external nofollow noopener" target="_blank">Unveiling Visual Perception in Language Models: An Attention Head Analysis Approach</a> </div> <div class="author"> Jing Bi,Â Junjia Guo,Â <em>Yunlong Tang</em>,Â Lianggong Bruce Wen,Â Zhang Liu,Â andÂ Chenliang Xu </div> <div class="periodical"> <em>In Proceedings of the Computer Vision and Pattern Recognition Conference</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">bi2024unveiling</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Unveiling Visual Perception in Language Models: An Attention Head Analysis Approach}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bi, Jing and Guo, Junjia and Tang, Yunlong and Wen, Lianggong Bruce and Liu, Zhang and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Computer Vision and Pattern Recognition Conference}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>function updateScholarBadgeColors(){const e=getComputedStyle(document.documentElement).getPropertyValue("--global-theme-color").trim(),t="dark"===document.documentElement.getAttribute("data-theme"),o=document.querySelectorAll(".scholar-badge");if(e&&o.length>0){const a=e.replace("#",""),r="fff";o.forEach(e=>{const o=e.getAttribute("data-citations");if(o){const l=t?"fff":"999999";e.src=`https://img.shields.io/badge/Cited%20by-${o}-${a}?logo=googlescholar&labelColor=${r}&style=flat`,e.style.border=`1px solid #${l}`,e.style.borderRadius="6px"}})}}document.addEventListener("DOMContentLoaded",updateScholarBadgeColors);const observer=new MutationObserver(function(e){e.forEach(function(e){"attributes"===e.type&&"data-theme"===e.attributeName&&updateScholarBadgeColors()})});observer.observe(document.documentElement,{attributes:!0,attributeFilter:["data-theme"]});</script> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">AAAI</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/teaser-avicuna-480.webp 480w,/assets/img/publication_preview/teaser-avicuna-800.webp 800w,/assets/img/publication_preview/teaser-avicuna-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/teaser-avicuna.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="teaser-avicuna.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="tang2024avicuna" class="col-sm-8"> <div class="title"> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/32784" rel="external nofollow noopener" target="_blank">Empowering LLMs with Pseudo-Untrimmed Videos for Audio-Visual Temporal Understanding</a> </div> <div class="author"> <em>Yunlong Tang</em>,Â Daiki Shimada,Â Jing Bi,Â Mingqian Feng,Â Hang Hua,Â andÂ Chenliang Xu<sup>â€ </sup> </div> <div class="periodical"> <em>In Proceedings of the AAAI Conference on Artificial Intelligence</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=xf1rCgoAAAAJ&amp;citation_for_view=xf1rCgoAAAAJ:IjCSPb-OGe4C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img class="scholar-badge" data-citations="38" src="https://img.shields.io/badge/Cited%20by-38-4285F4?logo=googlescholar&amp;labelColor=fff" alt="38 Google Scholar citations"> </a> </div> <a href="https://github.com/yunlong10/AVicuna" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/yunlong10/AVicuna?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tang2024avicuna</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Empowering LLMs with Pseudo-Untrimmed Videos for Audio-Visual Temporal Understanding}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tang, Yunlong and Shimada, Daiki and Bi, Jing and Feng, Mingqian and Hua, Hang and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the AAAI Conference on Artificial Intelligence}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{39}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{7293-7301}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1609/aaai.v39i7.32784}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>function updateScholarBadgeColors(){const e=getComputedStyle(document.documentElement).getPropertyValue("--global-theme-color").trim(),t="dark"===document.documentElement.getAttribute("data-theme"),o=document.querySelectorAll(".scholar-badge");if(e&&o.length>0){const a=e.replace("#",""),r="fff";o.forEach(e=>{const o=e.getAttribute("data-citations");if(o){const l=t?"fff":"999999";e.src=`https://img.shields.io/badge/Cited%20by-${o}-${a}?logo=googlescholar&labelColor=${r}&style=flat`,e.style.border=`1px solid #${l}`,e.style.borderRadius="6px"}})}}document.addEventListener("DOMContentLoaded",updateScholarBadgeColors);const observer=new MutationObserver(function(e){e.forEach(function(e){"attributes"===e.type&&"data-theme"===e.attributeName&&updateScholarBadgeColors()})});observer.observe(document.documentElement,{attributes:!0,attributeFilter:["data-theme"]});</script> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">AAAI</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/v2xum-llama-480.webp 480w,/assets/img/publication_preview/v2xum-llama-800.webp 800w,/assets/img/publication_preview/v2xum-llama-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/v2xum-llama.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="v2xum-llama.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hua2024v2xum" class="col-sm-8"> <div class="title"> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/32374" rel="external nofollow noopener" target="_blank">V2Xum-LLM: Cross-modal Video Summarization with Temporal Prompt Instruction Tuning</a> </div> <div class="author"> Hang Hua<sup>*</sup>,Â <em>Yunlong Tang<sup>*</sup></em>,Â Chenliang Xu,Â andÂ Jiebo Luo<sup>â€ </sup> </div> <div class="periodical"> <em>In Proceedings of the AAAI Conference on Artificial Intelligence</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> <a href="https://hanghuacs.github.io/v2xum/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=xf1rCgoAAAAJ&amp;citation_for_view=xf1rCgoAAAAJ:zYLM7Y9cAGgC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img class="scholar-badge" data-citations="49" src="https://img.shields.io/badge/Cited%20by-49-4285F4?logo=googlescholar&amp;labelColor=fff" alt="49 Google Scholar citations"> </a> </div> <a href="https://github.com/hanghuacs/V2Xum-LLM" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/hanghuacs/V2Xum-LLM?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">hua2024v2xum</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{V2Xum-LLM: Cross-modal Video Summarization with Temporal Prompt Instruction Tuning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hua, Hang and Tang, Yunlong and Xu, Chenliang and Luo, Jiebo}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the AAAI Conference on Artificial Intelligence}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{39}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3599-3607}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1609/aaai.v39i4.32374}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>function updateScholarBadgeColors(){const e=getComputedStyle(document.documentElement).getPropertyValue("--global-theme-color").trim(),t="dark"===document.documentElement.getAttribute("data-theme"),o=document.querySelectorAll(".scholar-badge");if(e&&o.length>0){const a=e.replace("#",""),r="fff";o.forEach(e=>{const o=e.getAttribute("data-citations");if(o){const l=t?"fff":"999999";e.src=`https://img.shields.io/badge/Cited%20by-${o}-${a}?logo=googlescholar&labelColor=${r}&style=flat`,e.style.border=`1px solid #${l}`,e.style.borderRadius="6px"}})}}document.addEventListener("DOMContentLoaded",updateScholarBadgeColors);const observer=new MutationObserver(function(e){e.forEach(function(e){"attributes"===e.type&&"data-theme"===e.attributeName&&updateScholarBadgeColors()})});observer.observe(document.documentElement,{attributes:!0,attributeFilter:["data-theme"]});</script> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">AAAI</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/cardiff-480.webp 480w,/assets/img/publication_preview/cardiff-800.webp 800w,/assets/img/publication_preview/cardiff-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/cardiff.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cardiff.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="tang2024cardiff" class="col-sm-8"> <div class="title"> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/32785" rel="external nofollow noopener" target="_blank">CaRDiff: Video Salient Object Ranking Chain of Thought Reasoning for Saliency Prediction with Diffusion</a> </div> <div class="author"> <em>Yunlong Tang</em>,Â Gen Zhan,Â Li Yang,Â Yiting Liao,Â andÂ Chenliang Xu<sup>â€ </sup> </div> <div class="periodical"> <em>In Proceedings of the AAAI Conference on Artificial Intelligence</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=xf1rCgoAAAAJ&amp;citation_for_view=xf1rCgoAAAAJ:W7OEmFMy1HYC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img class="scholar-badge" data-citations="13" src="https://img.shields.io/badge/Cited%20by-13-4285F4?logo=googlescholar&amp;labelColor=fff" alt="13 Google Scholar citations"> </a> </div> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tang2024cardiff</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{CaRDiff: Video Salient Object Ranking Chain of Thought Reasoning for Saliency Prediction with Diffusion}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tang, Yunlong and Zhan, Gen and Yang, Li and Liao, Yiting and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the AAAI Conference on Artificial Intelligence}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1609/aaai.v39i7.32785}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{39}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{7302-7310}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>function updateScholarBadgeColors(){const e=getComputedStyle(document.documentElement).getPropertyValue("--global-theme-color").trim(),t="dark"===document.documentElement.getAttribute("data-theme"),o=document.querySelectorAll(".scholar-badge");if(e&&o.length>0){const a=e.replace("#",""),r="fff";o.forEach(e=>{const o=e.getAttribute("data-citations");if(o){const l=t?"fff":"999999";e.src=`https://img.shields.io/badge/Cited%20by-${o}-${a}?logo=googlescholar&labelColor=${r}&style=flat`,e.style.border=`1px solid #${l}`,e.style.borderRadius="6px"}})}}document.addEventListener("DOMContentLoaded",updateScholarBadgeColors);const observer=new MutationObserver(function(e){e.forEach(function(e){"attributes"===e.type&&"data-theme"===e.attributeName&&updateScholarBadgeColors()})});observer.observe(document.documentElement,{attributes:!0,attributeFilter:["data-theme"]});</script> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CVPRW</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/fresca-480.webp 480w,/assets/img/publication_preview/fresca-800.webp 800w,/assets/img/publication_preview/fresca-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/fresca.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="fresca.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="huang2025fresca" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2504.02154" rel="external nofollow noopener" target="_blank">FreSca: Unveiling the Scaling Space in Diffusion Models</a> </div> <div class="author"> Chao Huang,Â Susan Liang,Â <em>Yunlong Tang</em>,Â Li Ma,Â Yapeng Tian,Â andÂ Chenliang Xu<sup>â€ </sup> </div> <div class="periodical"> <em>arXiv preprint arXiv:2504.02154</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> <a href="https://github.com/WikiChao/FreSca" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/WikiChao/FreSca?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">huang2025fresca</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{FreSca: Unveiling the Scaling Space in Diffusion Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huang, Chao and Liang, Susan and Tang, Yunlong and Ma, Li and Tian, Yapeng and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2504.02154}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>function updateScholarBadgeColors(){const e=getComputedStyle(document.documentElement).getPropertyValue("--global-theme-color").trim(),t="dark"===document.documentElement.getAttribute("data-theme"),o=document.querySelectorAll(".scholar-badge");if(e&&o.length>0){const a=e.replace("#",""),r="fff";o.forEach(e=>{const o=e.getAttribute("data-citations");if(o){const l=t?"fff":"999999";e.src=`https://img.shields.io/badge/Cited%20by-${o}-${a}?logo=googlescholar&labelColor=${r}&style=flat`,e.style.border=`1px solid #${l}`,e.style.borderRadius="6px"}})}}document.addEventListener("DOMContentLoaded",updateScholarBadgeColors);const observer=new MutationObserver(function(e){e.forEach(function(e){"attributes"===e.type&&"data-theme"===e.attributeName&&updateScholarBadgeColors()})});observer.observe(document.documentElement,{attributes:!0,attributeFilter:["data-theme"]});</script> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CVPRW</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/kubrick-480.webp 480w,/assets/img/publication_preview/kubrick-800.webp 800w,/assets/img/publication_preview/kubrick-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/kubrick.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="kubrick.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="kubrick2025" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2408.10453" rel="external nofollow noopener" target="_blank">Kubrick: Multimodal Agent Collaborations for Synthetic Video Generation</a> </div> <div class="author"> Liu He,Â Yizhi Song,Â Hejun Huang,Â Pinxin Liu,Â <em>Yunlong Tang</em>,Â Daniel Aliaga,Â andÂ Xin Zhou </div> <div class="periodical"> <em>arXiv preprint arXiv:2408.10453</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">kubrick2025</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Kubrick: Multimodal Agent Collaborations for Synthetic Video Generation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{He, Liu and Song, Yizhi and Huang, Hejun and Liu, Pinxin and Tang, Yunlong and Aliaga, Daniel and Zhou, Xin}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2408.10453}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>function updateScholarBadgeColors(){const e=getComputedStyle(document.documentElement).getPropertyValue("--global-theme-color").trim(),t="dark"===document.documentElement.getAttribute("data-theme"),o=document.querySelectorAll(".scholar-badge");if(e&&o.length>0){const a=e.replace("#",""),r="fff";o.forEach(e=>{const o=e.getAttribute("data-citations");if(o){const l=t?"fff":"999999";e.src=`https://img.shields.io/badge/Cited%20by-${o}-${a}?logo=googlescholar&labelColor=${r}&style=flat`,e.style.border=`1px solid #${l}`,e.style.borderRadius="6px"}})}}document.addEventListener("DOMContentLoaded",updateScholarBadgeColors);const observer=new MutationObserver(function(e){e.forEach(function(e){"attributes"===e.type&&"data-theme"===e.attributeName&&updateScholarBadgeColors()})});observer.observe(document.documentElement,{attributes:!0,attributeFilter:["data-theme"]});</script> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">3DV</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/gaussian_style-480.webp 480w,/assets/img/publication_preview/gaussian_style-800.webp 800w,/assets/img/publication_preview/gaussian_style-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/gaussian_style.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="gaussian_style.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="liu2024emo" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2402.00827" rel="external nofollow noopener" target="_blank">GaussianStyle: Gaussian Head Avatar via StyleGAN</a> </div> <div class="author"> Pinxin Liu,Â Luchuan Song,Â Daoan Zhang,Â Hang Hua,Â <em>Yunlong Tang</em>,Â Huaijin Tu,Â Jiebo Luo,Â andÂ Chenliang Xu<sup>â€ </sup> </div> <div class="periodical"> <em>International Conference on 3D Vision</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> <a href="https://github.com/andypinxinliu/HumanFaceProject" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/andypinxinliu/HumanFaceProject?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2024emo</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{GaussianStyle: Gaussian Head Avatar via StyleGAN}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Pinxin and Song, Luchuan and Zhang, Daoan and Hua, Hang and Tang, Yunlong and Tu, Huaijin and Luo, Jiebo and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on 3D Vision}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>function updateScholarBadgeColors(){const e=getComputedStyle(document.documentElement).getPropertyValue("--global-theme-color").trim(),t="dark"===document.documentElement.getAttribute("data-theme"),o=document.querySelectorAll(".scholar-badge");if(e&&o.length>0){const a=e.replace("#",""),r="fff";o.forEach(e=>{const o=e.getAttribute("data-citations");if(o){const l=t?"fff":"999999";e.src=`https://img.shields.io/badge/Cited%20by-${o}-${a}?logo=googlescholar&labelColor=${r}&style=flat`,e.style.border=`1px solid #${l}`,e.style.borderRadius="6px"}})}}document.addEventListener("DOMContentLoaded",updateScholarBadgeColors);const observer=new MutationObserver(function(e){e.forEach(function(e){"attributes"===e.type&&"data-theme"===e.attributeName&&updateScholarBadgeColors()})});observer.observe(document.documentElement,{attributes:!0,attributeFilter:["data-theme"]});</script> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/cat-v-480.webp 480w,/assets/img/publication_preview/cat-v-800.webp 800w,/assets/img/publication_preview/cat-v-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/cat-v.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cat-v.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="tang2025catv" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2504.05541" rel="external nofollow noopener" target="_blank">Caption Anything in Video: Fine-grained Object-centric Captioning via Spatiotemporal Multimodal Prompting</a> </div> <div class="author"> <em>Yunlong Tang</em>,Â Jing Bi,Â Chao Huang,Â Susan Liang,Â Daiki Shimada,Â Hang Hua,Â Yunzhong Xiao,Â Yizhi Song,Â Pinxin Liu,Â Mingqian Feng, and <span class="more-authors" title="click to view 9 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '9 more authors' ? 'Junjia Guo, Zhuo Liu, Luchuan Song, Ali Vosoughi, Jinxi He, Liu He, Zeliang Zhang, Jiebo Luo, Chenliang Xu&lt;sup&gt;â€ &lt;/sup&gt;' : '9 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">9 more authors</span> </div> <div class="periodical"> <em>arXiv preprint arXiv:2504.05541</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> <a href="https://github.com/yunlong10/CAT-V" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/yunlong10/CAT-V?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tang2025catv</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Caption Anything in Video: Fine-grained Object-centric Captioning via Spatiotemporal Multimodal Prompting}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tang, Yunlong and Bi, Jing and Huang, Chao and Liang, Susan and Shimada, Daiki and Hua, Hang and Xiao, Yunzhong and Song, Yizhi and Liu, Pinxin and Feng, Mingqian and Guo, Junjia and Liu, Zhuo and Song, Luchuan and Vosoughi, Ali and He, Jinxi and He, Liu and Zhang, Zeliang and Luo, Jiebo and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2504.05541}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>function updateScholarBadgeColors(){const e=getComputedStyle(document.documentElement).getPropertyValue("--global-theme-color").trim(),t="dark"===document.documentElement.getAttribute("data-theme"),o=document.querySelectorAll(".scholar-badge");if(e&&o.length>0){const a=e.replace("#",""),r="fff";o.forEach(e=>{const o=e.getAttribute("data-citations");if(o){const l=t?"fff":"999999";e.src=`https://img.shields.io/badge/Cited%20by-${o}-${a}?logo=googlescholar&labelColor=${r}&style=flat`,e.style.border=`1px solid #${l}`,e.style.borderRadius="6px"}})}}document.addEventListener("DOMContentLoaded",updateScholarBadgeColors);const observer=new MutationObserver(function(e){e.forEach(function(e){"attributes"===e.type&&"data-theme"===e.attributeName&&updateScholarBadgeColors()})});observer.observe(document.documentElement,{attributes:!0,attributeFilter:["data-theme"]});</script> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/reasoning_survey-480.webp 480w,/assets/img/publication_preview/reasoning_survey-800.webp 800w,/assets/img/publication_preview/reasoning_survey-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/reasoning_survey.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="reasoning_survey.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="bi2025mllmreasoning" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2504.03151" rel="external nofollow noopener" target="_blank">Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning</a> </div> <div class="author"> Jing Bi,Â Susan Liang,Â Xiaofei Zhou,Â Pinxin Liu,Â Junjia Guo,Â <em>Yunlong Tang</em>,Â Luchuan Song,Â Chao Huang,Â Guangyu Sun,Â Jinxi He, and <span class="more-authors" title="click to view 8 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '8 more authors' ? 'Jiarui Wu, Shu Yang, Daoan Zhang, Chen Chen, Lianggong Bruce Wen, Zhang Liu, Jiebo Luo, Chenliang Xu' : '8 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">8 more authors</span> </div> <div class="periodical"> <em>arXiv preprint arXiv:2504.03151</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://reason.jing.vision/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=xf1rCgoAAAAJ&amp;citation_for_view=xf1rCgoAAAAJ:MXK_kJrjxJIC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img class="scholar-badge" data-citations="10" src="https://img.shields.io/badge/Cited%20by-10-4285F4?logo=googlescholar&amp;labelColor=fff" alt="10 Google Scholar citations"> </a> </div> <a href="https://github.com/jing-bi/awesome-M.LLM-reasoning" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/jing-bi/awesome-M.LLM-reasoning?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> </div> </div> <script>function updateScholarBadgeColors(){const e=getComputedStyle(document.documentElement).getPropertyValue("--global-theme-color").trim(),t="dark"===document.documentElement.getAttribute("data-theme"),o=document.querySelectorAll(".scholar-badge");if(e&&o.length>0){const a=e.replace("#",""),r="fff";o.forEach(e=>{const o=e.getAttribute("data-citations");if(o){const l=t?"fff":"999999";e.src=`https://img.shields.io/badge/Cited%20by-${o}-${a}?logo=googlescholar&labelColor=${r}&style=flat`,e.style.border=`1px solid #${l}`,e.style.borderRadius="6px"}})}}document.addEventListener("DOMContentLoaded",updateScholarBadgeColors);const observer=new MutationObserver(function(e){e.forEach(function(e){"attributes"===e.type&&"data-theme"===e.attributeName&&updateScholarBadgeColors()})});observer.observe(document.documentElement,{attributes:!0,attributeFilter:["data-theme"]});</script> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/verify-480.webp 480w,/assets/img/publication_preview/verify-800.webp 800w,/assets/img/publication_preview/verify-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/verify.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="verify.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="bi2025verify" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2503.11557" rel="external nofollow noopener" target="_blank">VERIFY: A Benchmark of Visual Explanation and Reasoning for Investigating Multimodal Reasoning Fidelity</a> </div> <div class="author"> Jing Bi,Â Junjia Guo,Â Susan Liang,Â Guangyu Sun,Â Luchuan Song,Â <em>Yunlong Tang</em>,Â Jinxi He,Â Jiarui Wu,Â Ali Vosoughi,Â Chen Chen, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Chenliang Xu' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>arXiv preprint arXiv:2503.11557</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> <a href="https://verify-eqh.pages.dev/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">bi2025verify</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{VERIFY: A Benchmark of Visual Explanation and Reasoning for Investigating Multimodal Reasoning Fidelity}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bi, Jing and Guo, Junjia and Liang, Susan and Sun, Guangyu and Song, Luchuan and Tang, Yunlong and He, Jinxi and Wu, Jiarui and Vosoughi, Ali and Chen, Chen and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2503.11557}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>function updateScholarBadgeColors(){const e=getComputedStyle(document.documentElement).getPropertyValue("--global-theme-color").trim(),t="dark"===document.documentElement.getAttribute("data-theme"),o=document.querySelectorAll(".scholar-badge");if(e&&o.length>0){const a=e.replace("#",""),r="fff";o.forEach(e=>{const o=e.getAttribute("data-citations");if(o){const l=t?"fff":"999999";e.src=`https://img.shields.io/badge/Cited%20by-${o}-${a}?logo=googlescholar&labelColor=${r}&style=flat`,e.style.border=`1px solid #${l}`,e.style.borderRadius="6px"}})}}document.addEventListener("DOMContentLoaded",updateScholarBadgeColors);const observer=new MutationObserver(function(e){e.forEach(function(e){"attributes"===e.type&&"data-theme"===e.attributeName&&updateScholarBadgeColors()})});observer.observe(document.documentElement,{attributes:!0,attributeFilter:["data-theme"]});</script> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ACM MM</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/teaser-eagle-480.webp 480w,/assets/img/publication_preview/teaser-eagle-800.webp 800w,/assets/img/publication_preview/teaser-eagle-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/teaser-eagle.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="teaser-eagle.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="bi2024eagle" class="col-sm-8"> <div class="title"> <a href="https://dl.acm.org/doi/10.1145/3664647.3681618" rel="external nofollow noopener" target="_blank">EAGLE: Egocentric AGgregated Language-video Engine</a> </div> <div class="author"> Jing Bi,Â <em>Yunlong Tang</em>,Â Luchuan Song,Â Ali Vosoughi,Â Nguyen Nguyen,Â andÂ Chenliang Xu<sup>â€ </sup> </div> <div class="periodical"> <em>In Proceedings of the 32nd ACM International Conference on Multimedia</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> <a href="https://page-eagle.jing.vision/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=xf1rCgoAAAAJ&amp;citation_for_view=xf1rCgoAAAAJ:Y0pCki6q_DkC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img class="scholar-badge" data-citations="12" src="https://img.shields.io/badge/Cited%20by-12-4285F4?logo=googlescholar&amp;labelColor=fff" alt="12 Google Scholar citations"> </a> </div> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">bi2024eagle</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{EAGLE: Egocentric AGgregated Language-video Engine}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bi, Jing and Tang, Yunlong and Song, Luchuan and Vosoughi, Ali and Nguyen, Nguyen and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 32nd ACM International Conference on Multimedia}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>function updateScholarBadgeColors(){const e=getComputedStyle(document.documentElement).getPropertyValue("--global-theme-color").trim(),t="dark"===document.documentElement.getAttribute("data-theme"),o=document.querySelectorAll(".scholar-badge");if(e&&o.length>0){const a=e.replace("#",""),r="fff";o.forEach(e=>{const o=e.getAttribute("data-citations");if(o){const l=t?"fff":"999999";e.src=`https://img.shields.io/badge/Cited%20by-${o}-${a}?logo=googlescholar&labelColor=${r}&style=flat`,e.style.border=`1px solid #${l}`,e.style.borderRadius="6px"}})}}document.addEventListener("DOMContentLoaded",updateScholarBadgeColors);const observer=new MutationObserver(function(e){e.forEach(function(e){"attributes"===e.type&&"data-theme"===e.attributeName&&updateScholarBadgeColors()})});observer.observe(document.documentElement,{attributes:!0,attributeFilter:["data-theme"]});</script> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ECCVW</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/eccv-aim-480.webp 480w,/assets/img/publication_preview/eccv-aim-800.webp 800w,/assets/img/publication_preview/eccv-aim-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/eccv-aim.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="eccv-aim.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="moskalenko2024aim" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2409.14827" rel="external nofollow noopener" target="_blank">AIM 2024 Challenge on Video Saliency Prediction: Methods and Results</a> </div> <div class="author"> Andrey Moskalenko,Â Alexey Bryncev,Â Dmitry Vatolin,Â Radu Timofte,Â Gen Zhan,Â Li Yang,Â <em>Yunlong Tang</em>,Â Yiting Liao,Â Jiongzhi Lin,Â Baitao Huang, and <span class="more-authors" title="click to view 23 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '23 more authors' ? 'Morteza Moradi, Mohammad Moradi, Francesco Rundo, Concetto Spampinato, Ali Borji, Simone Palazzo, Yuxin Zhu, Yinan Sun, Huiyu Duan, Yuqin Cao, Ziheng Jia, Qiang Hu, Xiongkuo Min, Guangtao Zhai, Hao Fang, Runmin Cong, Xiankai Lu, Xiaofei Zhou, Wei Zhang, Chunyu Zhao, Wentao Mu, Tao Deng, Hamed R Tavakoli' : '23 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">23 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the European Conference on Computer Vision Workshops</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> <a href="https://challenges.videoprocessing.ai/challenges/video-saliency-prediction-leaderboard.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="https://github.com/msu-video-group/ECCVW24_Saliency_Prediction" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/msu-video-group/ECCVW24_Saliency_Prediction?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">moskalenko2024aim</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{AIM 2024 Challenge on Video Saliency Prediction: Methods and Results}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Moskalenko, Andrey and Bryncev, Alexey and Vatolin, Dmitry and Timofte, Radu and Zhan, Gen and Yang, Li and Tang, Yunlong and Liao, Yiting and Lin, Jiongzhi and Huang, Baitao and Moradi, Morteza and Moradi, Mohammad and Rundo, Francesco and Spampinato, Concetto and Borji, Ali and Palazzo, Simone and Zhu, Yuxin and Sun, Yinan and Duan, Huiyu and Cao, Yuqin and Jia, Ziheng and Hu, Qiang and Min, Xiongkuo and Zhai, Guangtao and Fang, Hao and Cong, Runmin and Lu, Xiankai and Zhou, Xiaofei and Zhang, Wei and Zhao, Chunyu and Mu, Wentao and Deng, Tao and Tavakoli, Hamed R}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the European Conference on Computer Vision Workshops}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>function updateScholarBadgeColors(){const e=getComputedStyle(document.documentElement).getPropertyValue("--global-theme-color").trim(),t="dark"===document.documentElement.getAttribute("data-theme"),o=document.querySelectorAll(".scholar-badge");if(e&&o.length>0){const a=e.replace("#",""),r="fff";o.forEach(e=>{const o=e.getAttribute("data-citations");if(o){const l=t?"fff":"999999";e.src=`https://img.shields.io/badge/Cited%20by-${o}-${a}?logo=googlescholar&labelColor=${r}&style=flat`,e.style.border=`1px solid #${l}`,e.style.borderRadius="6px"}})}}document.addEventListener("DOMContentLoaded",updateScholarBadgeColors);const observer=new MutationObserver(function(e){e.forEach(function(e){"attributes"===e.type&&"data-theme"===e.attributeName&&updateScholarBadgeColors()})});observer.observe(document.documentElement,{attributes:!0,attributeFilter:["data-theme"]});</script> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mmcomp-480.webp 480w,/assets/img/publication_preview/mmcomp-800.webp 800w,/assets/img/publication_preview/mmcomp-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/mmcomp.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mmcomp.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hua2024mmcomposition" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2410.09733" rel="external nofollow noopener" target="_blank">MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models</a> </div> <div class="author"> Hang Hua<sup>*</sup>,Â <em>Yunlong Tang<sup>*</sup></em>,Â Ziyun Zeng<sup>*</sup>,Â Liangliang Cao,Â Zhengyuan Yang,Â Hangfeng He,Â Chenliang Xu,Â andÂ Jiebo Luo </div> <div class="periodical"> <em>arXiv preprint arXiv:2410.09733</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> <a href="https://hanghuacs.github.io/MMComposition/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=xf1rCgoAAAAJ&amp;citation_for_view=xf1rCgoAAAAJ:eQOLeE2rZwMC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img class="scholar-badge" data-citations="13" src="https://img.shields.io/badge/Cited%20by-13-4285F4?logo=googlescholar&amp;labelColor=fff" alt="13 Google Scholar citations"> </a> </div> <a href="https://github.com/hanghuacs/MMComposition" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/hanghuacs/MMComposition?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hua2024mmcomposition</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hua, Hang and Tang, Yunlong and Zeng, Ziyun and Cao, Liangliang and Yang, Zhengyuan and He, Hangfeng and Xu, Chenliang and Luo, Jiebo}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2410.09733}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>function updateScholarBadgeColors(){const e=getComputedStyle(document.documentElement).getPropertyValue("--global-theme-color").trim(),t="dark"===document.documentElement.getAttribute("data-theme"),o=document.querySelectorAll(".scholar-badge");if(e&&o.length>0){const a=e.replace("#",""),r="fff";o.forEach(e=>{const o=e.getAttribute("data-citations");if(o){const l=t?"fff":"999999";e.src=`https://img.shields.io/badge/Cited%20by-${o}-${a}?logo=googlescholar&labelColor=${r}&style=flat`,e.style.border=`1px solid #${l}`,e.style.borderRadius="6px"}})}}document.addEventListener("DOMContentLoaded",updateScholarBadgeColors);const observer=new MutationObserver(function(e){e.forEach(function(e){"attributes"===e.type&&"data-theme"===e.attributeName&&updateScholarBadgeColors()})});observer.observe(document.documentElement,{attributes:!0,attributeFilter:["data-theme"]});</script> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/DBD-teaser-480.webp 480w,/assets/img/publication_preview/DBD-teaser-800.webp 800w,/assets/img/publication_preview/DBD-teaser-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/DBD-teaser.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="DBD-teaser.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="feng2024more" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2406.12663" rel="external nofollow noopener" target="_blank">Do More Details Always Introduce More Hallucinations in LVLM-based Image Captioning?</a> </div> <div class="author"> Mingqian Feng,Â <em>Yunlong Tang</em>,Â Zeliang Zhang,Â andÂ Chenliang Xu<sup>â€ </sup> </div> <div class="periodical"> <em>arXiv preprint arXiv:2406.12663</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">feng2024more</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Do More Details Always Introduce More Hallucinations in LVLM-based Image Captioning?}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Feng, Mingqian and Tang, Yunlong and Zhang, Zeliang and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2406.12663}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>function updateScholarBadgeColors(){const e=getComputedStyle(document.documentElement).getPropertyValue("--global-theme-color").trim(),t="dark"===document.documentElement.getAttribute("data-theme"),o=document.querySelectorAll(".scholar-badge");if(e&&o.length>0){const a=e.replace("#",""),r="fff";o.forEach(e=>{const o=e.getAttribute("data-citations");if(o){const l=t?"fff":"999999";e.src=`https://img.shields.io/badge/Cited%20by-${o}-${a}?logo=googlescholar&labelColor=${r}&style=flat`,e.style.border=`1px solid #${l}`,e.style.borderRadius="6px"}})}}document.addEventListener("DOMContentLoaded",updateScholarBadgeColors);const observer=new MutationObserver(function(e){e.forEach(function(e){"attributes"===e.type&&"data-theme"===e.attributeName&&updateScholarBadgeColors()})});observer.observe(document.documentElement,{attributes:!0,attributeFilter:["data-theme"]});</script> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/scaling-concept-480.webp 480w,/assets/img/publication_preview/scaling-concept-800.webp 800w,/assets/img/publication_preview/scaling-concept-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/scaling-concept.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="scaling-concept.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="huang2024scalingconcept" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2410.24151" rel="external nofollow noopener" target="_blank">Scaling Concept with Text-Guided Diffusion Models</a> </div> <div class="author"> Chao Huang,Â Susan Liang,Â <em>Yunlong Tang</em>,Â Yapeng Tian,Â Anurag Kumar,Â andÂ Chenliang Xu<sup>â€ </sup> </div> <div class="periodical"> <em>arXiv preprint arXiv:2410.24151</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> <a href="https://wikichao.github.io/ScalingConcept/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="https://github.com/WikiChao/ScalingConcept" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/WikiChao/ScalingConcept?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">huang2024scalingconcept</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Scaling Concept with Text-Guided Diffusion Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huang, Chao and Liang, Susan and Tang, Yunlong and Tian, Yapeng and Kumar, Anurag and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2410.24151}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>function updateScholarBadgeColors(){const e=getComputedStyle(document.documentElement).getPropertyValue("--global-theme-color").trim(),t="dark"===document.documentElement.getAttribute("data-theme"),o=document.querySelectorAll(".scholar-badge");if(e&&o.length>0){const a=e.replace("#",""),r="fff";o.forEach(e=>{const o=e.getAttribute("data-citations");if(o){const l=t?"fff":"999999";e.src=`https://img.shields.io/badge/Cited%20by-${o}-${a}?logo=googlescholar&labelColor=${r}&style=flat`,e.style.border=`1px solid #${l}`,e.style.borderRadius="6px"}})}}document.addEventListener("DOMContentLoaded",updateScholarBadgeColors);const observer=new MutationObserver(function(e){e.forEach(function(e){"attributes"===e.type&&"data-theme"===e.attributeName&&updateScholarBadgeColors()})});observer.observe(document.documentElement,{attributes:!0,attributeFilter:["data-theme"]});</script> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CVPRW</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/llmva-gebc-480.webp 480w,/assets/img/publication_preview/llmva-gebc-800.webp 800w,/assets/img/publication_preview/llmva-gebc-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/llmva-gebc.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="llmva-gebc.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="tang2023llmva" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2306.10354" rel="external nofollow noopener" target="_blank">LLMVA-GEBC: Large Language Model with Video Adapter for Generic Event Boundary Captioning</a> </div> <div class="author"> <em>Yunlong Tang</em>,Â Jinrui Zhang,Â Xiangchen Wang,Â Teng Wang,Â andÂ Feng Zheng<sup>â€ </sup> </div> <div class="periodical"> <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> <a href="https://github.com/zjr2000/LLMVA-GEBC" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/zjr2000/LLMVA-GEBC?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tang2023llmva</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{LLMVA-GEBC: Large Language Model with Video Adapter for Generic Event Boundary Captioning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tang, Yunlong and Zhang, Jinrui and Wang, Xiangchen and Wang, Teng and Zheng, Feng}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>function updateScholarBadgeColors(){const e=getComputedStyle(document.documentElement).getPropertyValue("--global-theme-color").trim(),t="dark"===document.documentElement.getAttribute("data-theme"),o=document.querySelectorAll(".scholar-badge");if(e&&o.length>0){const a=e.replace("#",""),r="fff";o.forEach(e=>{const o=e.getAttribute("data-citations");if(o){const l=t?"fff":"999999";e.src=`https://img.shields.io/badge/Cited%20by-${o}-${a}?logo=googlescholar&labelColor=${r}&style=flat`,e.style.border=`1px solid #${l}`,e.style.borderRadius="6px"}})}}document.addEventListener("DOMContentLoaded",updateScholarBadgeColors);const observer=new MutationObserver(function(e){e.forEach(function(e){"attributes"===e.type&&"data-theme"===e.attributeName&&updateScholarBadgeColors()})});observer.observe(document.documentElement,{attributes:!0,attributeFilter:["data-theme"]});</script> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICMC</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/launchpadgpt-480.webp 480w,/assets/img/publication_preview/launchpadgpt-800.webp 800w,/assets/img/publication_preview/launchpadgpt-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/launchpadgpt.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="launchpadgpt.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="xu2023launchpadgpt" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2307.04827" rel="external nofollow noopener" target="_blank">LaunchpadGPT: Language Model as Music Visualization Designer on Launchpad</a> </div> <div class="author"> Siting Xu<sup>*</sup>,Â <em>Yunlong Tang<sup>*</sup></em>,Â andÂ Feng Zheng<sup>â€ </sup> </div> <div class="periodical"> <em>In Proceedings of the International Computer Music Conference</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> <a href="https://github.com/yunlong10/LaunchpadGPT" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/yunlong10/LaunchpadGPT?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xu2023launchpadgpt</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{LaunchpadGPT: Language Model as Music Visualization Designer on Launchpad}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu, Siting and Tang, Yunlong and Zheng, Feng}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the International Computer Music Conference}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{213-217}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>function updateScholarBadgeColors(){const e=getComputedStyle(document.documentElement).getPropertyValue("--global-theme-color").trim(),t="dark"===document.documentElement.getAttribute("data-theme"),o=document.querySelectorAll(".scholar-badge");if(e&&o.length>0){const a=e.replace("#",""),r="fff";o.forEach(e=>{const o=e.getAttribute("data-citations");if(o){const l=t?"fff":"999999";e.src=`https://img.shields.io/badge/Cited%20by-${o}-${a}?logo=googlescholar&labelColor=${r}&style=flat`,e.style.border=`1px solid #${l}`,e.style.borderRadius="6px"}})}}document.addEventListener("DOMContentLoaded",updateScholarBadgeColors);const observer=new MutationObserver(function(e){e.forEach(function(e){"attributes"===e.type&&"data-theme"===e.attributeName&&updateScholarBadgeColors()})});observer.observe(document.documentElement,{attributes:!0,attributeFilter:["data-theme"]});</script> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/cat-480.webp 480w,/assets/img/publication_preview/cat-800.webp 800w,/assets/img/publication_preview/cat-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/cat.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cat.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2023caption" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2305.02677" rel="external nofollow noopener" target="_blank">Caption Anything: Interactive Image Description with Diverse Multimodal Controls</a> </div> <div class="author"> Teng Wang<sup>*</sup>,Â Jinrui Zhang<sup>*</sup>,Â Junjie Fei<sup>*</sup>,Â Hao Zheng,Â <em>Yunlong Tang</em>,Â Zhe Li,Â Mingqi Gao,Â andÂ Shanshan Zhao </div> <div class="periodical"> <em>arXiv preprint arXiv:2305.02677</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=xf1rCgoAAAAJ&amp;citation_for_view=xf1rCgoAAAAJ:u-x6o8ySG0sC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img class="scholar-badge" data-citations="130" src="https://img.shields.io/badge/Cited%20by-130-4285F4?logo=googlescholar&amp;labelColor=fff" alt="130 Google Scholar citations"> </a> </div> <a href="https://github.com/ttengwang/Caption-Anything" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/ttengwang/Caption-Anything?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">wang2023caption</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Caption Anything: Interactive Image Description with Diverse Multimodal Controls}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Teng and Zhang, Jinrui and Fei, Junjie and Zheng, Hao and Tang, Yunlong and Li, Zhe and Gao, Mingqi and Zhao, Shanshan}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2305.02677}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>function updateScholarBadgeColors(){const e=getComputedStyle(document.documentElement).getPropertyValue("--global-theme-color").trim(),t="dark"===document.documentElement.getAttribute("data-theme"),o=document.querySelectorAll(".scholar-badge");if(e&&o.length>0){const a=e.replace("#",""),r="fff";o.forEach(e=>{const o=e.getAttribute("data-citations");if(o){const l=t?"fff":"999999";e.src=`https://img.shields.io/badge/Cited%20by-${o}-${a}?logo=googlescholar&labelColor=${r}&style=flat`,e.style.border=`1px solid #${l}`,e.style.borderRadius="6px"}})}}document.addEventListener("DOMContentLoaded",updateScholarBadgeColors);const observer=new MutationObserver(function(e){e.forEach(function(e){"attributes"===e.type&&"data-theme"===e.attributeName&&updateScholarBadgeColors()})});observer.observe(document.documentElement,{attributes:!0,attributeFilter:["data-theme"]});</script> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ACCV</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/m_san-480.webp 480w,/assets/img/publication_preview/m_san-800.webp 800w,/assets/img/publication_preview/m_san-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/m_san.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="m_san.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Tang_2022_ACCV" class="col-sm-8"> <div class="title"> <a href="https://openaccess.thecvf.com/content/ACCV2022/html/Tang_Multi-modal_Segment_Assemblage_Network_for_Ad_Video_Editing_with_Importance-Coherence_ACCV_2022_paper.html" rel="external nofollow noopener" target="_blank">Multi-modal Segment Assemblage Network for Ad Video Editing with Importance-Coherence Reward</a> </div> <div class="author"> <em>Yunlong Tang</em>,Â Siting Xu,Â Teng Wang,Â Qin Lin,Â Qinglin Lu,Â andÂ Feng Zheng<sup>â€ </sup> </div> <div class="periodical"> <em>In Proceedings of the Asian Conference on Computer Vision</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=xf1rCgoAAAAJ&amp;citation_for_view=xf1rCgoAAAAJ:u5HHmVD_uO8C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img class="scholar-badge" data-citations="11" src="https://img.shields.io/badge/Cited%20by-11-4285F4?logo=googlescholar&amp;labelColor=fff" alt="11 Google Scholar citations"> </a> </div> <a href="https://github.com/yunlong10/Ads-1k" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/yunlong10/Ads-1k?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Tang_2022_ACCV</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tang, Yunlong and Xu, Siting and Wang, Teng and Lin, Qin and Lu, Qinglin and Zheng, Feng}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multi-modal Segment Assemblage Network for Ad Video Editing with Importance-Coherence Reward}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Asian Conference on Computer Vision}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3519-3535}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> <script>function updateScholarBadgeColors(){const e=getComputedStyle(document.documentElement).getPropertyValue("--global-theme-color").trim(),t="dark"===document.documentElement.getAttribute("data-theme"),o=document.querySelectorAll(".scholar-badge");if(e&&o.length>0){const a=e.replace("#",""),r="fff";o.forEach(e=>{const o=e.getAttribute("data-citations");if(o){const l=t?"fff":"999999";e.src=`https://img.shields.io/badge/Cited%20by-${o}-${a}?logo=googlescholar&labelColor=${r}&style=flat`,e.style.border=`1px solid #${l}`,e.style.borderRadius="6px"}})}}document.addEventListener("DOMContentLoaded",updateScholarBadgeColors);const observer=new MutationObserver(function(e){e.forEach(function(e){"attributes"===e.type&&"data-theme"===e.attributeName&&updateScholarBadgeColors()})});observer.observe(document.documentElement,{attributes:!0,attributeFilter:["data-theme"]});</script> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2025 Yunlong (Yolo) Tang. Theme by <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="external nofollow noopener">al-folio</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script>for(var wechatModal=document.getElementById("WeChatMod"),wechatBtn=document.querySelectorAll('[id="WeChatBtn"]'),i=0;i<wechatBtn.length;i++)wechatBtn[i].onclick=function(){wechatModal.style.display="block"};window.onclick=function(t){t.target==wechatModal&&(wechatModal.style.display="none")};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"Publications",description:"* Equal Contribution | \u2020 Corresponding Author",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/2021/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2021/distill/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/blog/2020/twitter/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march &amp; april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-started-my-part-time-internship-at-tencent-in-shenzhen-with-supervision-from-dr-wenhao-jiang-and-qin-lin-i-had-to-balance-it-with-my-university-coursework",title:"Started my part-time internship at Tencent in Shenzhen, with supervision from Dr. Wenhao...",description:"",section:"News"},{id:"news-i-left-tencent-and-joined-sustech-vip-lab-as-an-undergraduate-student-researcher",title:"I left Tencent and joined SUSTech VIP Lab as an undergraduate student researcher....",description:"",section:"News"},{id:"news-one-paper-about-multimodal-ad-video-editing-has-been-accepted-to-asian-conference-on-computer-vision-accv-2022",title:"One paper about multimodal Ad video editing has been accepted to Asian Conference...",description:"",section:"News"},{id:"news-thrilled-to-announce-i-ll-be-starting-my-ph-d-in-computer-science-at-the-university-of-rochester-from-fall-2023-working-with-prof-chenliang-xu",title:"Thrilled to announce I\u2019ll be starting my Ph.D. in Computer Science at the...",description:"",section:"News"},{id:"news-our-project-caption-anything-has-been-released-welcome-to-try-our-demo-and-star-our-github-repo",title:"Our project Caption Anything has been released! Welcome to try our demo and...",description:"",section:"News"},{id:"news-the-technical-report-for-caption-anything-has-been-released",title:"The technical report for Caption Anything has been released!",description:"",section:"News"},{id:"news-successfully-defended-my-undergraduate-thesis-titled-language-guided-video-cover-generation-which-has-been-awarded-the-excellent-undergraduate-thesis",title:"Successfully defended my undergraduate thesis titled Language-Guided Video Cover Generation, which has been...",description:"",section:"News"},{id:"news-our-team-won-the-first-place-in-loveu-long-form-video-understanding-challenge-at-cvpr-23-workshop",title:"Our team won the first place in LOVEU (Long-form Video Understanding) Challenge at...",description:"",section:"News"},{id:"news-graduated-from-sustech-obtained-my-bachelor-s-degree-and-the-honor-of-excellent-graduate-for-exceptional-performance",title:"Graduated from SUSTech, obtained my bachelor\u2019s degree and the honor of Excellent Graduate...",description:"",section:"News"},{id:"news-one-paper-accepted-to-international-computer-music-conference-icmc-2023",title:"One paper accepted to International Computer Music Conference (ICMC) 2023.",description:"",section:"News"},{id:"news-officially-joined-in-the-chenliang-xu-s-group-at-ur-cs-as-a-ph-d-student",title:"Officially joined in the Chenliang Xu\u2019s Group at UR CS as a Ph.D....",description:"",section:"News"},{id:"news-released-a-survey-for-video-understanding-with-llms-arxiv-github",title:"\ud83d\udd25\ud83d\udd25\ud83d\udd25 Released a survey for Video Understanding with LLMs (arXiv, GitHub).",description:"",section:"News"},{id:"news-i-will-join-bytedance-as-a-research-intern-this-summer",title:"I will join ByteDance as a Research Intern this summer.",description:"",section:"News"},{id:"news-released-avicuna-an-audio-visual-llm-empowered-by-pseudo-untrimmed-video-annotations-for-audio-visual-event-localization",title:"Released AVicuna, an Audio-Visual LLM empowered by pseudo-untrimmed video annotations for audio-visual event...",description:"",section:"News"},{id:"news-introducing-v2xum-llama-model-and-instruct-v2xum-dataset-for-cross-modal-video-summarization",title:"Introducing V2Xum-LLaMA model and Instruct-V2Xum dataset for cross-modal video summarization.",description:"",section:"News"},{id:"news-started-my-internship-at-bytedance-in-san-jose-ca-mentored-by-yiting-liao-amp-amp-gen-zhan",title:"\ud83c\udf1f Started my internship at ByteDance in San Jose, CA, mentored by Yiting...",description:"",section:"News"},{id:"news-introducing-differentiated-beam-decoding-dbd-a-novel-decoding-strategy-for-lvlm-hallucination-mitigation",title:"Introducing Differentiated Beam Decoding (DBD), a novel decoding strategy for LVLM hallucination mitigation....",description:"",section:"News"},{id:"news-one-paper-about-egocentric-video-understanding-with-llm-has-been-accepted-to-acm-mm-2024",title:"One paper about egocentric video understanding with LLM has been accepted to ACM...",description:"",section:"News"},{id:"news-we-39-ve-recently-updated-our-survey-quot-video-understanding-with-large-language-models-a-survey-quot",title:"\ud83d\udce2 We&#39;ve recently updated our survey: &quot;Video Understanding with Large Language Models: A...",description:"",section:"News",handler:()=>{window.location.href="/news/2024-07-23/"}},{id:"news-we-won-the-first-place-in-aim-2024-challenge-on-video-saliency-prediction-eccv-workshop-thanks-to-gen-zhan-and-li-yang",title:"\ud83c\udfc5 We won the first place in AIM 2024 Challenge on Video Saliency...",description:"",section:"News"},{id:"news-introducing-cardiff-a-framework-for-video-saliency-prediction-using-mllm-cot-reasoning-and-diffusion-model",title:"Introducing CaRDiff, a framework for video saliency prediction using MLLM CoT reasoning and...",description:"",section:"News"},{id:"news-mmcomposition-has-been-publicly-released-read-our-paper-check-out-the-latest-leaderboard-and-access-the-code-to-evaluate-your-own-models",title:"\ud83d\ude80 MMComposition has been publicly released. Read our Paper, check out the latest...",description:"",section:"News"},{id:"news-we-have-released-vidcomposition-a-benchmark-to-evaluate-mllms-understanding-of-video-compositions-project-page-paper-leaderboard",title:"We have released VidComposition, a benchmark to evaluate MLLMs\u2019 understanding of video compositions....",description:"",section:"News"},{id:"news-three-papers-on-video-llms-have-been-accepted-to-aaai-2025",title:"\ud83c\udf89 Three papers on Video-LLMs have been accepted to AAAI 2025!",description:"",section:"News"},{id:"news-introducing-our-survey-paper-on-genai-for-cel-animation-arxiv-github",title:"\ud83c\udfa8 Introducing our survey paper on GenAI for Cel-Animation \ud83d\udc49 arXiv | GitHub...",description:"",section:"News"},{id:"news-i-will-join-amazon-as-an-applied-scientist-intern-this-summer",title:"I will join Amazon as an Applied Scientist Intern this summer.",description:"",section:"News"},{id:"news-two-papers-have-been-accepted-to-cvpr-2025-including-our-vidcomposition-benchmark",title:"\ud83c\udf89 Two papers have been accepted to CVPR 2025, including our VidComposition benchmark!...",description:"",section:"News"},{id:"news-caption-anything-in-video-cat-v-has-been-released-arxiv-github",title:"\ud83d\udcf7 Caption Anything in Video (CAT-V) has been released \ud83d\udc49 arXiv | GitHub...",description:"",section:"News"},{id:"news-our-vid-llm-survey-has-been-accepted-by-the-ieee-transactions-on-circuits-and-systems-for-video-technology-tcsvt-ieee-xplore-github",title:"\ud83c\udf89 Our Vid-LLM survey has been accepted by the IEEE Transactions on Circuits...",description:"",section:"News"},{id:"news-started-my-internship-as-an-applied-scientist-intern-at-amazon-in-bellevue-wa",title:"\ud83c\udf1f Started my internship as an Applied Scientist Intern at Amazon in Bellevue,...",description:"",section:"News"},{id:"news-introducing-mmperspective-a-comprehensive-benchmark-for-mllms-on-perspective-understanding",title:"\ud83d\udcd0 Introducing MMPerspective, a comprehensive benchmark for MLLMs on perspective understanding.",description:"",section:"News"},{id:"projects-genai-for-cel-animation",title:"GenAI for Cel-Animation",description:"[ICCVW 2025] \ud83c\udfa8 A Comprehensive Survey on GenAI for Cel-Animation.",section:"Projects",handler:()=>{window.location.href="/projects/ai4anime/"}},{id:"projects-cat-\ufe0f",title:"CAT\uff3c(=^\u2025^)\u270f\ufe0f",description:"Caption-Anything is a versatile image processing tool that combines the capabilities of Segment Anything, Visual Captioning, and ChatGPT. Our solution generates descriptive captions for any object within an image, offering a range of language styles to accommodate diverse user preferences.",section:"Projects",handler:()=>{window.location.href="/projects/caption_anything/"}},{id:"projects-captionanything-in-video-cat-v",title:"CaptionAnything in Video (CAT-V)",description:"CAT-V is a training-free framework that enables fine-grained object-centric video captioning through spatiotemporal visual prompting and chain-of-thought reasoning.",section:"Projects",handler:()=>{window.location.href="/projects/cat-v/"}},{id:"projects-launchpadgpt",title:"LaunchpadGPT",description:"[ICMC 2023] \ud83c\udfb5 LaunchpadGPT: Language Model as Music Visualization Designer on Launchpad",section:"Projects",handler:()=>{window.location.href="/projects/launchpadgpt/"}},{id:"projects-mmcomposition",title:"MMComposition",description:"Benchmarking the compositionality capabilities of VLMs \ud83e\udd2f",section:"Projects",handler:()=>{window.location.href="/projects/mmcomposition/"}},{id:"projects-mmperspective",title:"MMPerspective",description:"[NeurIPS 2025] Do MLLMs Understand Perspective? A Comprehensive Benchmark for Perspective Perception, Reasoning, and Robustness is provided in this project.",section:"Projects",handler:()=>{window.location.href="/projects/mmperspective/"}},{id:"projects-scaling-concept",title:"Scaling Concept",description:"We use pretrained text-guided diffusion models to scale up/down concepts in image/audio.",section:"Projects",handler:()=>{window.location.href="/projects/scaling_concept/"}},{id:"projects-vidcomposition",title:"VidComposition",description:"[CVPR 2025] \ud83c\udfc6 See how Top MLLMs understand video compositions.",section:"Projects",handler:()=>{window.location.href="/projects/vidcomposition/"}},{id:"projects-vid-llm-survey",title:"Vid-LLM Survey",description:"[TCSVT] \ud83d\udd25 Video Understanding with Large Language Models: A Survey",section:"Projects",handler:()=>{window.location.href="/projects/vidllm_survey/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%79%75%6E%6C%6F%6E%67.%74%61%6E%67@%72%6F%63%68%65%73%74%65%72.%65%64%75","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=xf1rCgoAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/yunlong10","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/yolo-yunlong-tang","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/yoloytang","_blank")}},{id:"socials-work",title:"Work",section:"Socials",handler:()=>{window.open("https://yunlong10.github.io/assets/pdf/cv_yunlong_tang.pdf","_blank")}},{id:"socials-youtube",title:"YouTube",section:"Socials",handler:()=>{window.open("https://youtube.com/@yoloytang","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>