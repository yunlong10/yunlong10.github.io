<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Yunlong (Yolo) Tang </title> <meta name="author" content="Yunlong (Yolo) Tang"> <meta name="description" content="CS PhD Student | Multimodal Learning | Video Understanding "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo/icon.jpg?313e0410176f41a6e55d7f7cd141e7a4"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yunlong10.github.io/"> <script src="/assets/js/theme.js?6eeff2fb375dfabd5e33c4eefb1fadbc"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%79%75%6E%6C%6F%6E%67.%74%61%6E%67@%72%6F%63%68%65%73%74%65%72.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=xf1rCgoAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/yunlong10" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/yolo-yunlong-tang" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/yoloytang" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> <a href="https://youtube.com/@yoloytang" title="YouTube" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i></a> <a id="WeChatBtn" title="WeChat"><i class="fa-brands fa-weixin"></i></a> <div id="WeChatMod" class="wechat-modal"> <img src="/assets/img/wechat-qr.png" alt="WeChat QR" id="WeChatQR"> </div> <script>for(var wechatModal=document.getElementById("WeChatMod"),wechatBtn=document.querySelectorAll('[id="WeChatBtn"]'),i=0;i<wechatBtn.length;i++)wechatBtn[i].onclick=function(){wechatModal.style.display="block"};window.onclick=function(t){t.target==wechatModal&&(wechatModal.style.display="none")};</script> <a href="https://yunlong10.github.io/assets/pdf/cv_yunlong_tang.pdf" title="Work"><i class="ai ai-cv"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Yunlong</span> (Yolo) Tang </h1> <p class="desc"><span style="color:gray">[ Yolo Y. Tang • /<i>ˈjoʊloʊ tɛn </i>/ • 芸珑 • she/her/hers ]</span></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/website/prof_pic-480.webp 480w,/assets/img/website/prof_pic-800.webp 800w,/assets/img/website/prof_pic-1400.webp 1400w," sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/website/prof_pic.jpg?5bb39769a0f89bb6bb70215af9f4e4b9" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="website/prof_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"><center><div style="color:white; margin-top: -35px; margin-bottom: 15px; font-style: italic; white-space: nowrap; font-size: 0.65em; width: 100%; max-width: 100%; text-shadow: 1.5px 1.5px 3px rgba(0,0,0,0.6), 0 0 6px rgba(0,0,0,0.3);">Photo credit: <a href="https://sustcsonglin.github.io/" style="color: white; text-shadow: 1px 1px 1.5px rgba(0,0,0,0.6); text-decoration: underline; text-decoration-color: var(--global-theme-color);" rel="external nofollow noopener" target="_blank">Songlin Yang</a> </div></center></div> </div> <div class="clearfix"> <p>I’m a Ph.D. student at the <a href="https://www.rochester.edu/" rel="external nofollow noopener" target="_blank">University of Rochester</a>, advised by <a href="https://www.cs.rochester.edu/~cxu22/index.html" rel="external nofollow noopener" target="_blank">Prof. Chenliang Xu</a>, working on <em><a href="https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding" rel="external nofollow noopener" target="_blank">LMMs/Agents for Video Understanding</a></em>.</p> <p>I obtained my B.Eng. from <a href="https://www.sustech.edu.cn/en/" rel="external nofollow noopener" target="_blank">SUSTech</a> in 2023, supervised by <a href="https://scholar.google.com/citations?user=PcmyXHMAAAAJ" rel="external nofollow noopener" target="_blank">Prof. Feng Zheng</a>. I’ve interned at <a href="https://www.aboutamazon.com/" rel="external nofollow noopener" target="_blank">Amazon</a>, <a href="https://www.bytedance.com/en/" rel="external nofollow noopener" target="_blank">ByteDance</a>, and <a href="https://www.tencent.com/" rel="external nofollow noopener" target="_blank">Tencent</a>.</p> <p>Please feel free to <a href="/#social-links">contact me</a> if you’re interested in collaboration!</p> </div> <h2> <a href="/publications/" style="color: inherit">Selected Research</a> <a href="https://scholar.google.com/citations?user=xf1rCgoAAAAJ" rel="external nofollow noopener" target="_blank"><img id="citation-badge" src="https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/yunlong10/citation-badge/main/citations.json&amp;color=b509ac&amp;labelColor=fff&amp;label=Cited%20by" alt="Citations"></a> </h2> <div style="display: flex; align-items: center; gap: 0.5rem; margin-bottom: 1rem;"> <p style="font-size: 14px; margin: 0;">* Equal Contribution | † Corresponding Author</p> <div class="category-nav" style="margin: 0;"> <button class="category-btn" data-category="work" onclick="showCategory('work', this)">Work</button> <button class="category-btn" data-category="fun" onclick="showCategory('fun', this)">Fun</button> </div> </div> <script>function updateBadgeColor(){const e=getComputedStyle(document.documentElement).getPropertyValue("--global-theme-color").trim(),t="dark"===document.documentElement.getAttribute("data-theme"),o=document.getElementById("citation-badge");if(o&&e){const l=e.replace("#","");o.src=o.src.replace(/color=[^&]+/,"color="+l).replace(/labelColor=[^&]+/,"labelColor=fff").replace(/label=[^&]*/,"label=Cited%20by");const d=t?"fff":"999999";o.style.border=`1px solid #${d}`,o.style.borderRadius="6px"}}updateBadgeColor(),document.addEventListener("themeChanged",updateBadgeColor);</script> <style>.category-nav{display:flex;gap:0;margin-bottom:1.2rem;flex-wrap:wrap}.category-nav .category-btn{padding:.01em .28em;border:1.5px solid var(--global-theme-color,#b509ac);border-radius:4px;background:#fff;color:var(--global-theme-color,#b509ac);font-size:.8em;font-weight:500;cursor:pointer;transition:background .2s,color .2s,box-shadow .2s;box-shadow:0 1px 4px rgba(0,0,0,0.06);outline:0;opacity:.85;margin:0}.category-nav .category-btn:not(:first-child){margin-left:-1.5px;border-top-left-radius:0;border-bottom-left-radius:0}.category-nav .category-btn:not(:last-child){border-top-right-radius:0;border-bottom-right-radius:0}.category-nav .category-btn:hover,.category-nav .category-btn:focus{background:#fff;color:var(--global-theme-color,#b509ac);opacity:.85;box-shadow:0 1px 4px rgba(0,0,0,0.06)}.category-nav .category-btn.active{background:var(--global-theme-color,#b509ac)!important;color:#fff!important;opacity:1;box-shadow:0 2px 8px rgba(0,0,0,0.1);border:1.5px solid var(--global-theme-color,#b509ac)!important}@media(prefers-color-scheme:dark){.category-nav .category-btn{background:#232323;color:var(--global-theme-color,#b509ac);border:1.5px solid var(--global-theme-color,#b509ac)}.category-nav .category-btn.active{background:var(--global-theme-color,#b509ac)!important;color:#fff!important;border:1.5px solid var(--global-theme-color,#b509ac)!important}}[data-theme='dark'] .category-nav .category-btn{background:#232323;color:var(--global-theme-color,#b509ac);border:1.5px solid var(--global-theme-color,#b509ac)}[data-theme='dark'] .category-nav .category-btn.active{background:var(--global-theme-color,#b509ac)!important;color:#fff!important;border:1.5px solid var(--global-theme-color,#b509ac)!important}[data-theme='light'] .category-nav .category-btn{background:#fff;color:var(--global-theme-color,#b509ac)}[data-theme='light'] .category-nav .category-btn.active{background:var(--global-theme-color,#b509ac)!important;color:#fff!important}</style> <div id="publications-container"> <div id="work" class="publications" style="display:none;"> <div class="publication-cards"> <ol class="bibliography"> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">TCSVT</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/vidllm_survey-480.webp 480w,/assets/img/publication_preview/vidllm_survey-800.webp 800w,/assets/img/publication_preview/vidllm_survey-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/vidllm_survey.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="vidllm_survey.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="vidllmsurvey" class="col-sm-8"> <div class="title"> <a href="https://ieeexplore.ieee.org/document/10982110" rel="external nofollow noopener" target="_blank">Video Understanding with Large Language Models: A Survey 🔥🔥🔥</a> </div> <div class="author"> <em>Yunlong Tang<sup>*</sup></em>, Jing Bi<sup>*</sup>, Siting Xu<sup>*</sup>, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, and <span class="more-authors" title="click to view 10 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '10 more authors' ? 'Ali Vosoughi, Chao Huang, Zeliang Zhang, Pinxin Liu, Mingqian Feng, Feng Zheng, Jianguo Zhang, Ping Luo, Jiebo Luo, Chenliang Xu&lt;sup&gt;†&lt;/sup&gt;' : '10 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">10 more authors</span> </div> <div class="periodical"> <em>IEEE Transactions on Circuits and Systems for Video Technology</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/document/10982110" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite ( <i class="fas fa-quote-right" style="font-size: 1.1em; margin-right: 3px; -webkit-text-stroke: 1px currentColor; -webkit-text-fill-color: transparent;"></i>N/A ) </a> <a href="https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/yunlong10/Awesome-LLMs-for-Video-Understanding?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="abstract hidden"> <p> With the burgeoning growth of online video platforms and the escalating volume of video content, the demand for proficient video understanding tools has intensified markedly. Given the remarkable capabilities of large language models (LLMs) in language and multimodal tasks, this survey provides a detailed overview of recent advancements in video understanding that harness the power of LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly advanced, particularly their ability for open-ended multi-granularity (general, temporal, and spatiotemporal) reasoning combined with commonsense knowledge, suggesting a promising path for future video understanding. We examine the unique characteristics and capabilities of Vid-LLMs, categorizing the approaches into three main types: Video Analyzer x LLM, Video Embedder x LLM, and (Analyzer + Embedder) x LLM. Furthermore, we identify five sub-types based on the functions of LLMs in Vid-LLMs: LLM as Summarizer, LLM as Manager, LLM as Text Decoder, LLM as Regressor, and LLM as Hidden Layer. Furthermore, this survey presents a comprehensive study of the tasks, datasets, benchmarks, and evaluation methodologies for Vid-LLMs. Additionally, it explores the expansive applications of Vid-LLMs across various domains, highlighting their remarkable scalability and versatility in real-world video understanding challenges. Finally, it summarizes the limitations of existing Vid-LLMs and outlines directions for future research. For more information, readers are recommended to visit the repository at https://github.com/yunlong10/awesome-llms-for-video-understanding. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">vidllmsurvey</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Video Understanding with Large Language Models: A Survey 🔥🔥🔥}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tang, Yunlong and Bi, Jing and Xu, Siting and Song, Luchuan and Liang, Susan and Wang, Teng and Zhang, Daoan and An, Jie and Lin, Jingyang and Zhu, Rongyi and Vosoughi, Ali and Huang, Chao and Zhang, Zeliang and Liu, Pinxin and Feng, Mingqian and Zheng, Feng and Zhang, Jianguo and Luo, Ping and Luo, Jiebo and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Circuits and Systems for Video Technology}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mmperspective-480.webp 480w,/assets/img/publication_preview/mmperspective-800.webp 800w,/assets/img/publication_preview/mmperspective-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/mmperspective.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mmperspective.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="tang2025mmperspective" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2505.20426" rel="external nofollow noopener" target="_blank">MMPerspective: Do MLLMs Understand Perspective? A Comprehensive Benchmark for Perspective Perception, Reasoning, and Robustness</a> </div> <div class="author"> <em>Yunlong Tang</em>, Pinxin Liu, Mingqian Feng, Zhangyun Tan, Rui Mao, Chao Huang, Jing Bi, Yunzhong Xiao, Susan Liang, Hang Hua, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Ali Vosoughi, Luchuan Song, Zeliang Zhang, Chenliang Xu' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>The 39th Annual Conference on Neural Information Processing Systems</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://yunlong10.github.io/MMPerspective/" class="btn btn-sm z-depth-0" role="button">Website</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite </a> <a href="https://github.com/yunlong10/MMPerspective" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/yunlong10/MMPerspective?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="abstract hidden"> <p> Understanding perspective is fundamental to human visual perception, yet the extent to which multimodal large language models (MLLMs) internalize perspective geometry remains unclear. We introduce MMPerspective, the first benchmark specifically designed to systematically evaluate MLLMs’ understanding of perspective through 10 carefully crafted tasks across three complementary dimensions: Perspective Perception, Reasoning, and Robustness. Our benchmark comprises 2,711 real-world and synthetic image instances with 5,083 question-answer pairs that probe key capabilities, such as vanishing point perception and counting, perspective type reasoning, line relationship understanding in 3D space, invariance to perspective-preserving transformations, etc. Through a comprehensive evaluation of 43 state-of-the-art MLLMs, we uncover significant limitations: while models demonstrate competence on surface-level perceptual tasks, they struggle with compositional reasoning and maintaining spatial consistency under perturbations. Our analysis further reveals intriguing patterns between model architecture, scale, and perspective capabilities, highlighting both robustness bottlenecks and the benefits of chain-of-thought prompting. MMPerspective establishes a valuable testbed for diagnosing and advancing spatial understanding in vision-language systems. Resources available at: https://yunlong10.github.io/MMPerspective/ </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tang2025mmperspective</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MMPerspective: Do MLLMs Understand Perspective? A Comprehensive Benchmark for Perspective Perception, Reasoning, and Robustness}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tang, Yunlong and Liu, Pinxin and Feng, Mingqian and Tan, Zhangyun and Mao, Rui and Huang, Chao and Bi, Jing and Xiao, Yunzhong and Liang, Susan and Hua, Hang and Vosoughi, Ali and Song, Luchuan and Zhang, Zeliang and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The 39th Annual Conference on Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CVPR</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/vidcomposition-480.webp 480w,/assets/img/publication_preview/vidcomposition-800.webp 800w,/assets/img/publication_preview/vidcomposition-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/vidcomposition.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="vidcomposition.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="tang2024vidcompostion" class="col-sm-8"> <div class="title"> <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Tang_VidComposition_Can_MLLMs_Analyze_Compositions_in_Compiled_Videos_CVPR_2025_paper.html" rel="external nofollow noopener" target="_blank">VidComposition: Can MLLMs Analyze Compositions in Compiled Videos?</a> </div> <div class="author"> <em>Yunlong Tang<sup>*</sup></em>, Junjia Guo<sup>*</sup>, Hang Hua, Susan Liang, Mingqian Feng, Xinyang Li, Rui Mao, Chao Huang, Jing Bi, Zeliang Zhang, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Pooyan Fazli, Chenliang Xu&lt;sup&gt;†&lt;/sup&gt;' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the Computer Vision and Pattern Recognition Conference</em>, Jun 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://yunlong10.github.io/VidComposition/" class="btn btn-sm z-depth-0" role="button">Website</a> <a href="/assets/pdf/vidcomp.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite ( <i class="fas fa-quote-right" style="font-size: 1.1em; margin-right: 3px; -webkit-text-stroke: 1px currentColor; -webkit-text-fill-color: transparent;"></i>N/A ) </a> <a href="https://github.com/yunlong10/VidComposition" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/yunlong10/VidComposition?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tang2024vidcompostion</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{VidComposition: Can MLLMs Analyze Compositions in Compiled Videos?}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tang, Yunlong and Guo, Junjia and Hua, Hang and Liang, Susan and Feng, Mingqian and Li, Xinyang and Mao, Rui and Huang, Chao and Bi, Jing and Zhang, Zeliang and Fazli, Pooyan and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Computer Vision and Pattern Recognition Conference}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{8490-8500}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">AAAI</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/teaser-avicuna-480.webp 480w,/assets/img/publication_preview/teaser-avicuna-800.webp 800w,/assets/img/publication_preview/teaser-avicuna-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/teaser-avicuna.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="teaser-avicuna.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="tang2024avicuna" class="col-sm-8"> <div class="title"> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/32784" rel="external nofollow noopener" target="_blank">Empowering LLMs with Pseudo-Untrimmed Videos for Audio-Visual Temporal Understanding</a> </div> <div class="author"> <em>Yunlong Tang</em>, Daiki Shimada, Jing Bi, Mingqian Feng, Hang Hua, and Chenliang Xu<sup>†</sup> </div> <div class="periodical"> <em>In Proceedings of the AAAI Conference on Artificial Intelligence</em>, Jun 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/avicuna.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite ( <i class="fas fa-quote-right" style="font-size: 1.1em; margin-right: 3px; -webkit-text-stroke: 1px currentColor; -webkit-text-fill-color: transparent;"></i>N/A ) </a> <a href="https://github.com/yunlong10/AVicuna" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/yunlong10/AVicuna?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="abstract hidden"> <p> Large language models (LLMs) have demonstrated remarkable capabilities in natural language and multimodal domains. By fine-tuning multimodal LLMs with temporal annotations from well-annotated datasets, e.g., dense video captioning datasets, their temporal understanding capacity in video-language tasks can be obtained. However, there is a notable lack of untrimmed audio-visual video datasets with precise temporal annotations for events. This deficiency hinders LLMs from learning the alignment between time, audio-visual events, and text tokens, thus impairing their ability to localize audio-visual events in videos temporally. To address this gap, we introduce PU-VALOR, a comprehensive audio-visual dataset comprising over 114,000 pseudo-untrimmed videos with detailed temporal annotations. PU-VALOR is derived from the large-scale but coarse-annotated audio-visual dataset VALOR, through a subtle method involving event-based video clustering, random temporal scaling, and permutation. By fine-tuning a multimodal LLM on PU-VALOR, we developed AVicuna, a model capable of aligning audio-visual events with temporal intervals and corresponding text tokens. AVicuna excels in temporal localization and time-aware dialogue capabilities. Our experiments demonstrate that AVicuna effectively handles temporal understanding in audio-visual videos and achieves state-of-the-art performance on open-ended video QA, audio-visual QA, and audio-visual event dense localization tasks. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tang2024avicuna</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Empowering LLMs with Pseudo-Untrimmed Videos for Audio-Visual Temporal Understanding}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tang, Yunlong and Shimada, Daiki and Bi, Jing and Feng, Mingqian and Hua, Hang and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the AAAI Conference on Artificial Intelligence}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{39}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{7293-7301}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1609/aaai.v39i7.32784}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">AAAI</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/v2xum-llama-480.webp 480w,/assets/img/publication_preview/v2xum-llama-800.webp 800w,/assets/img/publication_preview/v2xum-llama-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/v2xum-llama.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="v2xum-llama.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="hua2024v2xum" class="col-sm-8"> <div class="title"> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/32374" rel="external nofollow noopener" target="_blank">V2Xum-LLM: Cross-modal Video Summarization with Temporal Prompt Instruction Tuning</a> </div> <div class="author"> Hang Hua<sup>*</sup>, <em>Yunlong Tang<sup>*</sup></em>, Chenliang Xu, and Jiebo Luo<sup>†</sup> </div> <div class="periodical"> <em>In Proceedings of the AAAI Conference on Artificial Intelligence</em>, Jun 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://hanghuacs.github.io/v2xum/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="/assets/pdf/v2xum.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite ( <i class="fas fa-quote-right" style="font-size: 1.1em; margin-right: 3px; -webkit-text-stroke: 1px currentColor; -webkit-text-fill-color: transparent;"></i>N/A ) </a> <a href="https://github.com/hanghuacs/V2Xum-LLM" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/hanghuacs/V2Xum-LLM?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">hua2024v2xum</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{V2Xum-LLM: Cross-modal Video Summarization with Temporal Prompt Instruction Tuning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hua, Hang and Tang, Yunlong and Xu, Chenliang and Luo, Jiebo}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the AAAI Conference on Artificial Intelligence}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{39}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3599-3607}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1609/aaai.v39i4.32374}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">AAAI</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/cardiff-480.webp 480w,/assets/img/publication_preview/cardiff-800.webp 800w,/assets/img/publication_preview/cardiff-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/cardiff.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cardiff.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="tang2024cardiff" class="col-sm-8"> <div class="title"> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/32785" rel="external nofollow noopener" target="_blank">CaRDiff: Video Salient Object Ranking Chain of Thought Reasoning for Saliency Prediction with Diffusion</a> </div> <div class="author"> <em>Yunlong Tang</em>, Gen Zhan, Li Yang, Yiting Liao, and Chenliang Xu<sup>†</sup> </div> <div class="periodical"> <em>In Proceedings of the AAAI Conference on Artificial Intelligence</em>, Jun 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/32785" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/pdf/cardiff.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite ( <i class="fas fa-quote-right" style="font-size: 1.1em; margin-right: 3px; -webkit-text-stroke: 1px currentColor; -webkit-text-fill-color: transparent;"></i>N/A ) </a> </div> <div class="abstract hidden"> <p> Video saliency prediction aims to identify the regions in a video that attract human attention and gaze, driven by bottom-up features from the video and top-down processes like memory and cognition. Among these top-down influences, language plays a crucial role in guiding attention by shaping how visual information is interpreted. Existing methods primarily focus on modeling perceptual information while neglecting the reasoning process facilitated by language, where ranking cues are crucial outcomes of this process and practical guidance for saliency prediction. In this paper, we propose CaRDiff (Caption, Rank, and generate with Diffusion), a framework that imitates the process by integrating multimodal large language model (MLLM), a grounding module, and a diffusion model, to enhance video saliency prediction. Specifically, we introduce a novel prompting method VSOR-CoT (Video Salient Object Ranking Chain of Thought), which utilizes an MLLM with a grounding module to caption video content and infer salient objects along with their rankings and positions. This process derives ranking maps that can be sufficiently leveraged by the diffusion model to decode the saliency maps for the given video accurately. Extensive experiments show the effectiveness of VSOR-CoT in improving the performance of video saliency prediction. The proposed CaRDiff performs better than state-of-the-art models on the MVS dataset and demonstrates cross-dataset capabilities on the DHF1k dataset through zero-shot evaluation. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tang2024cardiff</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{CaRDiff: Video Salient Object Ranking Chain of Thought Reasoning for Saliency Prediction with Diffusion}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tang, Yunlong and Zhan, Gen and Yang, Li and Liao, Yiting and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the AAAI Conference on Artificial Intelligence}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1609/aaai.v39i7.32785}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{39}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{7302-7310}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </div> <div id="fun" class="publications" style="display:none;"> <div class="publication-cards"> <ol class="bibliography"> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mmperspective-480.webp 480w,/assets/img/publication_preview/mmperspective-800.webp 800w,/assets/img/publication_preview/mmperspective-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/mmperspective.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mmperspective.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="tang2025mmperspective" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2505.20426" rel="external nofollow noopener" target="_blank">MMPerspective: Do MLLMs Understand Perspective? A Comprehensive Benchmark for Perspective Perception, Reasoning, and Robustness</a> </div> <div class="author"> <em>Yunlong Tang</em>, Pinxin Liu, Mingqian Feng, Zhangyun Tan, Rui Mao, Chao Huang, Jing Bi, Yunzhong Xiao, Susan Liang, Hang Hua, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Ali Vosoughi, Luchuan Song, Zeliang Zhang, Chenliang Xu' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>The 39th Annual Conference on Neural Information Processing Systems</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://yunlong10.github.io/MMPerspective/" class="btn btn-sm z-depth-0" role="button">Website</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite </a> <a href="https://github.com/yunlong10/MMPerspective" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/yunlong10/MMPerspective?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="abstract hidden"> <p> Understanding perspective is fundamental to human visual perception, yet the extent to which multimodal large language models (MLLMs) internalize perspective geometry remains unclear. We introduce MMPerspective, the first benchmark specifically designed to systematically evaluate MLLMs’ understanding of perspective through 10 carefully crafted tasks across three complementary dimensions: Perspective Perception, Reasoning, and Robustness. Our benchmark comprises 2,711 real-world and synthetic image instances with 5,083 question-answer pairs that probe key capabilities, such as vanishing point perception and counting, perspective type reasoning, line relationship understanding in 3D space, invariance to perspective-preserving transformations, etc. Through a comprehensive evaluation of 43 state-of-the-art MLLMs, we uncover significant limitations: while models demonstrate competence on surface-level perceptual tasks, they struggle with compositional reasoning and maintaining spatial consistency under perturbations. Our analysis further reveals intriguing patterns between model architecture, scale, and perspective capabilities, highlighting both robustness bottlenecks and the benefits of chain-of-thought prompting. MMPerspective establishes a valuable testbed for diagnosing and advancing spatial understanding in vision-language systems. Resources available at: https://yunlong10.github.io/MMPerspective/ </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tang2025mmperspective</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MMPerspective: Do MLLMs Understand Perspective? A Comprehensive Benchmark for Perspective Perception, Reasoning, and Robustness}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tang, Yunlong and Liu, Pinxin and Feng, Mingqian and Tan, Zhangyun and Mao, Rui and Huang, Chao and Bi, Jing and Xiao, Yunzhong and Liang, Susan and Hua, Hang and Vosoughi, Ali and Song, Luchuan and Zhang, Zeliang and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The 39th Annual Conference on Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICCVW</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/teaser-ai4anime-480.webp 480w,/assets/img/publication_preview/teaser-ai4anime-800.webp 800w,/assets/img/publication_preview/teaser-ai4anime-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/teaser-ai4anime.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="teaser-ai4anime.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="tang2025ai4anime" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2501.06250" rel="external nofollow noopener" target="_blank">Generative AI for Cel-Animation: A Survey 🔥🔥🔥</a> </div> <div class="author"> <em>Yunlong Tang</em>, Junjia Guo, Pinxin Liu, Zhiyuan Wang, Hang Hua, Jia-Xing Zhong, Yunzhong Xiao, Chao Huang, Luchuan Song, Susan Liang, and <span class="more-authors" title="click to view 7 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '7 more authors' ? 'Yizhi Song, Liu He, Jing Bi, Mingqian Feng, Xinyang Li, Zeliang Zhang, Chenliang Xu' : '7 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">7 more authors</span> </div> <div class="periodical"> <em>ICCV AISTORY Workshop; arXiv preprint arXiv:2501.06250</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/ai4anime.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite ( <i class="fas fa-quote-right" style="font-size: 1.1em; margin-right: 3px; -webkit-text-stroke: 1px currentColor; -webkit-text-fill-color: transparent;"></i>N/A ) </a> <a href="https://github.com/yunlong10/Awesome-AI4Animation" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/yunlong10/Awesome-AI4Animation?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> </div> <div class="abstract hidden"> <p> Traditional Celluloid (Cel) Animation production pipeline encompasses multiple essential steps, including storyboarding, layout design, keyframe animation, inbetweening, and colorization, which demand substantial manual effort, technical expertise, and significant time investment. These challenges have historically impeded the efficiency and scalability of Cel-Animation production. The rise of generative artificial intelligence (GenAI), encompassing large language models, multimodal models, and diffusion models, offers innovative solutions by automating tasks such as inbetween frame generation, colorization, and storyboard creation. This survey explores how GenAI integration is revolutionizing traditional animation workflows by lowering technical barriers, broadening accessibility for a wider range of creators through tools like AniDoc, ToonCrafter, and AniSora, and enabling artists to focus more on creative expression and artistic innovation. Despite its potential, issues such as maintaining visual consistency, ensuring stylistic coherence, and addressing ethical considerations continue to pose challenges. Furthermore, this paper discusses future directions and explores potential advancements in AI-assisted animation. For further exploration and resources, please visit our GitHub repository: https://github.com/yunlong10/Awesome-AI4Animation </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tang2025ai4anime</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Generative AI for Cel-Animation: A Survey 🔥🔥🔥}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tang, Yunlong and Guo, Junjia and Liu, Pinxin and Wang, Zhiyuan and Hua, Hang and Zhong, Jia-Xing and Xiao, Yunzhong and Huang, Chao and Song, Luchuan and Liang, Susan and Song, Yizhi and He, Liu and Bi, Jing and Feng, Mingqian and Li, Xinyang and Zhang, Zeliang and Xu, Chenliang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ICCV AISTORY Workshop; arXiv preprint arXiv:2501.06250}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <style>.award-badge{display:inline-block;background:linear-gradient(135deg,#ff6b6b,#ff8e8e);color:white;padding:.2rem .5rem;border-radius:.25rem;font-size:.75rem;font-weight:500;margin-left:8px;box-shadow:0 1px 3px rgba(255,107,107,0.3);vertical-align:middle;border:1px solid #ff6b6b;line-height:1.2}@media(prefers-color-scheme:dark){.award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}}[data-theme='dark'] .award-badge{background:linear-gradient(135deg,#ff6b6b,#ff8e8e);box-shadow:0 2px 4px rgba(255,107,107,0.4)}</style> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICMC</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/launchpadgpt-480.webp 480w,/assets/img/publication_preview/launchpadgpt-800.webp 800w,/assets/img/publication_preview/launchpadgpt-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/launchpadgpt.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="launchpadgpt.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="xu2023launchpadgpt" class="col-sm-8"> <div class="title"> <a href="https://arxiv.org/abs/2307.04827" rel="external nofollow noopener" target="_blank">LaunchpadGPT: Language Model as Music Visualization Designer on Launchpad</a> </div> <div class="author"> Siting Xu<sup>*</sup>, <em>Yunlong Tang<sup>*</sup></em>, and Feng Zheng<sup>†</sup> </div> <div class="periodical"> <em>In Proceedings of the International Computer Music Conference</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button"> Cite </a> <a href="https://github.com/yunlong10/LaunchpadGPT" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/github/stars/yunlong10/LaunchpadGPT?style=social" alt="GitHub stars" style="vertical-align: middle; margin-left: 5px;"> </a> <span class="award-badge">Oral</span> </div> <div class="abstract hidden"> <p> Launchpad is a popular music instrument for live performance and music production. In this paper, we propose LaunchpadGPT, a language model that can generate music visualization designs for Launchpad. We train the model on a large-scale dataset of music visualization designs and demonstrate its effectiveness in generating diverse and creative designs. We also develop a web-based platform that allows users to interact with the model and generate music visualization designs in real-time. Our code is available at https://github.com/yunlong10/LaunchpadGPT. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xu2023launchpadgpt</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{LaunchpadGPT: Language Model as Music Visualization Designer on Launchpad}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu, Siting and Tang, Yunlong and Zheng, Feng}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the International Computer Music Conference}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{213-217}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </div> </div> <script>!function(){function t(){i||(c=document.querySelectorAll(".publications"),a=document.querySelectorAll(".category-nav .category-btn"),i=!0)}function n(t){a.forEach(function(n){n.classList.toggle("active",n.getAttribute("data-category")===t)})}function o(t){c.forEach(function(n){n.style.display=n.id===t?"block":"none"}),n(t)}function e(n){i||t();var e=document.getElementById(n);e&&"none"!==e.style.display?o("work"===n?"fun":"work"):o(n)}var c,a,i=!1;window.showCategory=e,document.addEventListener("DOMContentLoaded",function(){t(),o("work")})}();</script> <h2>Misc.</h2> <br> <h5>Fun Facts</h5> <ul> <li>🩵 Preferred names: Yolo (en) or 芸珑 (zh).</li> <li>🍭 I’m into ACG, J-pop, singing, and cosplay.</li> <li>😝 Maybe I’m just cosplaying as a researcher—who knows?</li> </ul> <h5>Visitor Map</h5> <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?cl=ffffff&amp;d=mErswMtIf2K7G41Iql-K1paY9dnbAA1mf0bzUzgiHAs&amp;co=2698ba&amp;cmo=b509ac&amp;cmn=FF1493&amp;w=350"></script> <div style="color:gray; margin-top: 0px; opacity: 0.6; white-space: nowrap; text-align: center; font-size: 0.9em; width: 100%; max-width: 100%;"> <span>Thank you for your visit!</span> </div> <h5 id="social-links">Social Links</h5> <div class="social"> <div class="contact-icons"> <a href="mailto:%79%75%6E%6C%6F%6E%67.%74%61%6E%67@%72%6F%63%68%65%73%74%65%72.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=xf1rCgoAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/yunlong10" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/yolo-yunlong-tang" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/yoloytang" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> <a href="https://youtube.com/@yoloytang" title="YouTube" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i></a> <a id="WeChatBtn" title="WeChat"><i class="fa-brands fa-weixin"></i></a> <div id="WeChatMod" class="wechat-modal"> <img src="/assets/img/wechat-qr.png" alt="WeChat QR" id="WeChatQR"> </div> <script>for(var wechatModal=document.getElementById("WeChatMod"),wechatBtn=document.querySelectorAll('[id="WeChatBtn"]'),i=0;i<wechatBtn.length;i++)wechatBtn[i].onclick=function(){wechatModal.style.display="block"};window.onclick=function(t){t.target==wechatModal&&(wechatModal.style.display="none")};</script> <a href="https://yunlong10.github.io/assets/pdf/cv_yunlong_tang.pdf" title="Work"><i class="ai ai-cv"></i></a> </div> <div class="contact-note">Best by email. </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Yunlong (Yolo) Tang. Theme by <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="external nofollow noopener">al-folio</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script>for(var wechatModal=document.getElementById("WeChatMod"),wechatBtn=document.querySelectorAll('[id="WeChatBtn"]'),i=0;i<wechatBtn.length;i++)wechatBtn[i].onclick=function(){wechatModal.style.display="block"};window.onclick=function(t){t.target==wechatModal&&(wechatModal.style.display="none")};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"Publications",description:"* Equal Contribution | \u2020 Corresponding Author",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/2021/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2021/distill/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/blog/2020/twitter/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march &amp; april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-started-my-part-time-internship-at-tencent-in-shenzhen-with-supervision-from-dr-wenhao-jiang-and-qin-lin-i-had-to-balance-it-with-my-university-coursework",title:"Started my part-time internship at Tencent in Shenzhen, with supervision from Dr. Wenhao...",description:"",section:"News"},{id:"news-i-left-tencent-and-joined-sustech-vip-lab-as-an-undergraduate-student-researcher",title:"I left Tencent and joined SUSTech VIP Lab as an undergraduate student researcher....",description:"",section:"News"},{id:"news-one-paper-about-multimodal-ad-video-editing-has-been-accepted-to-asian-conference-on-computer-vision-accv-2022",title:"One paper about multimodal Ad video editing has been accepted to Asian Conference...",description:"",section:"News"},{id:"news-thrilled-to-announce-i-ll-be-starting-my-ph-d-in-computer-science-at-the-university-of-rochester-from-fall-2023-working-with-prof-chenliang-xu",title:"Thrilled to announce I\u2019ll be starting my Ph.D. in Computer Science at the...",description:"",section:"News"},{id:"news-our-project-caption-anything-has-been-released-welcome-to-try-our-demo-and-star-our-github-repo",title:"Our project Caption Anything has been released! Welcome to try our demo and...",description:"",section:"News"},{id:"news-the-technical-report-for-caption-anything-has-been-released",title:"The technical report for Caption Anything has been released!",description:"",section:"News"},{id:"news-successfully-defended-my-undergraduate-thesis-titled-language-guided-video-cover-generation-which-has-been-awarded-the-excellent-undergraduate-thesis",title:"Successfully defended my undergraduate thesis titled Language-Guided Video Cover Generation, which has been...",description:"",section:"News"},{id:"news-our-team-won-the-first-place-in-loveu-long-form-video-understanding-challenge-at-cvpr-23-workshop",title:"Our team won the first place in LOVEU (Long-form Video Understanding) Challenge at...",description:"",section:"News"},{id:"news-graduated-from-sustech-obtained-my-bachelor-s-degree-and-the-honor-of-excellent-graduate-for-exceptional-performance",title:"Graduated from SUSTech, obtained my bachelor\u2019s degree and the honor of Excellent Graduate...",description:"",section:"News"},{id:"news-one-paper-accepted-to-international-computer-music-conference-icmc-2023",title:"One paper accepted to International Computer Music Conference (ICMC) 2023.",description:"",section:"News"},{id:"news-officially-joined-in-the-chenliang-xu-s-group-at-ur-cs-as-a-ph-d-student",title:"Officially joined in the Chenliang Xu\u2019s Group at UR CS as a Ph.D....",description:"",section:"News"},{id:"news-released-a-survey-for-video-understanding-with-llms-arxiv-github",title:"\ud83d\udd25\ud83d\udd25\ud83d\udd25 Released a survey for Video Understanding with LLMs (arXiv, GitHub).",description:"",section:"News"},{id:"news-i-will-join-bytedance-as-a-research-intern-this-summer",title:"I will join ByteDance as a Research Intern this summer.",description:"",section:"News"},{id:"news-released-avicuna-an-audio-visual-llm-empowered-by-pseudo-untrimmed-video-annotations-for-audio-visual-event-localization",title:"Released AVicuna, an Audio-Visual LLM empowered by pseudo-untrimmed video annotations for audio-visual event...",description:"",section:"News"},{id:"news-introducing-v2xum-llama-model-and-instruct-v2xum-dataset-for-cross-modal-video-summarization",title:"Introducing V2Xum-LLaMA model and Instruct-V2Xum dataset for cross-modal video summarization.",description:"",section:"News"},{id:"news-started-my-internship-at-bytedance-in-san-jose-ca-mentored-by-yiting-liao-amp-amp-gen-zhan",title:"\ud83c\udf1f Started my internship at ByteDance in San Jose, CA, mentored by Yiting...",description:"",section:"News"},{id:"news-introducing-differentiated-beam-decoding-dbd-a-novel-decoding-strategy-for-lvlm-hallucination-mitigation",title:"Introducing Differentiated Beam Decoding (DBD), a novel decoding strategy for LVLM hallucination mitigation....",description:"",section:"News"},{id:"news-one-paper-about-egocentric-video-understanding-with-llm-has-been-accepted-to-acm-mm-2024",title:"One paper about egocentric video understanding with LLM has been accepted to ACM...",description:"",section:"News"},{id:"news-we-39-ve-recently-updated-our-survey-quot-video-understanding-with-large-language-models-a-survey-quot",title:"\ud83d\udce2 We&#39;ve recently updated our survey: &quot;Video Understanding with Large Language Models: A...",description:"",section:"News",handler:()=>{window.location.href="/news/2024-07-23/"}},{id:"news-we-won-the-first-place-in-aim-2024-challenge-on-video-saliency-prediction-eccv-workshop-thanks-to-gen-zhan-and-li-yang",title:"\ud83c\udfc5 We won the first place in AIM 2024 Challenge on Video Saliency...",description:"",section:"News"},{id:"news-introducing-cardiff-a-framework-for-video-saliency-prediction-using-mllm-cot-reasoning-and-diffusion-model",title:"Introducing CaRDiff, a framework for video saliency prediction using MLLM CoT reasoning and...",description:"",section:"News"},{id:"news-mmcomposition-has-been-publicly-released-read-our-paper-check-out-the-latest-leaderboard-and-access-the-code-to-evaluate-your-own-models",title:"\ud83d\ude80 MMComposition has been publicly released. Read our Paper, check out the latest...",description:"",section:"News"},{id:"news-we-have-released-vidcomposition-a-benchmark-to-evaluate-mllms-understanding-of-video-compositions-project-page-paper-leaderboard",title:"We have released VidComposition, a benchmark to evaluate MLLMs\u2019 understanding of video compositions....",description:"",section:"News"},{id:"news-three-papers-on-video-llms-have-been-accepted-to-aaai-2025",title:"\ud83c\udf89 Three papers on Video-LLMs have been accepted to AAAI 2025!",description:"",section:"News"},{id:"news-introducing-our-survey-paper-on-genai-for-cel-animation-arxiv-github",title:"\ud83c\udfa8 Introducing our survey paper on GenAI for Cel-Animation \ud83d\udc49 arXiv | GitHub...",description:"",section:"News"},{id:"news-i-will-join-amazon-as-an-applied-scientist-intern-this-summer",title:"I will join Amazon as an Applied Scientist Intern this summer.",description:"",section:"News"},{id:"news-two-papers-have-been-accepted-to-cvpr-2025-including-our-vidcomposition-benchmark",title:"\ud83c\udf89 Two papers have been accepted to CVPR 2025, including our VidComposition benchmark!...",description:"",section:"News"},{id:"news-caption-anything-in-video-cat-v-has-been-released-arxiv-github",title:"\ud83d\udcf7 Caption Anything in Video (CAT-V) has been released \ud83d\udc49 arXiv | GitHub...",description:"",section:"News"},{id:"news-our-vid-llm-survey-has-been-accepted-by-the-ieee-transactions-on-circuits-and-systems-for-video-technology-tcsvt-ieee-xplore-github",title:"\ud83c\udf89 Our Vid-LLM survey has been accepted by the IEEE Transactions on Circuits...",description:"",section:"News"},{id:"news-started-my-internship-as-an-applied-scientist-intern-at-amazon-in-bellevue-wa",title:"\ud83c\udf1f Started my internship as an Applied Scientist Intern at Amazon in Bellevue,...",description:"",section:"News"},{id:"news-introducing-mmperspective-a-comprehensive-benchmark-for-mllms-on-perspective-understanding",title:"\ud83d\udcd0 Introducing MMPerspective, a comprehensive benchmark for MLLMs on perspective understanding.",description:"",section:"News"},{id:"projects-genai-for-cel-animation",title:"GenAI for Cel-Animation",description:"[ICCVW 2025] \ud83c\udfa8 A Comprehensive Survey on GenAI for Cel-Animation.",section:"Projects",handler:()=>{window.location.href="/projects/ai4anime/"}},{id:"projects-cat-\ufe0f",title:"CAT\uff3c(=^\u2025^)\u270f\ufe0f",description:"Caption-Anything is a versatile image processing tool that combines the capabilities of Segment Anything, Visual Captioning, and ChatGPT. Our solution generates descriptive captions for any object within an image, offering a range of language styles to accommodate diverse user preferences.",section:"Projects",handler:()=>{window.location.href="/projects/caption_anything/"}},{id:"projects-captionanything-in-video-cat-v",title:"CaptionAnything in Video (CAT-V)",description:"CAT-V is a training-free framework that enables fine-grained object-centric video captioning through spatiotemporal visual prompting and chain-of-thought reasoning.",section:"Projects",handler:()=>{window.location.href="/projects/cat-v/"}},{id:"projects-launchpadgpt",title:"LaunchpadGPT",description:"[ICMC 2023] \ud83c\udfb5 LaunchpadGPT: Language Model as Music Visualization Designer on Launchpad",section:"Projects",handler:()=>{window.location.href="/projects/launchpadgpt/"}},{id:"projects-mmcomposition",title:"MMComposition",description:"Benchmarking the compositionality capabilities of VLMs \ud83e\udd2f",section:"Projects",handler:()=>{window.location.href="/projects/mmcomposition/"}},{id:"projects-mmperspective",title:"MMPerspective",description:"[NeurIPS 2025] Do MLLMs Understand Perspective? A Comprehensive Benchmark for Perspective Perception, Reasoning, and Robustness is provided in this project.",section:"Projects",handler:()=>{window.location.href="/projects/mmperspective/"}},{id:"projects-scaling-concept",title:"Scaling Concept",description:"We use pretrained text-guided diffusion models to scale up/down concepts in image/audio.",section:"Projects",handler:()=>{window.location.href="/projects/scaling_concept/"}},{id:"projects-vidcomposition",title:"VidComposition",description:"[CVPR 2025] \ud83c\udfc6 See how Top MLLMs understand video compositions.",section:"Projects",handler:()=>{window.location.href="/projects/vidcomposition/"}},{id:"projects-vid-llm-survey",title:"Vid-LLM Survey",description:"[TCSVT] \ud83d\udd25 Video Understanding with Large Language Models: A Survey",section:"Projects",handler:()=>{window.location.href="/projects/vidllm_survey/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%79%75%6E%6C%6F%6E%67.%74%61%6E%67@%72%6F%63%68%65%73%74%65%72.%65%64%75","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=xf1rCgoAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/yunlong10","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/yolo-yunlong-tang","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/yoloytang","_blank")}},{id:"socials-work",title:"Work",section:"Socials",handler:()=>{window.open("https://yunlong10.github.io/assets/pdf/cv_yunlong_tang.pdf","_blank")}},{id:"socials-youtube",title:"YouTube",section:"Socials",handler:()=>{window.open("https://youtube.com/@yoloytang","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>