---
---
@article{tang2025videolmm_posttraining,
  title = {Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models},
  author = {Tang, Yunlong and Bi, Jing and Liu, Pinxin and Pan, Zhenyu and Tan, Zhangyun and Shen, Qianxiang and Liu, Jiani and Hua, Hang and Guo, Junjia and Xiao, Yunzhong and Huang, Chao and Wang, Zhiyuan and Liang, Susan and Liu, Xinyi and Song, Yizhi and Nie, Yuhe and Zhong, Jia-Xing and Li, Bozheng and Qi, Daiqing and Zeng, Ziyun and Vosoughi, Ali and Song, Luchuan and Zhang, Zeliang and Shimada, Daiki and Liu, Han and Luo, Jiebo and Xu, Chenliang},
  journal = {arXiv preprint arXiv:2510.05034},
  year = {2025},
  preview = {videolmm_posttraining.png},
  html = {https://arxiv.org/abs/2510.05034},
  bibtex_show = {true},
  stars = {yunlong10/Awesome-Video-LMM-Post-Training},
  code = {https://github.com/yunlong10/Awesome-Video-LMM-Post-Training},
}
@article{vidllmsurvey,
  title={Video Understanding with Large Language Models: A Survey ðŸ”¥ðŸ”¥ðŸ”¥}, 
  author={Tang*, Yunlong and Bi*, Jing and Xu*, Siting and Song, Luchuan and Liang, Susan and Wang, Teng and Zhang, Daoan and An, Jie and Lin, Jingyang and Zhu, Rongyi and Vosoughi, Ali and Huang, Chao and Zhang, Zeliang and Liu, Pinxin and Feng, Mingqian and Zheng, Feng and Zhang, Jianguo and Luo, Ping and Luo, Jiebo and Xuâ€ , Chenliang},
  journal={IEEE Transactions on Circuits and Systems for Video Technology},
  year={2025},
  preview={vidllm_survey.png},
  selected={true},
  html={https://ieeexplore.ieee.org/document/10982110},
  code={https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding},
  bibtex_show={true},
  category={work},
  abbr = {TCSVT},
  stars={yunlong10/Awesome-LLMs-for-Video-Understanding},
  google_scholar_id={qjMakFHDy7sC},
}
@article{tang2025mmperspective,
  title = {MMPerspective: Do MLLMs Understand Perspective? A Comprehensive Benchmark for Perspective Perception, Reasoning, and Robustness},
  author = {Tang, Yunlong and Liu, Pinxin and Feng, Mingqian and Tan, Zhangyun and Mao, Rui and Huang, Chao and Bi, Jing and Xiao, Yunzhong and Liang, Susan and Hua, Hang and Vosoughi, Ali and Song, Luchuan and Zhang, Zeliang and Xu, Chenliang},
  journal = {The 39th Annual Conference on Neural Information Processing Systems},
  year = {2025},
  abbr = {NeurIPS},
  preview={mmperspective.png},
  category={fun work},
  html={https://arxiv.org/abs/2505.20426},
  website={https://yunlong10.github.io/MMPerspective/},
  code={https://github.com/yunlong10/MMPerspective},
  stars={yunlong10/MMPerspective},
  bibtex_show={true}
}
@article{hua2025mmigbench,
  title={MMIG-Bench: Towards Comprehensive and Explainable Evaluation of Multi-Modal Image Generation Models},
  author={Hua, Hang and Zeng, Ziyun and Song, Yizhi and Tang, Yunlong and He, Liu and Aliaga, Daniel and Xiong, Wei and Luo, Jiebo},
  journal={The 39th Annual Conference on Neural Information Processing Systems},
  year={2025},
  abbr={NeurIPS},
  preview={mmigbench.png},
  html={https://arxiv.org/abs/2505.19415},
  code={https://github.com/hanghuacs/MMIG-Bench},
  website={https://hanghuacs.github.io/MMIG-Bench/},
  stars={hanghuacs/MMIG-Bench},
  bibtex_show={true}
}
@article{huang2025zerosep,
  title={ZeroSep: Separate Anything in Audio with Zero Training},
  author={Huang, Chao and Ma, Yuesheng and Huang, Junxuan and Liang, Susan and Tang, Yunlong and Bi, Jing and Liu, Wenqiang and Mesgarani, Nima and Xu, Chenliang},
  journal={The 39th Annual Conference on Neural Information Processing Systems},
  html={https://arxiv.org/abs/2505.23625},
  preview={zerosep.png},
  website={https://wikichao.github.io/ZeroSep/},
  abbr = {NeurIPS},
  year={2025},
  bibtex_show={true}
}
@article{liu2025harnessing,
  title={Harnessing the Computation Redundancy in ViTs to Boost Adversarial Transferability},
  author={Liu, Jiani and Wang, Zhiyuan and Zhang, Zeliang and Huang, Chao and Liang, Susan and Tang, Yunlong and Xu, Chenliang},
  journal={The 39th Annual Conference on Neural Information Processing Systems},
  html={https://arxiv.org/abs/2504.10804},
  preview={harnessing.png},
  abbr={NeurIPS},
  year={2025},
  bibtex_show={true}
}
@InProceedings{tang2024vidcompostion,
  title={VidComposition: Can MLLMs Analyze Compositions in Compiled Videos?},
  author={Tang*, Yunlong and Guo*, Junjia and Hua, Hang and Liang, Susan and Feng, Mingqian and Li, Xinyang and Mao, Rui and Huang, Chao and Bi, Jing and Zhang, Zeliang and Fazli, Pooyan and Xuâ€ , Chenliang},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  year={2025},
  pages={8490-8500},
  preview={vidcomposition.png},
  bibtex_show={true},
  html={https://openaccess.thecvf.com/content/CVPR2025/html/Tang_VidComposition_Can_MLLMs_Analyze_Compositions_in_Compiled_Videos_CVPR_2025_paper.html},
  website={https://yunlong10.github.io/VidComposition/},
  code={https://github.com/yunlong10/VidComposition},
  stars={yunlong10/VidComposition},
  category={work},
  abbr = {CVPR},
  selected={true},
  google_scholar_id={_FxGoFyzp5QC},
}
@article{tang2025ai4anime,
  title={Generative AI for Cel-Animation: A Survey ðŸ”¥ðŸ”¥ðŸ”¥}, 
  author={Tang, Yunlong and Guo, Junjia and Liu, Pinxin and Wang, Zhiyuan and Hua, Hang and Zhong, Jia-Xing and Xiao, Yunzhong and Huang, Chao and Song, Luchuan and Liang, Susan and Song, Yizhi and He, Liu and Bi, Jing and Feng, Mingqian and Li, Xinyang and Zhang, Zeliang and Xu, Chenliang},
  journal={arXiv preprint arXiv:2501.06250},
  abbr={ICCVW},
  year={2025},
  category={fun},
  html={https://arxiv.org/abs/2501.06250},
  code={https://github.com/yunlong10/Awesome-AI4Animation},
  stars={yunlong10/Awesome-AI4Animation},
  preview={teaser-ai4anime.png},
  bibtex_show={true},
  google_scholar_id={Se3iqnhoufwC},
}
@InProceedings{bi2024unveiling,
  title={Unveiling Visual Perception in Language Models: An Attention Head Analysis Approach},
  author={Bi, Jing and Guo, Junjia and Tang, Yunlong and Wen, Lianggong Bruce and Liu, Zhang and Xu, Chenliang},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  year={2025},
  html={https://openaccess.thecvf.com/content/CVPR2025/html/Bi_Unveiling_Visual_Perception_in_Language_Models_An_Attention_Head_Analysis_CVPR_2025_paper.html},
  bibtex_show={true},
  preview={unveiling.png},
  abbr = {CVPR},
}
@InProceedings{tang2024avicuna,
  title={Empowering LLMs with Pseudo-Untrimmed Videos for Audio-Visual Temporal Understanding},
  author={Tang, Yunlong and Shimada, Daiki and Bi, Jing and Feng, Mingqian and Hua, Hang and Xuâ€ , Chenliang},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  number={7},
  pages={7293-7301},
  year={2025},
  doi={10.1609/aaai.v39i7.32784},
  category={work},
  preview={teaser-avicuna.png},
  html={https://ojs.aaai.org/index.php/AAAI/article/view/32784},
  code={https://github.com/yunlong10/AVicuna},
  stars={yunlong10/AVicuna},
  bibtex_show={true},
  selected={true},
  abbr = {AAAI},
  google_scholar_id={IjCSPb-OGe4C},
}
@InProceedings{hua2024v2xum,
  title={V2Xum-LLM: Cross-modal Video Summarization with Temporal Prompt Instruction Tuning},
  author={Hua*, Hang and Tang*, Yunlong and Xu, Chenliang and Luoâ€ , Jiebo},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  number={4},
  pages={3599-3607},
  year={2025},
  category={work},
  doi={10.1609/aaai.v39i4.32374},
  preview={v2xum-llama.png},
  html={https://ojs.aaai.org/index.php/AAAI/article/view/32374},
  website={https://hanghuacs.github.io/v2xum/},
  stars={hanghuacs/V2Xum-LLM},
  code={https://github.com/hanghuacs/V2Xum-LLM},
  bibtex_show={true},
  selected={true},
  abbr = {AAAI},
  google_scholar_id={zYLM7Y9cAGgC},
}
@InProceedings{tang2024cardiff,
  title={CaRDiff: Video Salient Object Ranking Chain of Thought Reasoning for Saliency Prediction with Diffusion}, 
  author={Tang, Yunlong and Zhan, Gen and Yang, Li and Liao, Yiting and Xuâ€ , Chenliang},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2025},
  html={https://ojs.aaai.org/index.php/AAAI/article/view/32785},
  doi={10.1609/aaai.v39i7.32785},
  volume={39},
  number={7},
  pages={7302-7310},
  preview={cardiff.png},
  category={work},
  selected={true},
  bibtex_show={true},
  abbr = {AAAI},
  google_scholar_id={W7OEmFMy1HYC},
}
@article{huang2025fresca,
  title={FreSca: Unveiling the Scaling Space in Diffusion Models},
  author={Huang, Chao and Liang, Susan and Tang, Yunlong and Ma, Li and Tian, Yapeng and Xuâ€ , Chenliang},
  journal={arXiv preprint arXiv:2504.02154},
  year={2025},
  html={https://arxiv.org/abs/2504.02154},
  preview={fresca.png},
  code={https://github.com/WikiChao/FreSca},
  stars={WikiChao/FreSca},
  bibtex_show={true},
  abbr = {CVPRW},
}
@article{kubrick2025,
  title={Kubrick: Multimodal Agent Collaborations for Synthetic Video Generation},
  author={He, Liu and Song, Yizhi and Huang, Hejun and Liu, Pinxin and Tang, Yunlong and Aliaga, Daniel and Zhou, Xin},
  journal={arXiv preprint arXiv:2408.10453},
  year={2025},
  html={https://arxiv.org/abs/2408.10453},
  bibtex_show={true},
  preview={kubrick.png},
  abbr = {CVPRW},
}
@article{liu2024emo,
  title={GaussianStyle: Gaussian Head Avatar via StyleGAN},
  author={Liu, Pinxin and Song, Luchuan and Zhang, Daoan and Hua, Hang and Tang, Yunlong and Tu, Huaijin and Luo, Jiebo and Xuâ€ , Chenliang},
  journal={International Conference on 3D Vision},
  year={2025},
  html={https://arxiv.org/abs/2402.00827},
  code={https://github.com/andypinxinliu/HumanFaceProject},
  stars={andypinxinliu/HumanFaceProject},
  bibtex_show={true},
  abbr={3DV},
  preview={gaussian_style.png}
}
@article{tang2025catv,
  title={Caption Anything in Video: Fine-grained Object-centric Captioning via Spatiotemporal Multimodal Prompting},
  author={Tang, Yunlong and Bi, Jing and Huang, Chao and Liang, Susan and Shimada, Daiki and Hua, Hang and Xiao, Yunzhong and Song, Yizhi and Liu, Pinxin and Feng, Mingqian and Guo, Junjia and Liu, Zhuo and Song, Luchuan and Vosoughi, Ali and He, Jinxi and He, Liu and Zhang, Zeliang and Luo, Jiebo and Xuâ€ , Chenliang},
  journal={arXiv preprint arXiv:2504.05541},
  year={2025},
  preview={cat-v.png},
  html={https://arxiv.org/abs/2504.05541},
  code={https://github.com/yunlong10/CAT-V},
  stars={yunlong10/CAT-V},
  bibtex_show={true}
}
@article{bi2025mllmreasoning,
  title={Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning},
  author={Bi, Jing and Liang, Susan and Zhou, Xiaofei and Liu, Pinxin and Guo, Junjia and Tang, Yunlong and Song, Luchuan and Huang, Chao and Sun, Guangyu and He, Jinxi and Wu, Jiarui and Yang, Shu and Zhang, Daoan and Chen, Chen and Wen, Lianggong Bruce and Liu, Zhang and Luo, Jiebo and Xu, Chenliang},
  journal={arXiv preprint arXiv:2504.03151},
  year={2025},
  html={https://arxiv.org/abs/2504.03151},
  preview={reasoning_survey.png},
  code={https://github.com/jing-bi/awesome-M.LLM-reasoning},
  stars={jing-bi/awesome-M.LLM-reasoning},
  website={https://reason.jing.vision/},
  google_scholar_id={MXK_kJrjxJIC}
}
@article{bi2025verify,
  title={VERIFY: A Benchmark of Visual Explanation and Reasoning for Investigating Multimodal Reasoning Fidelity},
  author={Bi, Jing and Guo, Junjia and Liang, Susan and Sun, Guangyu and Song, Luchuan and Tang, Yunlong and He, Jinxi and Wu, Jiarui and Vosoughi, Ali and Chen, Chen and Xu, Chenliang},
  journal={arXiv preprint arXiv:2503.11557},
  year={2025},
  html={https://arxiv.org/abs/2503.11557},
  website={https://verify-eqh.pages.dev/},
  preview={verify.png},
  bibtex_show={true}
}
@InProceedings{bi2024eagle,
  title={EAGLE: Egocentric AGgregated Language-video Engine}, 
  author={Bi, Jing and Tang, Yunlong and Song, Luchuan and Vosoughi, Ali and Nguyen, Nguyen and Xuâ€ , Chenliang},
  booktitle={Proceedings of the 32nd ACM International Conference on Multimedia},
  year={2024},
  preview={teaser-eagle.png},
  html={https://dl.acm.org/doi/10.1145/3664647.3681618},
  website={https://page-eagle.jing.vision/},
  abbr={ACM MM},
  bibtex_show={true},
  google_scholar_id={Y0pCki6q_DkC}
}
@InProceedings{moskalenko2024aim,
  title={AIM 2024 Challenge on Video Saliency Prediction: Methods and Results},
  author={Moskalenko, Andrey and Bryncev, Alexey and Vatolin, Dmitry and Timofte, Radu and Zhan, Gen and Yang, Li and Tang, Yunlong and Liao, Yiting and Lin, Jiongzhi and Huang, Baitao and Moradi, Morteza and Moradi, Mohammad and Rundo, Francesco and Spampinato, Concetto and Borji, Ali and Palazzo, Simone and Zhu, Yuxin and Sun, Yinan and Duan, Huiyu and Cao, Yuqin and Jia, Ziheng and Hu, Qiang and Min, Xiongkuo and Zhai, Guangtao and Fang, Hao and Cong, Runmin and Lu, Xiankai and Zhou, Xiaofei and Zhang, Wei and Zhao, Chunyu and Mu, Wentao and Deng, Tao and Tavakoli, Hamed R},
  booktitle={Proceedings of the European Conference on Computer Vision Workshops},
  abbr={ECCVW},
  code={https://github.com/msu-video-group/ECCVW24_Saliency_Prediction},
  stars={msu-video-group/ECCVW24_Saliency_Prediction},
  year={2024},
  html={https://arxiv.org/abs/2409.14827},
  website={https://challenges.videoprocessing.ai/challenges/video-saliency-prediction-leaderboard.html},
  preview={eccv-aim.jpg},
  bibtex_show={true},
}
@article{hua2024mmcomposition,
  title={MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models},
  author={Hua*, Hang and Tang*, Yunlong and Zeng*, Ziyun and Cao, Liangliang and Yang, Zhengyuan and He, Hangfeng and Xu, Chenliang and Luo, Jiebo},
  journal={arXiv preprint arXiv:2410.09733},
  year={2024},
  bibtex_show={true},
  code={https://github.com/hanghuacs/MMComposition},
  stars={hanghuacs/MMComposition},
  html={https://arxiv.org/abs/2410.09733},
  website={https://hanghuacs.github.io/MMComposition/},
  preview={mmcomp.png},
  google_scholar_id={eQOLeE2rZwMC}
}
@article{feng2024more,
  title={Do More Details Always Introduce More Hallucinations in LVLM-based Image Captioning?},
  author={Feng, Mingqian and Tang, Yunlong and Zhang, Zeliang and Xuâ€ , Chenliang},
  journal={arXiv preprint arXiv:2406.12663},
  year={2024},
  preview={DBD-teaser.jpg},
  html={https://arxiv.org/abs/2406.12663},
  bibtex_show={true}
}
@article{huang2024scalingconcept,
  title={Scaling Concept with Text-Guided Diffusion Models},
  author={Huang, Chao and Liang, Susan and Tang, Yunlong and Tian, Yapeng and Kumar, Anurag and Xuâ€ , Chenliang},
  journal={arXiv preprint arXiv:2410.24151},
  year={2024},
  preview={scaling-concept.png},
  html={https://arxiv.org/abs/2410.24151},
  bibtex_show={true},
  website={https://wikichao.github.io/ScalingConcept/},
  code={https://github.com/WikiChao/ScalingConcept},
  stars={WikiChao/ScalingConcept}
}
@article{tang2023llmva,
  title={LLMVA-GEBC: Large Language Model with Video Adapter for Generic Event Boundary Captioning},
  author={Tang, Yunlong and Zhang, Jinrui and Wang, Xiangchen and Wang, Teng and Zhengâ€ , Feng},
  journal={IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  year={2023},
  preview={llmva-gebc.png},
  html={https://arxiv.org/abs/2306.10354},
  code={https://github.com/zjr2000/LLMVA-GEBC},
  abbr={CVPRW},
  bibtex_show={true},
  stars={zjr2000/LLMVA-GEBC}
}
@InProceedings{xu2023launchpadgpt,
  title={LaunchpadGPT: Language Model as Music Visualization Designer on Launchpad},
  author={Xu*, Siting and Tang*, Yunlong and Zhengâ€ , Feng},
  booktitle={Proceedings of the International Computer Music Conference},
  year={2023},
  pages={213-217},
  preview={launchpadgpt.png},
  abbr={ICMC},
  category={fun},
  bibtex_show={true},
  html={https://arxiv.org/abs/2307.04827},
  code={https://github.com/yunlong10/LaunchpadGPT},
  stars={yunlong10/LaunchpadGPT}
}
@article{wang2023caption,
  title={Caption Anything: Interactive Image Description with Diverse Multimodal Controls},
  author={Wang*, Teng and Zhang*, Jinrui and Fei*, Junjie and Zheng, Hao and Tang, Yunlong and Li, Zhe and Gao, Mingqi and Zhao, Shanshan},
  journal={arXiv preprint arXiv:2305.02677},
  year={2023},
  preview={cat.png},
  html={https://arxiv.org/abs/2305.02677},
  code={https://github.com/ttengwang/Caption-Anything},
  bibtex_show={true},
  stars={ttengwang/Caption-Anything},
  google_scholar_id={u-x6o8ySG0sC}
}
@InProceedings{Tang_2022_ACCV,
    author    = {Tang, Yunlong and Xu, Siting and Wang, Teng and Lin, Qin and Lu, Qinglin and Zhengâ€ , Feng},
    title     = {Multi-modal Segment Assemblage Network for Ad Video Editing with Importance-Coherence Reward},
    booktitle = {Proceedings of the Asian Conference on Computer Vision},
    year      = {2022},
    pages     = {3519-3535},
    preview   = {m_san.png},
    abbr      = {ACCV},
    bibtex_show={true},
    html={https://openaccess.thecvf.com/content/ACCV2022/html/Tang_Multi-modal_Segment_Assemblage_Network_for_Ad_Video_Editing_with_Importance-Coherence_ACCV_2022_paper.html},
    code={https://github.com/yunlong10/Ads-1k},
    stars={yunlong10/Ads-1k},
    google_scholar_id={u5HHmVD_uO8C}
}
