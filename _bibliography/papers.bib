---
---
@article{tang2024cardiff,
  title={CaRDiff: Video Salient Object Ranking Chain of Thought Reasoning for Saliency Prediction with Diffusion}, 
  author={Tang, Yunlong and Zhan, Gen and Yang, Li and Liao, Yiting and Xuâ€ , Chenliang},
  journal={AAAI Conference on Artificial Intelligence (AAAI)},
  year={2025},
  html={https://arxiv.org/abs/2408.12009},
  pdf={https://arxiv.org/pdf/2408.12009},
  preview={cardiff.png},
  abstract={
    Video saliency prediction aims to identify the regions in a video that attract human attention and gaze, driven by bottom-up features from the video and top-down processes like memory and cognition.
    Among these top-down influences, language plays a crucial role in guiding attention by shaping how visual information is interpreted.
    Existing methods primarily focus on modeling perceptual information while neglecting the reasoning process facilitated by language, where ranking cues are crucial outcomes of this process and practical guidance for saliency prediction.
    In this paper, we propose CaRDiff (Caption, Rank, and generate with Diffusion), a framework that imitates the process by integrating multimodal large language model (MLLM), a grounding module, and a diffusion model, to enhance video saliency prediction.
    Specifically, we introduce a novel prompting method VSOR-CoT (Video Salient Object Ranking Chain of Thought), which utilizes an MLLM with a grounding module to caption video content and infer salient objects along with their rankings and positions.
    This process derives ranking maps that can be sufficiently leveraged by the diffusion model to decode the saliency maps for the given video accurately.
    Extensive experiments show the effectiveness of VSOR-CoT in improving the performance of video saliency prediction.
    The proposed CaRDiff performs better than state-of-the-art models on the MVS dataset and demonstrates cross-dataset capabilities on the DHF1k dataset through zero-shot evaluation.
  },
  selected={true},
  bibtex_show={true},
  abbr = {AAAI},
}
@article{vidllmsurvey,
  title={Video Understanding with Large Language Models: A Survey ðŸ”¥ðŸ”¥ðŸ”¥}, 
  author={Tang*, Yunlong and Bi*, Jing and Xu*, Siting and Song, Luchuan and Liang, Susan and Wang, Teng and Zhang, Daoan and An, Jie and Lin, Jingyang and Zhu, Rongyi and Vosoughi, Ali and Huang, Chao and Zhang, Zeliang and Liu, Pinxin and Feng, Mingqian and Zheng, Feng and Zhang, Jianguo and Luo, Ping and Luo, Jiebo and Xuâ€ , Chenliang},
  journal={arXiv preprint arXiv:2312.17432},
  year={2023},
  preview={vidllm_survey.png},
  selected={true},
  html={https://arxiv.org/abs/2312.17432},
  code={https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding},
  pdf={https://arxiv.org/pdf/2312.17432},
  bibtex_show={true},
  abstract={
    With the burgeoning growth of online video platforms and the escalating volume of video content, the demand for proficient video understanding tools has intensified markedly.
    Given the remarkable capabilities of large language models (LLMs) in language and multimodal tasks, this survey provides a detailed overview of recent advancements in video understanding that harness the power of LLMs (Vid-LLMs).
    The emergent capabilities of Vid-LLMs are surprisingly advanced, particularly their ability for open-ended multi-granularity (general, temporal, and spatiotemporal) reasoning combined with commonsense knowledge, suggesting a promising path for future video understanding.
    We examine the unique characteristics and capabilities of Vid-LLMs, categorizing the approaches into three main types: Video Analyzer x LLM, Video Embedder x LLM, and (Analyzer + Embedder) x LLM.
    Furthermore, we identify five sub-types based on the functions of LLMs in Vid-LLMs: LLM as Summarizer, LLM as Manager, LLM as Text Decoder, LLM as Regressor, and LLM as Hidden Layer.
    Furthermore, this survey presents a comprehensive study of the tasks, datasets, benchmarks, and evaluation methodologies for Vid-LLMs.
    Additionally, it explores the expansive applications of Vid-LLMs across various domains, highlighting their remarkable scalability and versatility in real-world video understanding challenges.
    Finally, it summarizes the limitations of existing Vid-LLMs and outlines directions for future research.
    For more information, readers are recommended to visit the repository at https://github.com/yunlong10/awesome-llms-for-video-understanding.
  },
  stars={yunlong10/Awesome-LLMs-for-Video-Understanding}
}
@article{tang2024avicuna,
  title={Empowering LLMs with Pseudo-Untrimmed Videos for Audio-Visual Temporal Understanding},
  author={Tang, Yunlong and Shimada, Daiki and Bi, Jing and Feng, Mingqian and Hua, Hang and Xuâ€ , Chenliang},
  journal={AAAI Conference on Artificial Intelligence (AAAI)},
  year={2025},
  abstract={
    Large language models (LLMs) have demonstrated remarkable capabilities in natural language and multimodal domains.
    By fine-tuning multimodal LLMs with temporal annotations from well-annotated datasets, e.g., dense video captioning datasets, their temporal understanding capacity in video-language tasks can be obtained.
    However, there is a notable lack of untrimmed audio-visual video datasets with precise temporal annotations for events.
    This deficiency hinders LLMs from learning the alignment between time, audio-visual events, and text tokens, thus impairing their ability to localize audio-visual events in videos temporally.
    To address this gap, we introduce PU-VALOR, a comprehensive audio-visual dataset comprising over 114,000 pseudo-untrimmed videos with detailed temporal annotations.
    PU-VALOR is derived from the large-scale but coarse-annotated audio-visual dataset VALOR, through a subtle method involving event-based video clustering, random temporal scaling, and permutation.
    By fine-tuning a multimodal LLM on PU-VALOR, we developed AVicuna, a model capable of aligning audio-visual events with temporal intervals and corresponding text tokens. AVicuna excels in temporal localization and time-aware dialogue capabilities.
    Our experiments demonstrate that AVicuna effectively handles temporal understanding in audio-visual videos and achieves state-of-the-art performance on open-ended video QA, audio-visual QA, and audio-visual event dense localization tasks.
  },
  preview={teaser-avicuna.png},
  html={https://arxiv.org/abs/2403.16276},
  pdf={https://arxiv.org/pdf/2403.16276},
  bibtex_show={true},
  selected={true},
  abbr = {AAAI},
}
@article{hua2024v2xum,
  title={V2Xum-LLM: Cross-modal Video Summarization with Temporal Prompt Instruction Tuning},
  author={Hua*, Hang and Tang*, Yunlong and Xu, Chenliang and Luoâ€ , Jiebo},
  journal={AAAI Conference on Artificial Intelligence (AAAI)},
  year={2025},
  abstract={
    Video summarization aims to create short, accurate, and cohesive summaries of longer videos.
    Despite the existence of various video summarization datasets, a notable limitation is their limited amount of source videos, which hampers the effective training of advanced large vision-language models (VLMs).
    Additionally, most existing datasets are created for video-to-video summarization, overlooking the contemporary need for multimodal video content summarization.
    Recent efforts have been made to expand from unimodal to multimodal video summarization, categorizing the task into three sub-tasks based on the summary's modality: video-to-video (V2V), video-to-text (V2T), and a combination of video and text summarization (V2VT).
    However, the textual summaries in previous multimodal datasets are inadequate.
    To address these issues, we introduce Instruct-V2Xum, a cross-modal video summarization dataset featuring 30,000 diverse videos sourced from YouTube, with lengths ranging from 40 to 940 seconds and an average summarization ratio of 16.39%.
    Each video summary in Instruct-V2Xum is paired with a textual summary that references specific frame indexes, facilitating the generation of aligned video and textual summaries.
    In addition, we propose a new video summarization framework named V2Xum-LLM. V2Xum-LLM, specifically V2Xum-LLaMA in this study, is the first framework that unifies different video summarization tasks into one large language model's (LLM) text decoder and achieves task-controllable video summarization with temporal prompts and task instructions.
    Experiments show that V2Xum-LLaMA outperforms strong baseline models on multiple video summarization tasks.
    Furthermore, we propose an enhanced evaluation metric for V2V and V2VT summarization tasks.
  },
  preview={v2xum-llama.png},
  html={https://arxiv.org/abs/2404.12353},
  pdf={https://arxiv.org/pdf/2404.12353},
  website={https://hanghuacs.github.io/v2xum/},
  bibtex_show={true},
  selected={true},
  abbr = {AAAI},
}
@article{tang2024vidcompostion,
  title={VidComposition: Can MLLMs Analyze Compositions in Compiled Videos?},
  author={Tang*, Yunlong and Guo*, Junjia and Hua, Hang and Liang, Susan and Feng, Mingqian and Li, Xinyang and Mao, Rui and Huang, Chao and Bi, Jing and Zhang, Zeliang and Fazli, Pooyan and Xuâ€ , Chenliang},
  journal={arXiv preprint arXiv:2411.10979},
  year={2024},
  preview={vidcomposition.png},
  bibtex_show={true},
  html={https://arxiv.org/abs/2411.10979},
  website={https://yunlong10.github.io/VidComposition/},
  code={https://github.com/yunlong10/VidComposition},
  stars={yunlong10/VidComposition},
  abstract={
    The advancement of Multimodal Large Language Models (MLLMs) has enabled significant progress in multimodal understanding, expanding their capacity to analyze video content.
    However, existing evaluation benchmarks for MLLMs primarily focus on abstract video comprehension, lacking a detailed assessment of their ability to understand video compositions, the nuanced interpretation of how visual elements combine and interact within highly compiled video contexts.
    We introduce VidComposition, a new benchmark specifically designed to evaluate the video composition understanding capabilities of MLLMs using carefully curated compiled videos and cinematic-level annotations.
    VidComposition includes 982 videos with 1706 multiple-choice questions, covering various compositional aspects such as camera movement, angle, shot size, narrative structure, character actions and emotions, etc.
    Our comprehensive evaluation of 33 open-source and proprietary MLLMs reveals a significant performance gap between human and model capabilities. This highlights the limitations of current MLLMs in understanding complex, compiled video compositions and offers insights into areas for further improvement.
    The leaderboard and evaluation code are available at https://yunlong10.github.io/VidComposition/.
  },
}
@article{huang2024scalingconcept,
  title={Scaling Concept with Text-Guided Diffusion Models},
  author={Huang, Chao and Liang, Susan and Tang, Yunlong and Tian, Yapeng and Kumar, Anurag and Xuâ€ , Chenliang},
  journal={arXiv preprint arXiv:2410.24151},
  year={2024},
  preview={scaling-concept.png},
  html={https://arxiv.org/abs/2410.24151},
  bibtex_show={true},
  website={https://wikichao.github.io/ScalingConcept/},
  code={https://github.com/WikiChao/ScalingConcept},
  stars={WikiChao/ScalingConcept},
  abstract={
    Text-guided diffusion models have revolutionized generative tasks by producing high-fidelity content from text descriptions.
    They have also enabled an editing paradigm where concepts can be replaced through text conditioning (e.g., a dog to a tiger).
    In this work, we explore a novel approach: instead of replacing a concept, can we enhance or suppress the concept itself?
    Through an empirical study, we identify a trend where concepts can be decomposed in text-guided diffusion models.
    Leveraging this insight, we introduce ScalingConcept, a simple yet effective method to scale decomposed concepts up or down in real input without introducing new elements.
    To systematically evaluate our approach, we present the WeakConcept-10 dataset, where concepts are imperfect and need to be enhanced.
    More importantly, ScalingConcept enables a variety of novel zero-shot applications across image and audio domains, including tasks such as canonical pose generation and generative sound highlighting or removal.}
}
@article{hua2024mmcomposition,
  title={MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models},
  author={Hua*, Hang and Tang*, Yunlong and Zeng*, Ziyun and Cao, Liangliang and Yang, Zhengyuan and He, Hangfeng and Xu, Chenliang and Luo, Jiebo},
  journal={arXiv preprint arXiv:2410.09733},
  year={2024},
  bibtex_show={true},
  code={https://github.com/hanghuacs/MMComposition},
  stars={hanghuacs/MMComposition},
  html={https://arxiv.org/abs/2410.09733},
  website={https://hanghuacs.github.io/MMComposition/},
  abstract={
    The advent of large Vision-Language Models (VLMs) has significantly advanced multimodal understanding, enabling more sophisticated and accurate integration of visual and textual information across various tasks, including image and video captioning, visual question answering, and cross-modal retrieval.
    Despite VLMs' superior capabilities, researchers lack a comprehensive understanding of their compositionality -- the ability to understand and produce novel combinations of known visual and textual components.
    Prior benchmarks provide only a relatively rough compositionality evaluation from the perspectives of objects, relations, and attributes while neglecting deeper reasoning about object interactions, counting, and complex compositions.
    However, compositionality is a critical ability that facilitates coherent reasoning and understanding across modalities for VLMs.
    To address this limitation, we propose MMCOMPOSITION, a novel human-annotated benchmark for comprehensively and accurately evaluating VLMs' compositionality.
    Our proposed benchmark serves as a complement to these earlier works.
    With MMCOMPOSITION, we can quantify and explore the compositionality of the mainstream VLMs.
    Surprisingly, we find GPT-4o's compositionality inferior to the best open-source model, and we analyze the underlying reasons.
    Our experimental analysis reveals the limitations of VLMs in fine-grained compositional perception and reasoning, and points to areas for improvement in VLM design and training.
    Resources available at: https://hanghuacs.github.io/MMComposition/
  },
  preview={mmcomp.png}
}
@InProceedings{bi2024eagle,
  title={EAGLE: Egocentric AGgregated Language-video Engine}, 
  author={Bi, Jing and Tang, Yunlong and Song, Luchuan and Vosoughi, Ali and Nguyen, Nguyen and Xuâ€ , Chenliang},
  booktitle={Proceedings of the 32nd ACM International Conference on Multimedia (ACM MM)},
  year={2024},
  preview={teaser-eagle.png},
  html={https://dl.acm.org/doi/10.1145/3664647.3681618},
  pdf={https://dl.acm.org/doi/pdf/10.1145/3664647.3681618},
  abbr={ACM MM},
  abstract={
    The rapid evolution of egocentric video analysis brings new insights into understanding human activities and intentions from a first-person perspective.
    Despite this progress, the fragmentation in tasks like action recognition, procedure learning, and moment retrieval, etc., coupled with inconsistent annotations and isolated model development, hinders a holistic interpretation of video content.
    In response, we introduce the EAGLE (Egocentric AGgregated Language-video Engine) model and the EAGLE-400K dataset to provide a unified framework that integrates various egocentric video understanding tasks. 
    EAGLE-400K, the first large-scale instruction-tuning dataset tailored for egocentric video, features 400K diverse samples to enhance a broad spectrum task from activity recognition to procedure knowledge learning.
    Moreover, EAGLE, a strong video-based multimodal large language model (MLLM), is designed to effectively capture both spatial and temporal information.
    In addition, we propose a set of evaluation metrics designed to facilitate a thorough assessment of MLLM for egocentric video understanding.
    Our extensive experiments demonstrate EAGLE's superior performance over existing models, highlighting its ability to balance task-specific understanding with comprehensive video interpretation.
    With EAGLE, we aim to pave the way for novel research opportunities and practical applications in real-world scenarios.
  },
  selected={true},
  bibtex_show={true},
}
@InProceedings{moskalenko2024aim,
  title={AIM 2024 Challenge on Video Saliency Prediction: Methods and Results},
  author={Moskalenko, Andrey and Bryncev, Alexey and Vatolin, Dmitry and Timofte, Radu and Zhan, Gen and Yang, Li and Tang, Yunlong and Liao, Yiting and Lin, Jiongzhi and Huang, Baitao and Moradi, Morteza and Moradi, Mohammad and Rundo, Francesco and Spampinato, Concetto and Borji, Ali and Palazzo, Simone and Zhu, Yuxin and Sun, Yinan and Duan, Huiyu and Cao, Yuqin and Jia, Ziheng and Hu, Qiang and Min, Xiongkuo and Zhai, Guangtao and Fang, Hao and Cong, Runmin and Lu, Xiankai and Zhou, Xiaofei and Zhang, Wei and Zhao, Chunyu and Mu, Wentao and Deng, Tao and Tavakoli, Hamed R},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV) Workshops},
  abbr={ECCV Workshops},
  code={https://github.com/msu-video-group/ECCVW24_Saliency_Prediction},
  stars={msu-video-group/ECCVW24_Saliency_Prediction},
  year={2024},
  html={https://arxiv.org/abs/2409.14827},
  website={https://challenges.videoprocessing.ai/challenges/video-saliency-prediction-leaderboard.html},
  abstract={
    This paper reviews the Challenge on Video Saliency Prediction at AIM 2024.
    The goal of the participants was to develop a method for predicting accurate saliency maps for the provided set of video sequences.
    Saliency maps are widely exploited in various applications, including video compression, quality assessment, visual perception studies, the advertising industry, etc.
    For this competition, a previously unused large-scale audio-visual mouse saliency (AViMoS) dataset of 1500 videos with more than 70 observers per video was collected using crowdsourced mouse tracking.
    The dataset collection methodology has been validated using conventional eye-tracking data and has shown high consistency.
    Over 30 teams registered in the challenge, and there are 7 teams that submitted the results in the final phase.
    The final phase solutions were tested and ranked by commonly used quality metrics on a private test subset.
    The results of this evaluation and the descriptions of the solutions are presented in this report.
    All data, including the private test subset, is made publicly available on the challenge homepage - https://challenges.videoprocessing.ai/challenges/video-saliency-prediction.html},
  preview={eccv-aim.jpg},
  bibtex_show={true},
}
@article{liu2024emo,
  title={GaussianStyle: Gaussian Head Avatar via StyleGAN},
  author={Liu, Pinxin and Song, Luchuan and Zhang, Daoan and Hua, Hang and Tang, Yunlong and Tu, Huaijin and Luo, Jiebo and Xuâ€ , Chenliang},
  journal={International Conference on 3D Vision (3DV)},
  year={2025},
  html={https://arxiv.org/abs/2402.00827},
  pdf={https://arxiv.org/pdf/2402.00827},
  bibtex_show={true},
  abstract={
    Existing methods like Neural Radiation Fields (NeRF) and 3D Gaussian Splatting (3DGS) have made significant strides in facial attribute control such as facial animation and components editing, yet they struggle with fine-grained representation and scalability in dynamic head modeling.
    To address these limitations, we propose GaussianStyle, a novel framework that integrates the volumetric strengths of 3DGS with the powerful implicit representation of StyleGAN.
    The GaussianStyle preserves structural information, such as expressions and poses, using Gaussian points, while projecting the implicit volumetric representation into StyleGAN to capture high-frequency details and mitigate the over-smoothing commonly observed in neural texture rendering.
    Experimental outcomes indicate that our method achieves state-of-the-art performance in reenactment, novel view synthesis, and animation.
  },
  abbr={3DV},
  preview={gaussian_style.png}
}
@article{wang2023caption,
  title={Caption Anything: Interactive Image Description with Diverse Multimodal Controls},
  author={Wang*, Teng and Zhang*, Jinrui and Fei*, Junjie and Zheng, Hao and Tang, Yunlong and Li, Zhe and Gao, Mingqi and Zhao, Shanshan},
  journal={arXiv preprint arXiv:2305.02677},
  year={2023},
  preview={qingming.gif},
  html={https://arxiv.org/abs/2305.02677},
  code={https://github.com/ttengwang/Caption-Anything},
  pdf={https://arxiv.org/pdf/2305.02677},
  bibtex_show={true},
  abstract={
    Controllable image captioning is an emerging multimodal topic that aims to describe the image with natural language following human purpose, e.g., looking at the specified regions or telling in a particular text style.
    State-of-the-art methods are trained on annotated pairs of input controls and output captions.
    However, the scarcity of such well-annotated multimodal data largely limits their usability and scalability for interactive AI systems.
    Leveraging unimodal instruction-following foundation models is a promising alternative that benefits from broader sources of data.
    In this paper, we present Caption AnyThing (CAT), a foundation model augmented image captioning framework supporting a wide range of multimodel controls: 1) visual controls, including points, boxes, and trajectories; 2) language controls, such as sentiment, length, language, and factuality.
    Powered by Segment Anything Model (SAM) and ChatGPT, we unify the visual and language prompts into a modularized framework, enabling the flexible combination between different controls.
    Extensive case studies demonstrate the user intention alignment capabilities of our framework, shedding light on effective user interaction modeling in vision-language applications.
    Our code is publicly available at https://github.com/ttengwang/caption-anything
  },
  stars={ttengwang/Caption-Anything}
}
@article{feng2024more,
  title={Do More Details Always Introduce More Hallucinations in LVLM-based Image Captioning?},
  author={Feng, Mingqian and Tang, Yunlong and Zhang, Zeliang and Xuâ€ , Chenliang},
  journal={arXiv preprint arXiv:2406.12663},
  year={2024},
  preview={DBD-teaser.jpg},
  html={https://arxiv.org/abs/2406.12663},
  pdf={https://arxiv.org/pdf/2406.12663},
  bibtex_show={true},
  abstract={
    Large Vision-Language Models (LVLMs) excel in integrating visual and linguistic contexts to produce detailed content, facilitating applications such as image captioning.
    However, using LVLMs to generate descriptions often faces the challenge of object hallucination (OH), where the output text misrepresents actual objects in the input image.
    While previous studies attribute the occurrence of OH to the inclusion of more details, our study finds technical flaws in existing metrics, leading to unreliable evaluations of models and conclusions about OH.
    This has sparked a debate on the question: Do more details always introduce more hallucinations in LVLM-based image captioning?
    In this paper, we address this debate by proposing a novel decoding strategy, Differentiated Beam Decoding (DBD), along with a reliable new set of evaluation metrics: CLIP-Precision, CLIP-Recall, and CLIP-F1. DBD decodes the wealth of information hidden in visual input into distinct language representations called unit facts in parallel.
    This decoding is achieved via a well-designed differential score that guides the parallel search and candidate screening.
    The selected unit facts are then aggregated to generate the final caption. Our proposed metrics evaluate the comprehensiveness and accuracy of image captions by comparing the embedding groups of ground-truth image regions and generated text partitions.
    Extensive experiments on the Visual Genome dataset validate the effectiveness of our approach, demonstrating that it produces detailed descriptions while maintaining low hallucination levels.}
}
@article{tang2023llmva,
  title={LLMVA-GEBC: Large Language Model with Video Adapter for Generic Event Boundary Captioning},
  author={Tang, Yunlong and Zhang, Jinrui and Wang, Xiangchen and Wang, Teng and Zhengâ€ , Feng},
  journal={IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  year={2023},
  preview={llmva-gebc.png},
  html={https://arxiv.org/abs/2306.10354},
  code={https://github.com/zjr2000/LLMVA-GEBC},
  pdf={https://arxiv.org/pdf/2306.10354},
  abbr={CVPR Workshops},
  bibtex_show={true},
  abstract={
    Our winning entry for the CVPR 2023 Generic Event Boundary Captioning (GEBC) competition is detailed in this paper.
    Unlike conventional video captioning tasks, GEBC demands that the captioning model possess an understanding of immediate changes in status around the designated video boundary, making it a difficult task.
    This paper proposes an effective model LLMVA-GEBC (Large Language Model with Video Adapter for Generic Event Boundary Captioning): (1) We utilize a pretrained LLM for generating human-like captions with high quality.
    (2) To adapt the model to the GEBC task, we take the video Q-former as an adapter and train it with the frozen visual feature extractors and LLM.
    Our proposed method achieved a 76.14 score on the test set and won the first place in the challenge. Our code is available at https://github.com/zjr2000/LLMVA-GEBC.
  },
  stars={zjr2000/LLMVA-GEBC}
}
@InProceedings{xu2023launchpadgpt,
  title={LaunchpadGPT: Language Model as Music Visualization Designer on Launchpad},
  author={Xu*, Siting and Tang*, Yunlong and Zhengâ€ , Feng},
  booktitle={Proceedings of the International Computer Music Conference (ICMC)},
  year={2023},
  pages={213-217},
  preview={launchpadgpt.png},
  abbr={ICMC},
  html={https://arxiv.org/abs/2307.04827},
  code={https://github.com/yunlong10/LaunchpadGPT},
  pdf={https://arxiv.org/pdf/2307.04827},
  stars={yunlong10/LaunchpadGPT},
  abstract={
    Launchpad is a popular music instrument for live performance and music production.
    In this paper, we propose LaunchpadGPT, a language model that can generate music visualization designs for Launchpad.
    We train the model on a large-scale dataset of music visualization designs and demonstrate its effectiveness in generating diverse and creative designs.
    We also develop a web-based platform that allows users to interact with the model and generate music visualization designs in real-time.
    Our code is available at https://github.com/yunlong10/LaunchpadGPT.
  },
}
@InProceedings{Tang_2022_ACCV,
    author    = {Tang, Yunlong and Xu, Siting and Wang, Teng and Lin, Qin and Lu, Qinglin and Zhengâ€ , Feng},
    title     = {Multi-modal Segment Assemblage Network for Ad Video Editing with Importance-Coherence Reward},
    booktitle = {Proceedings of the Asian Conference on Computer Vision (ACCV)},
    year      = {2022},
    pages     = {3519-3535},
    preview   = {m_san.png},
    abbr      = {ACCV},
    bibtex_show={true},
    html={https://openaccess.thecvf.com/content/ACCV2022/html/Tang_Multi-modal_Segment_Assemblage_Network_for_Ad_Video_Editing_with_Importance-Coherence_ACCV_2022_paper.html},
    pdf={https://openaccess.thecvf.com/content/ACCV2022/papers/Tang_Multi-modal_Segment_Assemblage_Network_for_Ad_Video_Editing_with_Importance-Coherence_ACCV_2022_paper.pdf},
    abstract={
      Advertisement video editing aims to automatically edit advertising videos into shorter videos while retaining coherent content and crucial information conveyed by advertisers.
      It mainly contains two stages: video segmentation and segment assemblage.
      The existing method performs well at video segmentation stages but suffers from the problems of dependencies on extra cumbersome models and poor performance at the segment assemblage stage.
      To address these problems, we propose M-SAN (Multi-modal Segment Assemblage Network) which can perform efficient and coherent segment assemblage task end-to-end.
      It utilizes multi-modal representation extracted from the segments and follows the Encoder-Decoder Ptr-Net framework with the Attention mechanism.
      Importance-coherence reward is designed for training M-SAN.
      We experiment on the Ads-1k dataset with 1000+ videos under rich ad scenarios collected from advertisers.
      To evaluate the methods, we propose a unified metric, Imp-Coh@Time, which comprehensively assesses the importance, coherence, and duration of the outputs at the same time.
      Experimental results show that our method achieves better performance than random selection and the previous method on the metric. Ablation experiments further verify that multi-modal representation and importance-coherence reward significantly improve the performance.
      Ads-1k dataset is available at: https://github.com/yunlong10/Ads-1k.
    },
    code={https://github.com/yunlong10/Ads-1k},
    stars={yunlong10/Ads-1k}
}

