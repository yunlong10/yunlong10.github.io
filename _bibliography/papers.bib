---
---
@article{bi2024eagle,
  title={EAGLE: Egocentric AGgregated Language-video Engine}, 
  author={Bi, Jing and Tang, Yunlong and Song, Luchuan and Vosoughi, Ali and Nguyen, Nguyen and Xu†, Chenliang},
  journal={ACM International Conference on Multimedia (ACM MM)},
  year={2024},
  preview={teaser-eagle.png},
  abbr={ACM MM},
  abstract={
    The rapid evolution of egocentric video analysis brings new insights into understanding human activities and intentions from a first-person perspective.
    Despite this progress, the fragmentation in tasks like action recognition, procedure learning, and moment retrieval, etc., coupled with inconsistent annotations and isolated model development, hinders a holistic interpretation of video content.
    In response, we introduce the EAGLE (Egocentric AGgregated Language-video Engine) model and the EAGLE-400K dataset to provide a unified framework that integrates various egocentric video understanding tasks. 
    EAGLE-400K, the first large-scale instruction-tuning dataset tailored for egocentric video, features 400K diverse samples to enhance a broad spectrum task from activity recognition to procedure knowledge learning.
    Moreover, EAGLE, a strong video-based multimodal large language model (MLLM), is designed to effectively capture both spatial and temporal information.
    In addition, we propose a set of evaluation metrics designed to facilitate a thorough assessment of MLLM for egocentric video understanding.
    Our extensive experiments demonstrate EAGLE's superior performance over existing models, highlighting its ability to balance task-specific understanding with comprehensive video interpretation.
    With EAGLE, we aim to pave the way for novel research opportunities and practical applications in real-world scenarios.
  },
  selected={true}
}
@article{vidllmsurvey,
  title={Video Understanding with Large Language Models: A Survey}, 
  author={Tang*, Yunlong and Bi*, Jing and Xu*, Siting and Song, Luchuan and Liang, Susan and Wang, Teng and Zhang, Daoan and An, Jie and Lin, Jingyang and Zhu, Rongyi and Vosoughi, Ali and Huang, Chao and Zhang, Zeliang and Liu, Pinxin and Feng, Mingqian and Zheng, Feng and Zhang, Jianguo and Luo, Ping and Luo, Jiebo and Xu†, Chenliang},
  journal={arXiv preprint arXiv:2312.17432},
  year={2023},
  preview={vidllm_survey.png},
  selected={true},
  html={https://arxiv.org/abs/2312.17432},
  code={https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding},
  pdf={https://arxiv.org/pdf/2312.17432},
  bibtex_show={true},
  abstract={
    With the burgeoning growth of online video platforms and the escalating volume of video content, the demand for proficient video understanding tools has intensified markedly.
    Given the remarkable capabilities of large language models (LLMs) in language and multimodal tasks, this survey provides a detailed overview of recent advancements in video understanding that harness the power of LLMs (Vid-LLMs).
    The emergent capabilities of Vid-LLMs are surprisingly advanced, particularly their ability for open-ended multi-granularity (general, temporal, and spatiotemporal) reasoning combined with commonsense knowledge, suggesting a promising path for future video understanding.
    We examine the unique characteristics and capabilities of Vid-LLMs, categorizing the approaches into three main types: Video Analyzer x LLM, Video Embedder x LLM, and (Analyzer + Embedder) x LLM.
    Furthermore, we identify five sub-types based on the functions of LLMs in Vid-LLMs: LLM as Summarizer, LLM as Manager, LLM as Text Decoder, LLM as Regressor, and LLM as Hidden Layer.
    Furthermore, this survey presents a comprehensive study of the tasks, datasets, benchmarks, and evaluation methodologies for Vid-LLMs.
    Additionally, it explores the expansive applications of Vid-LLMs across various domains, highlighting their remarkable scalability and versatility in real-world video understanding challenges.
    Finally, it summarizes the limitations of existing Vid-LLMs and outlines directions for future research.
    For more information, readers are recommended to visit the repository at https://github.com/yunlong10/awesome-llms-for-video-understanding.}
}
@article{tang2024avicuna,
  title={AVicuna: Audio-Visual LLM with Interleaver and Context-Boundary Alignment for Temporal Referential Dialogue},
  author={Tang, Yunlong and Shimada, Daiki and Bi, Jing and Xu†, Chenliang},
  journal={arXiv preprint arXiv:2403.16276},
  year={2024},
  preview={teaser-avicuna.png},
  html={https://arxiv.org/abs/2403.16276},
  pdf={https://arxiv.org/pdf/2403.16276},
  bibtex_show={true}
}
@article{hua2024v2xum,
  title={V2Xum-LLM: Cross-modal Video Summarization with Temporal Prompt Instruction Tuning},
  author={Hua*, Hang and Tang*, Yunlong and Xu, Chenliang and Luo†, Jiebo},
  journal={arXiv preprint arXiv:2404.12353},
  year={2024},
  preview={v2xum-llama.png},
  html={https://arxiv.org/abs/2404.12353},
  pdf={https://arxiv.org/pdf/2404.12353},
  bibtex_show={true},
}
@article{liu2024emo,
  title={Emo-Avatar: Efficient Monocular Video Style Avatar through Texture Rendering},
  author={Liu, Pinxin and Song, Luchuan and Zhang, Daoan and Hua, Hang and Tang, Yunlong and Tu, Huaijin and Luo, Jiebo and Xu†, Chenliang},
  journal={arXiv preprint arXiv:2402.00827},
  year={2024},
  preview={emo-avatar-pipeline.png}
}
@article{wang2023caption,
  title={Caption Anything: Interactive Image Description with Diverse Multimodal Controls},
  author={Wang*, Teng and Zhang*, Jinrui and Fei*, Junjie and Zheng, Hao and Tang, Yunlong and Li, Zhe and Gao, Mingqi and Zhao, Shanshan},
  journal={arXiv preprint arXiv:2305.02677},
  year={2023},
  preview={qingming.gif},
  html={https://arxiv.org/abs/2305.02677},
  code={https://github.com/ttengwang/Caption-Anything},
  pdf={https://arxiv.org/pdf/2305.02677},
  bibtex_show={true},
  selected={true},
  abstract={
    Controllable image captioning is an emerging multimodal topic that aims to describe the image with natural language following human purpose, e.g., looking at the specified regions or telling in a particular text style.
    State-of-the-art methods are trained on annotated pairs of input controls and output captions.
    However, the scarcity of such well-annotated multimodal data largely limits their usability and scalability for interactive AI systems.
    Leveraging unimodal instruction-following foundation models is a promising alternative that benefits from broader sources of data.
    In this paper, we present Caption AnyThing (CAT), a foundation model augmented image captioning framework supporting a wide range of multimodel controls: 1) visual controls, including points, boxes, and trajectories; 2) language controls, such as sentiment, length, language, and factuality.
    Powered by Segment Anything Model (SAM) and ChatGPT, we unify the visual and language prompts into a modularized framework, enabling the flexible combination between different controls.
    Extensive case studies demonstrate the user intention alignment capabilities of our framework, shedding light on effective user interaction modeling in vision-language applications.
    Our code is publicly available at https://github.com/ttengwang/caption-anything},
}
@article{feng2024more,
  title={Do More Details Always Introduce More Hallucinations in LVLM-based Image Captioning?},
  author={Feng, Mingqian and Tang, Yunlong and Zhang, Zeliang and Xu†, Chenliang},
  journal={arXiv preprint arXiv:2406.12663},
  year={2024},
  preview={DBD-teaser.jpg},
  html={https://arxiv.org/abs/2406.12663},
  pdf={https://arxiv.org/pdf/2406.12663},
  bibtex_show={true},
  abstract={
    Large Vision-Language Models (LVLMs) excel in integrating visual and linguistic contexts to produce detailed content, facilitating applications such as image captioning.
    However, using LVLMs to generate descriptions often faces the challenge of object hallucination (OH), where the output text misrepresents actual objects in the input image.
    While previous studies attribute the occurrence of OH to the inclusion of more details, our study finds technical flaws in existing metrics, leading to unreliable evaluations of models and conclusions about OH.
    This has sparked a debate on the question: Do more details always introduce more hallucinations in LVLM-based image captioning?
    In this paper, we address this debate by proposing a novel decoding strategy, Differentiated Beam Decoding (DBD), along with a reliable new set of evaluation metrics: CLIP-Precision, CLIP-Recall, and CLIP-F1. DBD decodes the wealth of information hidden in visual input into distinct language representations called unit facts in parallel.
    This decoding is achieved via a well-designed differential score that guides the parallel search and candidate screening.
    The selected unit facts are then aggregated to generate the final caption. Our proposed metrics evaluate the comprehensiveness and accuracy of image captions by comparing the embedding groups of ground-truth image regions and generated text partitions.
    Extensive experiments on the Visual Genome dataset validate the effectiveness of our approach, demonstrating that it produces detailed descriptions while maintaining low hallucination levels.}
}
@article{tang2023llmva,
  title={LLMVA-GEBC: Large Language Model with Video Adapter for Generic Event Boundary Captioning},
  author={Tang, Yunlong and Zhang, Jinrui and Wang, Xiangchen and Wang, Teng and Zheng†, Feng},
  journal={arXiv preprint arXiv:2306.10354},
  year={2023},
  preview={llmva-gebc.png},
  html={https://arxiv.org/abs/2306.10354},
  code={https://github.com/zjr2000/LLMVA-GEBC},
  pdf={https://arxiv.org/pdf/2306.10354},
  abbr={CVPR Workshops},
  bibtex_show={true},
  abstract={
    Our winning entry for the CVPR 2023 Generic Event Boundary Captioning (GEBC) competition is detailed in this paper.
    Unlike conventional video captioning tasks, GEBC demands that the captioning model possess an understanding of immediate changes in status around the designated video boundary, making it a difficult task.
    This paper proposes an effective model LLMVA-GEBC (Large Language Model with Video Adapter for Generic Event Boundary Captioning): (1) We utilize a pretrained LLM for generating human-like captions with high quality.
    (2) To adapt the model to the GEBC task, we take the video Q-former as an adapter and train it with the frozen visual feature extractors and LLM.
    Our proposed method achieved a 76.14 score on the test set and won the first place in the challenge. Our code is available at https://github.com/zjr2000/LLMVA-GEBC.
  }
}
@article{xu2023launchpadgpt,
  title={LaunchpadGPT: Language Model as Music Visualization Designer on Launchpad},
  author={Xu*, Siting and Tang*, Yunlong and Zheng†, Feng},
  journal={International Computer Music Conference (ICMC)},
  year={2023},
  preview={launchpadgpt.png},
  abbr={ICMC},
  html={https://arxiv.org/abs/2307.04827},
  code={https://github.com/yunlong10/LaunchpadGPT},
  pdf={https://arxiv.org/pdf/2307.04827},
}
@InProceedings{Tang_2022_ACCV,
    author    = {Tang, Yunlong and Xu, Siting and Wang, Teng and Lin, Qin and Lu, Qinglin and Zheng†, Feng},
    title     = {Multi-modal Segment Assemblage Network for Ad Video Editing with Importance-Coherence Reward},
    booktitle = {Proceedings of the Asian Conference on Computer Vision (ACCV)},
    year      = {2022},
    pages     = {3519-3535},
    preview   = {m_san.png},
    abbr      = {ACCV},
    bibtex_show={true},
    html={https://openaccess.thecvf.com/content/ACCV2022/html/Tang_Multi-modal_Segment_Assemblage_Network_for_Ad_Video_Editing_with_Importance-Coherence_ACCV_2022_paper.html},
    pdf={https://openaccess.thecvf.com/content/ACCV2022/papers/Tang_Multi-modal_Segment_Assemblage_Network_for_Ad_Video_Editing_with_Importance-Coherence_ACCV_2022_paper.pdf},
    abstract={
      Advertisement video editing aims to automatically edit advertising videos into shorter videos while retaining coherent content and crucial information conveyed by advertisers.
      It mainly contains two stages: video segmentation and segment assemblage.
      The existing method performs well at video segmentation stages but suffers from the problems of dependencies on extra cumbersome models and poor performance at the segment assemblage stage.
      To address these problems, we propose M-SAN (Multi-modal Segment Assemblage Network) which can perform efficient and coherent segment assemblage task end-to-end.
      It utilizes multi-modal representation extracted from the segments and follows the Encoder-Decoder Ptr-Net framework with the Attention mechanism.
      Importance-coherence reward is designed for training M-SAN.
      We experiment on the Ads-1k dataset with 1000+ videos under rich ad scenarios collected from advertisers.
      To evaluate the methods, we propose a unified metric, Imp-Coh@Time, which comprehensively assesses the importance, coherence, and duration of the outputs at the same time.
      Experimental results show that our method achieves better performance than random selection and the previous method on the metric. Ablation experiments further verify that multi-modal representation and importance-coherence reward significantly improve the performance.
      Ads-1k dataset is available at: https://github.com/yunlong10/Ads-1k.
    },
    selected={true},    
    code={https://github.com/yunlong10/Ads-1k}
}

